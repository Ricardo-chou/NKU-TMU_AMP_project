{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac73ec49-5ed7-4779-9071-b9d89403916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: fair-esm in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: openpyxl in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (3.1.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (0.22.1)\n",
      "Requirement already satisfied: pandas in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (1.1.5)\n",
      "Requirement already satisfied: numpy in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (1.19.5)\n",
      "Requirement already satisfied: torch in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (1.8.0)\n",
      "Collecting xlrd\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a6/0c/c2a72d51fe56e08a08acc85d13013558a2d793028ae7385448a6ccdfae64/xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "\u001B[K     |████████████████████████████████| 96 kB 65.0 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: et-xmlfile in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Installing collected packages: xlrd\n",
      "Successfully installed xlrd-2.0.1\n",
      "\u001B[33mWARNING: You are using pip version 21.0.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.8/bin/python3.7 -m pip install --upgrade pip' command.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fair-esm openpyxl scikit-learn pandas numpy torch xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90bf85d9-cc83-4f10-9faa-ab99bcf7e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 1.8.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages\n",
      "Requires: numpy, typing-extensions\n",
      "Required-by: torchvision, torchtext\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561aac4c-82c2-4319-8c66-d473d7d74a81",
   "metadata": {},
   "source": [
    "# **第一步：准备工作台！(环境设置与导入库)**\n",
    "\n",
    "想象一下，我们要开始一个重要的生物实验！首先得把实验台收拾干净，把所有需要的工具和试剂准备好，对吧？这一步就是做这个准备工作。我们会：\n",
    "\n",
    "1.  **导入“工具箱” :** 加载所有需要的 Python 库，比如 `pandas` (处理表格数据超方便)、`numpy` (科学计算基础)、`torch` 和 `esm` (我们请来的“蛋白质语言大师” ESM 模型就住在这里面)、以及 `sklearn` (机器学习“武器库”，随机森林就在这)。\n",
    "2.  **设定“实验参数” :** 告诉程序我们的数据放在哪里 (`DATA_DIR`)，要用哪个版本的 ESM 模型 (`ESM_MODEL_NAME` - 我们会选一个对 CPU 友好的版本哦！)，以及比赛规则（比如最多只能突变几个位点 `MAX_MUTATIONS`）。\n",
    "3.  **检查“硬件”:** 看看有没有 GPU 可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6771f530-01d6-488d-ba54-a5a0be261d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # 处理表格数据（如 Excel、CSV）\n",
    "import numpy as np   # 数学计算库\n",
    "import torch         # PyTorch，主要用于模型加载与运行\n",
    "import esm           # Facebook 的蛋白语言模型库 ESM（用于提取蛋白序列嵌入）\n",
    "import os            # 操作文件路径\n",
    "import random        # 设置随机种子\n",
    "from sklearn.model_selection import train_test_split  # 训练集测试集划分\n",
    "from sklearn.metrics import r2_score                  # 模型性能评估指标\n",
    "import re           # 正则表达式，用于处理突变信息\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略警告信息（不影响主逻辑）\n",
    "\n",
    "# 常量定义 \n",
    "TRAIN_DATA_FILE = os.path.join('GFP_data.xlsx')  \n",
    "# 包含亮度、突变信息等训练数据的 Excel 文件\n",
    "WT_SEQ_FILE = os.path.join('AAseqs of 4 GFP proteins.txt')  \n",
    "# 4 个 GFP 蛋白的氨基酸序列（wild-type 序列），后续将基于这些序列设计突变\n",
    "EXCLUSION_FILE = os.path.join('Exclusion_List.csv')  \n",
    "# 不允许的序列清单，可能是失败序列、毒性序列等（用于过滤）\n",
    "\n",
    "\n",
    "# --- 模型与生成参数 ---\n",
    "ESM_MODEL_NAME = \"esm2_t12_35M_UR50D\" # 选择一个中等大小的ESM模型，平衡速度和性能\n",
    "MAX_MUTATIONS = 6 # 比赛规则：最多6个突变\n",
    "N_CANDIDATES_TO_GENERATE = 500 # 生成候选序列的数量（可调整）\n",
    "TOP_N_SELECT = 10 # 最终选择的序列数量\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 设置随机种子以便结果可复现（可选）\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f143d-9aca-4f20-830f-29df29b407ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    " # **第二步：获取原料并清洗！(数据加载与预处理)**\n",
    "\n",
    "实验材料来了！我们需要处理好这些“原料”才能进行下一步。我们会：\n",
    "\n",
    "1.  **加载“配方表” :** 读取 `GFP_data.xlsx` 文件，这里面记录了很多已知的 GFP 突变以及它们的亮度信息。这是我们学习的基础！\n",
    "2.  **找到“原始模板” :** 从 `AAseqs of 4 GFP proteins.txt` 文件里找到我们这次的目标——avGFP 的野生型氨基酸序列。这是我们进行改造的基础。\n",
    "3.  **拿到“禁止名单” :** 加载 `Exclusion_List.csv` 文件。记住！这里面的序列是**不能**提交的，所以我们得时刻对照着它。\n",
    "4.  **筛选与转换 :** 我们只关心 avGFP 的数据，所以会先筛选一下。然后，最关键的一步：把 \"G101A\" 这种突变描述，应用到原始模板上，生成每一个突变体**完整**的氨基酸序列。这就像按配方修改原始模板，得到具体的成品序列。\n",
    "5.  **清洗整理 :** 检查一下，确保生成的序列是有效的，亮度值也是正常的数字。把一些有问题的“原料”剔除掉，保证我们后续使用的是高质量数据。\n",
    "\n",
    "处理完这些，我们就有了干净、规范的训练数据啦！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3e7c5-cea9-4f0a-9deb-d5ecf25f55a7",
   "metadata": {},
   "source": [
    "## 2.1 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "770c4590-f020-4215-82e3-ca159e546a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training data...\")\n",
    "try:\n",
    "    gfp_df = pd.ExcelFile(TRAIN_DATA_FILE, sheet_name='brightness') # 假设亮度数据在名为 'brightness' 的 sheet\n",
    "    print(f\"Loaded {len(gfp_df)} rows from {TRAIN_DATA_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Training data file not found at {TRAIN_DATA_FILE}\")\n",
    "    # exit() # 如果文件不存在，可能需要停止执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931e6ad-2b02-4daa-8ff9-7244d3f70ae9",
   "metadata": {},
   "source": [
    "## 2.2 加载 avGFP 野生型序列 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329667ad-b658-4f6c-aca1-3da1fcc577bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading avGFP WT sequence...\n",
      "Found avGFP WT sequence (Length: 238).\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading avGFP WT sequence...\")\n",
    "avGFP_WT_sequence = None\n",
    "try:\n",
    "    with open(WT_SEQ_FILE, 'r') as f:\n",
    "        # 假设文件格式是 >Header \\n Sequence \\n >Header2...\n",
    "        # 我们需要找到 avGFP 的序列\n",
    "        header = \"\"\n",
    "        seq_lines = []\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                # 如果找到了上一个序列，并且是avGFP，保存它\n",
    "                if \"avGFP\" in header and seq_lines:\n",
    "                    avGFP_WT_sequence = \"\".join(seq_lines).strip()\n",
    "                    break # 找到后退出循环\n",
    "                # 开始新的序列记录\n",
    "                header = line.strip()\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line.strip())\n",
    "        # 处理文件最后一个序列的情况\n",
    "        if avGFP_WT_sequence is None and \"avGFP\" in header and seq_lines:\n",
    "             avGFP_WT_sequence = \"\".join(seq_lines).strip()\n",
    "\n",
    "    if avGFP_WT_sequence:\n",
    "        print(f\"Found avGFP WT sequence (Length: {len(avGFP_WT_sequence)}).\")\n",
    "        # print(avGFP_WT_sequence) # 可以取消注释查看序列\n",
    "    else:\n",
    "        print(\"Error: avGFP WT sequence not found in\", WT_SEQ_FILE)\n",
    "        # 手动设置一个默认值或停止执行\n",
    "        # avGFP_WT_sequence = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\" # 示例\n",
    "        # print(\"Using default WT sequence.\")\n",
    "        # exit()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: WT sequence file not found at {WT_SEQ_FILE}\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7e8d29-36c0-4b01-9fbf-edaa5a594ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'sheet_name'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_37730/2857899309.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Loading training data...\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mgfp_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExcelFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTRAIN_DATA_FILE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msheet_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'brightness'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# 假设亮度数据在名为 'brightness' 的 sheet\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Loaded {len(gfp_df)} rows from {TRAIN_DATA_FILE}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mexcept\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'sheet_name'"
     ]
    }
   ],
   "source": [
    "# --- 2.3 加载排除列表 ---\n",
    "print(\"Loading exclusion list...\")\n",
    "try:\n",
    "    exclusion_df = pd.read_csv(EXCLUSION_FILE)\n",
    "    # 假设排除序列在名为 'sequences-not-submit' 的列中\n",
    "    exclusion_sequences = set(exclusion_df['sequences-not-submit'].astype(str))\n",
    "    print(f\"Loaded {len(exclusion_sequences)} sequences into exclusion list.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Exclusion list file not found at {EXCLUSION_FILE}\")\n",
    "    exclusion_sequences = set() # 如果文件不存在，创建一个空集合\n",
    "    print(\"Warning: Proceeding without an exclusion list.\")\n",
    "except KeyError:\n",
    "    print(f\"Error: Column 'sequences-not-submit' not found in {EXCLUSION_FILE}\")\n",
    "    exclusion_sequences = set()\n",
    "    print(\"Warning: Proceeding without an exclusion list.\")\n",
    "\n",
    "\n",
    "# --- 2.4 预处理训练数据 ---\n",
    "print(\"Preprocessing training data...\")\n",
    "# 筛选 avGFP 数据\n",
    "avGFP_train_df = gfp_df[gfp_df['GFP type'] == 'avGFP'].copy()\n",
    "print(f\"Filtered down to {len(avGFP_train_df)} avGFP entries.\")\n",
    "\n",
    "# 定义函数：根据突变字符串生成完整序列\n",
    "def generate_mutated_sequence(mutation_str, wt_sequence):\n",
    "    \"\"\"\n",
    "    根据突变描述字符串和野生型序列生成突变后的完整序列。\n",
    "    mutation_str: e.g., \"WT\", \"G101A\", \"A12B:C34D\"\n",
    "    wt_sequence: 野生型氨基酸序列字符串\n",
    "    \"\"\"\n",
    "    if not isinstance(mutation_str, str) or not wt_sequence:\n",
    "        return None\n",
    "    if mutation_str.strip().upper() == 'WT':\n",
    "        return wt_sequence\n",
    "\n",
    "    sequence = list(wt_sequence)\n",
    "    mutations = mutation_str.split(':') # 支持多个突变，以冒号分隔\n",
    "    valid_mutation_count = 0\n",
    "    try:\n",
    "        for mut in mutations:\n",
    "            match = re.match(r'([A-Z])(\\d+)([A-Z*.])$', mut.strip(), re.IGNORECASE) # 匹配 G101A, T203*, V163.\n",
    "            if match:\n",
    "                original_aa, pos, new_aa = match.groups()\n",
    "                pos = int(pos) - 1 # 转换为 0-based index\n",
    "\n",
    "                # 检查位置是否有效\n",
    "                if pos < 0 or pos >= len(sequence):\n",
    "                    # print(f\"Warning: Invalid position {pos+1} in mutation '{mut}' for sequence length {len(sequence)}. Skipping mutation.\")\n",
    "                    continue # 跳过无效位置的突变\n",
    "\n",
    "                # 检查原始氨基酸是否匹配 (可选，但建议)\n",
    "                if sequence[pos].upper() != original_aa.upper():\n",
    "                    # print(f\"Warning: Original AA mismatch at position {pos+1} in mutation '{mut}'. Expected {sequence[pos]}, got {original_aa}. Applying mutation anyway.\")\n",
    "                    pass # 允许不匹配，但打印警告\n",
    "\n",
    "                # 处理特殊字符\n",
    "                if new_aa == '*': # 终止密码子 - 通常不希望出现在中间\n",
    "                    # print(f\"Warning: Stop codon '*' mutation '{mut}' encountered. Treating as deletion or invalid sequence for this tutorial.\")\n",
    "                    # 对于亮度预测，终止密码子通常导致无功能蛋白，可以返回None或特殊标记\n",
    "                    return None # 或者根据需要处理\n",
    "                elif new_aa == '.': # 表示与原氨基酸相同 (无突变)\n",
    "                    new_aa = sequence[pos] # 保持不变\n",
    "\n",
    "                sequence[pos] = new_aa.upper()\n",
    "                valid_mutation_count += 1\n",
    "            else:\n",
    "                # print(f\"Warning: Could not parse mutation '{mut}'. Skipping.\")\n",
    "                pass # 跳过无法解析的突变格式\n",
    "        # 如果没有成功应用任何突变（可能是格式问题），返回None\n",
    "        # if valid_mutation_count == 0 and mutations:\n",
    "        #     return None\n",
    "        return \"\".join(sequence)\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing mutation string '{mutation_str}': {e}\")\n",
    "        return None # 返回 None 表示序列生成失败\n",
    "\n",
    "# 应用函数生成序列\n",
    "avGFP_train_df['full_sequence'] = avGFP_train_df['aaMutations'].apply(\n",
    "    lambda x: generate_mutated_sequence(x, avGFP_WT_sequence)\n",
    ")\n",
    "\n",
    "# 清理数据：移除序列生成失败或亮度无效的行\n",
    "original_len = len(avGFP_train_df)\n",
    "avGFP_train_df.dropna(subset=['full_sequence', 'Brightness'], inplace=True)\n",
    "# 确保亮度是数值类型\n",
    "avGFP_train_df['Brightness'] = pd.to_numeric(avGFP_train_df['Brightness'], errors='coerce')\n",
    "avGFP_train_df.dropna(subset=['Brightness'], inplace=True)\n",
    "\n",
    "print(f\"Removed {original_len - len(avGFP_train_df)} rows due to invalid sequences or brightness.\")\n",
    "print(f\"Final training set size: {len(avGFP_train_df)}\")\n",
    "\n",
    "# 查看处理后的数据\n",
    "print(\"\\nSample of processed training data:\")\n",
    "print(avGFP_train_df[['aaMutations', 'Brightness', 'full_sequence']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06166a-0e02-4773-a817-d7ad187feb52",
   "metadata": {},
   "source": [
    "---\n",
    "🧠 **第三步：让机器读懂蛋白质语言！(特征工程 - ESM 嵌入)**\n",
    "\n",
    "蛋白质序列就像一种特殊的语言，直接丢给机器学习模型，它可能看不懂。我们需要把它翻译成模型能理解的“数字语言”。这就是 ESM 大显身手的时候了！但是，考虑到大家的 CPU 可能比较“吃力”，我们做了特别优化：\n",
    "\n",
    "1.  **请个“轻量级”大师 👨‍🏫:** 我们会加载一个**更小巧、更快速**的 ESM 模型版本 (`esm2_t6_8M_UR50D`)。它虽然参数少一点，但跑起来会快很多！\n",
    "2.  **“抽样”学习 📉:** 我们不需要把训练数据里成千上万条序列全都扔给 ESM 处理（那样太慢了！）。我们会从中**随机抽取一部分**（比如 1 万条）作为代表来进行学习。这就像考试前划重点，能大大节省时间！⏳\n",
    "3.  **生成“数字指纹” 🔢:** 对于抽样选出的每一条蛋白质序列，ESM 模型会阅读它，并输出一个固定长度的数字列表（向量），我们称之为“嵌入”(Embedding)。这个嵌入就像是这条蛋白质序列的“数字指纹”，包含了它的关键生物学信息。\n",
    "\n",
    "有了这些“数字指纹”，我们的机器学习模型就能更好地理解蛋白质序列了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9430787d-01ef-4e33-856a-bc2cff4edc9f",
   "metadata": {},
   "outputs": [
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_50",
     "meta": {},
     "name": "stdout",
     "output_type": "stream",
     "parent_header": {
      "date": "2025-04-27T14:11:15.305674Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_50",
      "msg_type": "stream",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     },
     "text": [
      "检测到CUDA GPU，将使用GPU进行计算。\n",
      "正在使用设备: cuda\n",
      "将加载的ESM模型: esm2_t30_150M_UR50D\n",
      "将使用的批次大小: 16\n",
      "\n",
      "正在检查训练数据大小以进行采样...\n",
      "训练数据大小 (51715) 超过限制 (5000)。正在采样...\n",
      "使用 5000 个采样序列进行嵌入。\n",
      "\n",
      "正在加载ESM模型: esm2_t30_150M_UR50D 到设备 cuda...\n",
      "ESM模型在 8.33 秒内成功加载。\n",
      "正在为 5000 个序列生成嵌入，共 313 个批次（批次大小: 16，设备: cuda）...\n",
      "  已处理批次 10/313... (耗时: 10.54 秒)\n",
      "  已处理批次 20/313... (耗时: 17.25 秒)\n",
      "  已处理批次 30/313... (耗时: 23.79 秒)\n",
      "  已处理批次 40/313... (耗时: 29.90 秒)\n",
      "  已处理批次 50/313... (耗时: 36.21 秒)\n",
      "  已处理批次 60/313... (耗时: 42.74 秒)\n",
      "  已处理批次 70/313... (耗时: 49.45 秒)\n",
      "  已处理批次 80/313... (耗时: 56.13 秒)\n",
      "  已处理批次 90/313... (耗时: 62.88 秒)\n",
      "  已处理批次 100/313... (耗时: 69.62 秒)\n",
      "  已处理批次 110/313... (耗时: 76.36 秒)\n",
      "  已处理批次 120/313... (耗时: 83.09 秒)\n",
      "  已处理批次 130/313... (耗时: 89.87 秒)\n",
      "  已处理批次 140/313... (耗时: 96.67 秒)\n",
      "  已处理批次 150/313... (耗时: 103.48 秒)\n",
      "  已处理批次 160/313... (耗时: 110.28 秒)\n",
      "  已处理批次 170/313... (耗时: 117.07 秒)\n",
      "  已处理批次 180/313... (耗时: 123.84 秒)\n",
      "  已处理批次 190/313... (耗时: 130.64 秒)\n",
      "  已处理批次 200/313... (耗时: 137.45 秒)\n",
      "  已处理批次 210/313... (耗时: 144.25 秒)\n",
      "  已处理批次 220/313... (耗时: 151.05 秒)\n",
      "  已处理批次 230/313... (耗时: 157.85 秒)\n",
      "  已处理批次 240/313... (耗时: 164.65 秒)\n",
      "  已处理批次 250/313... (耗时: 171.47 秒)\n",
      "  已处理批次 260/313... (耗时: 178.28 秒)\n",
      "  已处理批次 270/313... (耗时: 185.10 秒)\n",
      "  已处理批次 280/313... (耗时: 191.93 秒)\n",
      "  已处理批次 290/313... (耗时: 198.74 秒)\n",
      "  已处理批次 300/313... (耗时: 205.54 秒)\n",
      "  已处理批次 310/313... (耗时: 212.33 秒)\n",
      "  已处理批次 313/313... (耗时: 214.04 秒)\n",
      "嵌入生成在 214.04 秒内完成。\n",
      "平均每个序列耗时: 0.0428 秒\n",
      "生成的嵌入张量形状: torch.Size([5000, 640])\n",
      "嵌入中未检测到NaN值。\n",
      "\n",
      "准备好的X（嵌入）形状: (5000, 640), y（亮度）形状: (5000,)\n",
      "步骤3（嵌入生成）完成。\n",
      "\n",
      "正在清理内存...\n",
      "清空CUDA缓存...\n",
      "清理完成。\n",
      "\n",
      "数据准备就绪，可以进行模型训练。X shape: (5000, 640), y shape: (5000,)\n"
     ]
    },
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_84",
     "meta": {
      "dependencies_met": true,
      "engine": "91221703-bed9-4a3d-953a-9ea6a097dcc2",
      "started": "2025-04-27T14:11:15.116611Z",
      "status": "ok"
     },
     "output_type": "execute_reply",
     "parent_header": {
      "date": "2025-04-27T14:14:57.546273Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_84",
      "msg_type": "execute_reply",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     }
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import esm  # fair-esm库\n",
    "import os\n",
    "import time  # 用于计时\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略一些不影响结果的警告\n",
    "\n",
    "# --- 优化常量 ---\n",
    "# 根据设备选择合适的ESM模型\n",
    "ESM_MODEL_NAME_CPU = \"esm2_t6_8M_UR50D\"\n",
    "ESM_MODEL_NAME_GPU = \"esm2_t30_150M_UR50D\" # 可以选用更大的模型，例如 \"esm2_t33_650M_UR50D\"，取决于GPU内存\n",
    "\n",
    "# 根据设备选择合适的批次大小\n",
    "CPU_BATCH_SIZE = 8\n",
    "GPU_BATCH_SIZE = 16 # GPU通常可以处理更大的批次，可以根据GPU内存调整\n",
    "\n",
    "# 用于嵌入的最大训练样本数（可按需调整）\n",
    "# 如果数据集小于此值，则使用所有样本\n",
    "MAX_TRAIN_SAMPLES_FOR_EMBEDDING = 5000\n",
    "SEED = 42 # 确保采样可复现\n",
    "\n",
    "# --- 1. 设备检测 ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    ESM_MODEL_NAME = ESM_MODEL_NAME_GPU\n",
    "    BATCH_SIZE = GPU_BATCH_SIZE\n",
    "    print(\"检测到CUDA GPU，将使用GPU进行计算。\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    ESM_MODEL_NAME = ESM_MODEL_NAME_CPU\n",
    "    BATCH_SIZE = CPU_BATCH_SIZE\n",
    "    print(\"未检测到CUDA GPU或CUDA不可用，将使用CPU进行计算。\")\n",
    "\n",
    "print(f\"正在使用设备: {DEVICE}\")\n",
    "print(f\"将加载的ESM模型: {ESM_MODEL_NAME}\")\n",
    "print(f\"将使用的批次大小: {BATCH_SIZE}\")\n",
    "\n",
    "\n",
    "# --- 3.0（优化）必要时对训练数据进行采样 ---\n",
    "print(\"\\n正在检查训练数据大小以进行采样...\")\n",
    "# 直接使用已加载的 avGFP_train_df\n",
    "if len(avGFP_train_df) > MAX_TRAIN_SAMPLES_FOR_EMBEDDING:\n",
    "    print(f\"训练数据大小 ({len(avGFP_train_df)}) 超过限制 ({MAX_TRAIN_SAMPLES_FOR_EMBEDDING})。正在采样...\")\n",
    "    # 对DataFrame进行采样以减少用于嵌入的序列数量\n",
    "    sampled_train_df = avGFP_train_df.sample(n=MAX_TRAIN_SAMPLES_FOR_EMBEDDING, random_state=SEED)\n",
    "    print(f\"使用 {len(sampled_train_df)} 个采样序列进行嵌入。\")\n",
    "else:\n",
    "    print(f\"训练数据大小 ({len(avGFP_train_df)}) 在限制范围内。使用所有序列。\")\n",
    "    # 使用完整的DataFrame\n",
    "    sampled_train_df = avGFP_train_df.copy()  # 使用副本以避免修改原始数据\n",
    "\n",
    "# --- 3.1 加载适合选定设备的ESM模型 ---\n",
    "print(f\"\\n正在加载ESM模型: {ESM_MODEL_NAME} 到设备 {DEVICE}...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # 加载模型和字母表\n",
    "    esm_model, alphabet = esm.pretrained.load_model_and_alphabet(ESM_MODEL_NAME)\n",
    "    esm_model.eval()  # 设置为评估模式\n",
    "    esm_model = esm_model.to(DEVICE)  # 将模型移动到选定的设备 (GPU or CPU)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    print(f\"ESM模型在 {time.time() - start_time:.2f} 秒内成功加载。\")\n",
    "except Exception as e:\n",
    "    print(f\"加载ESM模型时出错: {e}\")\n",
    "    print(\"请确保已安装 'fair-esm' 库 (pip install fair-esm) 且模型名称正确。\")\n",
    "    print(\"对于GPU使用，请确保已安装与CUDA兼容的PyTorch版本。\")\n",
    "    exit()  # 无法加载模型则退出\n",
    "\n",
    "# --- 3.2 定义嵌入函数（适用于CPU和GPU） ---\n",
    "def get_esm_embeddings(sequences, model, alphabet, batch_converter, device, batch_size):\n",
    "    \"\"\"\n",
    "    在指定设备(CPU或GPU)上生成ESM嵌入（平均池化）。\n",
    "    将输入数据和模型都移动到 `device`。\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    num_sequences = len(sequences)\n",
    "    num_batches = (num_sequences + batch_size - 1) // batch_size\n",
    "    model.eval() # 确保模型在评估模式\n",
    "    model = model.to(device) # 确保模型在目标设备\n",
    "\n",
    "    print(f\"正在为 {num_sequences} 个序列生成嵌入，共 {num_batches} 个批次（批次大小: {batch_size}，设备: {device}）...\")\n",
    "    start_time_embed = time.time()\n",
    "\n",
    "    with torch.no_grad():  # 对推理速度和内存至关重要\n",
    "        for i in range(0, num_sequences, batch_size):\n",
    "            batch_seqs = sequences[i:i + batch_size]\n",
    "            batch_labels = [f\"seq_{j + i}\" for j in range(len(batch_seqs))]  # 每个批次项的唯一标签\n",
    "            data = list(zip(batch_labels, batch_seqs))\n",
    "\n",
    "            try:\n",
    "                # 1. 准备批次\n",
    "                _, _, batch_tokens = batch_converter(data)\n",
    "                # 将令牌移动到目标设备 (GPU or CPU)\n",
    "                batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "                # 2. 获取表示（只需要最后一层）\n",
    "                # results = model(batch_tokens, repr_layers=[model.num_layers], return_contacts=False) # 旧版写法\n",
    "                # 适配新版 fair-esm (>=2.0.0)\n",
    "                results = model(batch_tokens, repr_layers=[model.num_layers])\n",
    "                token_representations = results[\"representations\"][model.num_layers]\n",
    "\n",
    "                # 3. 对序列长度进行平均池化（忽略CLS/EOS/PAD令牌）\n",
    "                # token_representations形状: [batch_size, seq_len+2, embed_dim]\n",
    "                seq_repr_list = []\n",
    "                for j, seq in enumerate(batch_seqs):\n",
    "                    # +1 是因为 batch_converter 添加了 <cls> token\n",
    "                    actual_len = len(seq)\n",
    "                    # 从索引 1 开始到 actual_len 结束 (不包括 <eos> 和 padding)\n",
    "                    # 注意：需要确保batch_converter没有添加其他的特殊token影响长度计算\n",
    "                    # 对于标准esm模型，通常是<cls>...seq...<eos><pad>...\n",
    "                    # 所以 token_representations[j, 1 : actual_len + 1, :] 是正确的\n",
    "                    seq_tokens_repr = token_representations[j, 1 : actual_len + 1, :]\n",
    "                    seq_repr = seq_tokens_repr.mean(dim=0) # 对实际序列长度的表示进行平均\n",
    "                    seq_repr_list.append(seq_repr)\n",
    "\n",
    "                batch_seq_repr = torch.stack(seq_repr_list, dim=0) # [batch_size, embed_dim]\n",
    "\n",
    "                # 4. 存储结果 (移回CPU以聚合和后续处理，如转Numpy)\n",
    "                embeddings.append(batch_seq_repr.cpu())\n",
    "\n",
    "                if (i // batch_size + 1) % 10 == 0 or (i // batch_size + 1) == num_batches:  # 每10个批次或最后一个批次打印进度\n",
    "                    elapsed_time = time.time() - start_time_embed\n",
    "                    print(f\"  已处理批次 {i // batch_size + 1}/{num_batches}... (耗时: {elapsed_time:.2f} 秒)\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e) and device == torch.device(\"cuda\"):\n",
    "                    print(f\"\\n处理批次 {i // batch_size + 1} 时发生CUDA内存不足错误: {e}\")\n",
    "                    print(f\"当前批次大小: {batch_size}。尝试减小批次大小或使用更小的模型。\")\n",
    "                    # 记录错误并跳过，但会导致数据丢失\n",
    "                    embed_dim = model.embed_dim\n",
    "                    error_placeholder = torch.full((len(batch_seqs), embed_dim), float('nan'), device='cpu') # 存放在CPU\n",
    "                    embeddings.append(error_placeholder)\n",
    "                    torch.cuda.empty_cache() # 尝试释放一些内存\n",
    "                else:\n",
    "                    print(f\"处理批次 {i // batch_size + 1} 时发生运行时错误: {e}\")\n",
    "                    embed_dim = model.embed_dim\n",
    "                    error_placeholder = torch.full((len(batch_seqs), embed_dim), float('nan'), device='cpu')\n",
    "                    embeddings.append(error_placeholder)\n",
    "            except Exception as e:\n",
    "                print(f\"处理批次 {i // batch_size + 1} 时发生未知错误: {e}\")\n",
    "                # 处理错误，例如跳过批次或用NaN填充\n",
    "                embed_dim = model.embed_dim  # 获取预期维度\n",
    "                error_placeholder = torch.full((len(batch_seqs), embed_dim), float('nan'), device='cpu')\n",
    "                embeddings.append(error_placeholder)\n",
    "\n",
    "\n",
    "    total_embed_time = time.time() - start_time_embed\n",
    "    print(f\"嵌入生成在 {total_embed_time:.2f} 秒内完成。\")\n",
    "    if num_sequences > 0:\n",
    "      print(f\"平均每个序列耗时: {total_embed_time / num_sequences:.4f} 秒\")\n",
    "\n",
    "\n",
    "    if not embeddings:\n",
    "        return torch.tensor([])  # 返回空张量\n",
    "\n",
    "    # 连接所有批次的结果\n",
    "    try:\n",
    "        full_embeddings = torch.cat(embeddings, dim=0)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"连接嵌入批次时出错: {e}\")\n",
    "        print(\"这可能发生在批次处理出错导致维度不匹配时。请检查之前的错误信息。\")\n",
    "        # 尝试找出有效批次并连接，或者返回错误\n",
    "        try:\n",
    "            embed_dim = model.embed_dim # 获取模型维度\n",
    "            valid_embeddings = [emb for emb in embeddings if isinstance(emb, torch.Tensor) and emb.ndim == 2 and emb.shape[1] == embed_dim and not torch.isnan(emb).all()]\n",
    "            if valid_embeddings:\n",
    "                print(\"尝试仅连接有效的嵌入批次...\")\n",
    "                full_embeddings = torch.cat(valid_embeddings, dim=0)\n",
    "            else:\n",
    "                print(\"没有有效的嵌入批次可以连接。\")\n",
    "                return torch.tensor([]) # 返回空张量\n",
    "        except Exception as concat_err:\n",
    "             print(f\"尝试连接有效嵌入时再次出错: {concat_err}\")\n",
    "             return torch.tensor([]) # 返回空张量\n",
    "\n",
    "\n",
    "    return full_embeddings  # 作为单个张量返回 (在 CPU 上)\n",
    "\n",
    "# --- 3.3 为（可能采样的）训练数据生成嵌入 ---\n",
    "train_sequences_to_embed = sampled_train_df['full_sequence'].tolist()\n",
    "\n",
    "X = None # 初始化 X\n",
    "y = None # 初始化 y\n",
    "\n",
    "if train_sequences_to_embed:\n",
    "    # 获取嵌入作为PyTorch张量\n",
    "    train_embeddings_tensor = get_esm_embeddings(\n",
    "        train_sequences_to_embed,\n",
    "        esm_model,\n",
    "        alphabet,\n",
    "        batch_converter,\n",
    "        DEVICE,            # 传递检测到的设备\n",
    "        batch_size=BATCH_SIZE # 传递适合设备的批次大小\n",
    "    )\n",
    "\n",
    "    if train_embeddings_tensor.numel() > 0: # 检查张量是否为空\n",
    "        print(f\"生成的嵌入张量形状: {train_embeddings_tensor.shape}\")\n",
    "\n",
    "        # 将张量转换为numpy数组以与scikit-learn兼容\n",
    "        # 因为上面 append 时已经 .cpu()，所以这里 tensor 已经在 CPU 上\n",
    "        train_embeddings_np = train_embeddings_tensor.numpy()\n",
    "\n",
    "        # --- 处理嵌入过程中可能出现的NaN值 ---\n",
    "        nan_rows_mask = np.isnan(train_embeddings_np).any(axis=1)\n",
    "        if np.any(nan_rows_mask):\n",
    "            num_nan_rows = nan_rows_mask.sum()\n",
    "            print(f\"警告: 在嵌入中发现 {num_nan_rows} 行NaN值（可能是由于错误）。正在移除这些行及其对应的原始数据。\")\n",
    "\n",
    "            # 过滤嵌入数组和相应的DataFrame行\n",
    "            valid_indices_bool = ~nan_rows_mask\n",
    "            # 获取原始sampled_train_df中对应valid_indices_bool为True的索引\n",
    "            original_indices = sampled_train_df.index[valid_indices_bool]\n",
    "\n",
    "            train_embeddings_np = train_embeddings_np[valid_indices_bool]\n",
    "            # 使用 .loc 和原始索引进行过滤，确保正确对齐\n",
    "            sampled_train_df_filtered = sampled_train_df.loc[original_indices]\n",
    "\n",
    "            print(f\"过滤后的嵌入数据大小: {train_embeddings_np.shape[0]}\")\n",
    "            print(f\"过滤后的DataFrame大小: {len(sampled_train_df_filtered)}\")\n",
    "\n",
    "\n",
    "            # 检查过滤后是否还有数据\n",
    "            if train_embeddings_np.shape[0] == 0:\n",
    "                 print(\"\\n错误: 移除NaN值后没有剩余数据。无法继续。\")\n",
    "                 X, y = None, None\n",
    "            else:\n",
    "                 # --- 为模型训练准备最终的X和y ---\n",
    "                 if train_embeddings_np.shape[0] == len(sampled_train_df_filtered):\n",
    "                     X = train_embeddings_np\n",
    "                     y = sampled_train_df_filtered['Brightness'].values\n",
    "                     print(f\"\\n准备好的X（嵌入）形状: {X.shape}, y（亮度）形状: {y.shape}\")\n",
    "                     print(\"步骤3（嵌入生成）完成。\")\n",
    "                 else:\n",
    "                     # 这个情况理论上不应发生，因为我们同时过滤了两者\n",
    "                     print(f\"\\n错误: NaN过滤后最终嵌入 ({train_embeddings_np.shape[0]}) 和DataFrame行 ({len(sampled_train_df_filtered)}) 数量不匹配。\")\n",
    "                     print(\"无法进行训练。请检查过滤逻辑。\")\n",
    "                     X, y = None, None\n",
    "\n",
    "        else:\n",
    "             # 没有NaN值，直接使用\n",
    "             print(\"嵌入中未检测到NaN值。\")\n",
    "             X = train_embeddings_np\n",
    "             # 直接从 sampled_train_df 获取 y，因为没有过滤\n",
    "             y = sampled_train_df['Brightness'].values\n",
    "             print(f\"\\n准备好的X（嵌入）形状: {X.shape}, y（亮度）形状: {y.shape}\")\n",
    "             print(\"步骤3（嵌入生成）完成。\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n嵌入过程未能生成有效的嵌入张量（可能所有批次都出错或输入为空）。\")\n",
    "        X, y = None, None\n",
    "\n",
    "else:\n",
    "    print(\"\\n采样/预处理后没有可用的序列进行嵌入。\")\n",
    "    X, y = None, None\n",
    "\n",
    "# --- 清理（可选，有助于释放内存，特别是GPU内存） ---\n",
    "print(\"\\n正在清理内存...\")\n",
    "del esm_model, alphabet, batch_converter\n",
    "# 删除可能已创建的张量和numpy数组\n",
    "if 'train_embeddings_tensor' in locals() and isinstance(train_embeddings_tensor, torch.Tensor):\n",
    "    del train_embeddings_tensor\n",
    "if 'train_embeddings_np' in locals():\n",
    "    del train_embeddings_np\n",
    "# 删除采样或过滤后的DataFrame副本\n",
    "if 'sampled_train_df' in locals():\n",
    "    del sampled_train_df\n",
    "if 'sampled_train_df_filtered' in locals():\n",
    "     del sampled_train_df_filtered\n",
    "\n",
    "# 如果使用了GPU，显式清空缓存\n",
    "if DEVICE == torch.device(\"cuda\"):\n",
    "    print(\"清空CUDA缓存...\")\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"清理完成。\")\n",
    "\n",
    "# --- 现在X和y（如果成功创建）已准备好用于步骤4（模型训练） ---\n",
    "if X is not None and y is not None:\n",
    "     print(f\"\\n数据准备就绪，可以进行模型训练。X shape: {X.shape}, y shape: {y.shape}\")\n",
    "     # 这里可以接上你的模型训练代码，例如：\n",
    "     # from sklearn.model_selection import train_test_split\n",
    "     # from sklearn.ensemble import RandomForestRegressor\n",
    "     # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "     # model = RandomForestRegressor(n_estimators=100, random_state=SEED)\n",
    "     # model.fit(X_train, y_train)\n",
    "     # print(\"模型训练完成。\")\n",
    "     # score = model.score(X_test, y_test)\n",
    "     # print(f\"模型在测试集上的 R^2 分数: {score:.4f}\")\n",
    "else:\n",
    "     print(\"\\n数据准备失败，无法进行模型训练。请检查之前的日志输出。\")\n",
    "\n",
    "# 示例: 如果X不为None，则打印(X.shape, y.shape)，否则打印(\"X is None\")\n",
    "# print(\"\\n--- 最终检查 ---\")\n",
    "# print((X.shape, y.shape) if X is not None else \"X is None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083f863-80c4-40a2-9a17-1f4c56bbf545",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "🎓 **第四步：训练我们的亮度预测器！(模型训练 - 随机森林)**\n",
    "\n",
    "现在，我们有了蛋白质的“数字指纹”（X，来自 ESM 嵌入）和它们对应的已知“亮度分数”（y）。是时候训练一个模型，让它学会根据指纹预测亮度了！\n",
    "\n",
    "1.  **分班考试 📝:** 我们会把数据分成两部分：大部分（训练集）用来“教”模型学习规律，一小部分（验证集）用来“测试”模型学得怎么样，防止它“死记硬背”。\n",
    "2.  **请“随机森林”老师 🌳🌳🌳:** 我们选择“随机森林” (Random Forest) 作为我们的预测模型。你可以把它想象成很多棵决策树组成的“智囊团”，它们各自学习，然后投票决定最终的预测结果。这种方法通常很稳健，效果也不错。\n",
    "3.  **开始学习 🧑‍💻:** 用训练集的数据，“喂”给随机森林模型，让它努力学习从 ESM 指纹到亮度值的映射关系。\n",
    "4.  **模拟测验 ✅:** 训练完成后，用我们之前留出的验证集来考考模型，看看它的预测准确度（比如用 R² 分数评估）怎么样。这能帮我们了解这个“亮度预测器”靠不靠谱。\n",
    "\n",
    "训练完成后，我们就得到了一个可以预测未知序列亮度的模型啦！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7870f396-3653-413c-b418-2ff0b5fbd616",
   "metadata": {},
   "outputs": [
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_88",
     "meta": {},
     "name": "stdout",
     "output_type": "stream",
     "parent_header": {
      "date": "2025-04-27T14:14:57.726075Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_88",
      "msg_type": "stream",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     },
     "text": [
      "Split data into training (4000) and validation (1000) sets.\n",
      "\n",
      "Training Random Forest Regressor...\n",
      "Random Forest training complete.\n",
      "\n",
      "Model Performance on Validation Set:\n",
      "  R-squared (R²): 0.2793\n"
     ]
    },
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_90",
     "meta": {
      "dependencies_met": true,
      "engine": "91221703-bed9-4a3d-953a-9ea6a097dcc2",
      "started": "2025-04-27T14:14:57.548572Z",
      "status": "ok"
     },
     "output_type": "execute_reply",
     "parent_header": {
      "date": "2025-04-27T14:16:01.711135Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_90",
      "msg_type": "execute_reply",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     }
    }
   ],
   "source": [
    "# --- 4.1 划分训练集和验证集 ---\n",
    "# 如果数据量较少，可以考虑交叉验证，这里用简单的划分\n",
    "if len(X) > 10: # 确保有足够数据划分\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "    print(f\"Split data into training ({len(X_train)}) and validation ({len(X_val)}) sets.\")\n",
    "else:\n",
    "    print(\"Dataset too small for validation split, using all data for training.\")\n",
    "    X_train, y_train = X, y\n",
    "    X_val, y_val = None, None # 没有验证集\n",
    "\n",
    "# --- 4.2 初始化并训练随机森林模型 ---\n",
    "print(\"\\nTraining Random Forest Regressor...\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100, # 树的数量，可以调整\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1, # 使用所有可用的 CPU 核心\n",
    "    max_depth=20, # 限制树的深度，防止过拟合 (可调整)\n",
    "    min_samples_leaf=3 # 叶节点最小样本数 (可调整)\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Random Forest training complete.\")\n",
    "\n",
    "# --- 4.3 (可选) 评估模型性能 ---\n",
    "if X_val is not None:\n",
    "    y_pred_val = rf_model.predict(X_val)\n",
    "    r2 = r2_score(y_val, y_pred_val)\n",
    "    print(f\"\\nModel Performance on Validation Set:\")\n",
    "    print(f\"  R-squared (R²): {r2:.4f}\")\n",
    "    # R² 接近 1 表示模型拟合得较好，接近 0 或负数表示拟合很差\n",
    "else:\n",
    "    # 可以在训练集上评估，但这通常会过于乐观\n",
    "    y_pred_train = rf_model.predict(X_train)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    print(\"\\nModel Performance on Training Set (may be optimistic):\")\n",
    "    print(f\"  R-squared (R²): {r2_train:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac14d7f9-5fc2-4bd4-97e1-77e1e254e844",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "💡 **第五步：设计新的潜力序列！(候选序列生成 - 知识引导)**\n",
    "\n",
    "激动人心的创造环节来了！我们要设计全新的 avGFP 序列，目标是找到比原始模板更亮的！但我们不瞎猜，而是采取“知识引导”策略：\n",
    "\n",
    "1.  **划定“重点区域” 🎯:** 这一步需要大家发挥聪明才智啦！你需要去查阅文献、分析我们提供的数据（尤其是 `beforetop10` 里的高分序列），甚至看看蛋白质的 3D 结构 (PDB 文件)，找出那些**最有可能影响亮度**的氨基酸位置。把这些位置汇总起来，形成一个“候选位点池”。这就像寻宝前先研究藏宝图！🗺️\n",
    "2.  **组合突变 🧪:** 程序会从你精心挑选的“候选位点池”中，随机选择 1 到 6 个不同的位置。\n",
    "3.  **引入变化 ✨:** 在选中的这几个位置上，随机地将原来的氨基酸替换成其他 19 种氨基酸中的一种。\n",
    "4.  **大量制造 🏭:** 重复步骤 2 和 3 很多很多次（比如几千次），生成大量全新的、带有 1-6 个突变的 avGFP 候选序列。这些序列都是基于你的“知识引导”产生的，希望能更有潜力！\n",
    "\n",
    "这样，我们就得到了一大批等待评估的新设计！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dfc18d4-6665-4758-a8d6-a0b85d5b7f4e",
   "metadata": {},
   "outputs": [
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_175",
     "meta": {},
     "name": "stdout",
     "output_type": "stream",
     "parent_header": {
      "date": "2025-04-27T14:29:58.750205Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_175",
      "msg_type": "stream",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     },
     "text": [
      "\n",
      "Using a candidate position pool of 25 sites (0-based index):\n",
      "\n",
      "Generating 500 candidate sequences...\n",
      "  Generated 50/500 unique candidates...\n",
      "  Generated 100/500 unique candidates...\n",
      "  Generated 150/500 unique candidates...\n",
      "  Generated 200/500 unique candidates...\n",
      "  Generated 250/500 unique candidates...\n",
      "  Generated 300/500 unique candidates...\n",
      "  Generated 350/500 unique candidates...\n",
      "  Generated 400/500 unique candidates...\n",
      "  Generated 450/500 unique candidates...\n",
      "  Generated 500/500 unique candidates...\n",
      "Generated a total of 500 unique candidate sequences.\n"
     ]
    },
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_176",
     "meta": {
      "dependencies_met": true,
      "engine": "91221703-bed9-4a3d-953a-9ea6a097dcc2",
      "started": "2025-04-27T14:29:58.724813Z",
      "status": "ok"
     },
     "output_type": "execute_reply",
     "parent_header": {
      "date": "2025-04-27T14:29:58.751498Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_176",
      "msg_type": "execute_reply",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     }
    }
   ],
   "source": [
    "# --- 5.1 定义候选位点池 (示例) ---\n",
    "# !!! 关键步骤：这个列表应该基于您的研究 !!!\n",
    "# 示例：包含一些文献中提到的稳定/亮度相关位点，以及靠近发色团的位点\n",
    "\n",
    "candidate_position_pool = [\n",
    "    # 靠近发色团 (at 65-67)\n",
    "    64, 68, 69, 70, 71, 72,\n",
    "    # 文献中提到的与稳定/亮度相关的 (示例)\n",
    "    10, 30, 64, # F64L 是 Superfolder GFP 的关键突变之一\n",
    "    101, 105, 109,\n",
    "    145, 147, 153, # M153T 也是 Superfolder GFP 突变\n",
    "    163, 167, # V163A 也是 Superfolder GFP 突变\n",
    "    171, 187,\n",
    "    203, 205, 221, 231, 232, 235\n",
    "]\n",
    "# 转换为 0-based index 用于代码处理\n",
    "candidate_position_pool_0based = [p - 1 for p in candidate_position_pool]\n",
    "print(f\"\\nUsing a candidate position pool of {len(candidate_position_pool)} sites (0-based index):\")\n",
    "# print(candidate_position_pool_0based)\n",
    "\n",
    "# --- 5.2 定义生成候选序列的函数 ---\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY' # 20种标准氨基酸\n",
    "\n",
    "def generate_single_candidate(wt_sequence, position_pool_0based, max_mutations):\n",
    "    \"\"\"生成一个随机组合突变的候选序列\"\"\"\n",
    "    num_mutations = random.randint(1, max_mutations)\n",
    "    # 从池中随机选择 num_mutations 个不同的位置\n",
    "    positions_to_mutate = random.sample(position_pool_0based, num_mutations)\n",
    "\n",
    "    mutated_sequence = list(wt_sequence)\n",
    "    mutation_details = []\n",
    "\n",
    "    for pos in positions_to_mutate:\n",
    "        original_aa = wt_sequence[pos]\n",
    "        # 随机选择一个不同于原始氨基酸的新氨基酸\n",
    "        possible_new_aas = [aa for aa in amino_acids if aa != original_aa]\n",
    "        new_aa = random.choice(possible_new_aas)\n",
    "        mutated_sequence[pos] = new_aa\n",
    "        mutation_details.append(f\"{original_aa}{pos+1}{new_aa}\") # 记录突变 (1-based)\n",
    "\n",
    "    return \"\".join(mutated_sequence), \":\".join(sorted(mutation_details, key=lambda x: int(re.search(r'\\d+', x).group()))) # 按位置排序突变描述\n",
    "\n",
    "# --- 5.3 生成大量候选序列 ---\n",
    "print(f\"\\nGenerating {N_CANDIDATES_TO_GENERATE} candidate sequences...\")\n",
    "candidate_sequences = {} # 使用字典存储 {sequence: mutation_str} 以确保唯一性\n",
    "generated_count = 0\n",
    "attempts = 0\n",
    "max_attempts = N_CANDIDATES_TO_GENERATE * 5 # 设置尝试上限，防止无限循环\n",
    "\n",
    "while generated_count < N_CANDIDATES_TO_GENERATE and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    seq, mut_str = generate_single_candidate(avGFP_WT_sequence, candidate_position_pool_0based, MAX_MUTATIONS)\n",
    "    if seq not in candidate_sequences and seq != avGFP_WT_sequence: # 确保不重复且不是野生型\n",
    "        candidate_sequences[seq] = mut_str\n",
    "        generated_count += 1\n",
    "        if generated_count % (N_CANDIDATES_TO_GENERATE // 10) == 0:\n",
    "            print(f\"  Generated {generated_count}/{N_CANDIDATES_TO_GENERATE} unique candidates...\")\n",
    "\n",
    "if generated_count < N_CANDIDATES_TO_GENERATE:\n",
    "    print(f\"Warning: Could only generate {generated_count} unique candidates after {attempts} attempts.\")\n",
    "\n",
    "candidate_list = list(candidate_sequences.keys())\n",
    "mutation_list = [candidate_sequences[seq] for seq in candidate_list]\n",
    "print(f\"Generated a total of {len(candidate_list)} unique candidate sequences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfcd0e-dd3a-4b0a-90a7-388c0c21d700",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "🔍 **第六步：预测、筛选与淘汰！(预测与过滤)**\n",
    "\n",
    "我们创造了一堆新序列，现在要用之前训练好的“亮度预测器”来给它们打分，并进行严格筛选：\n",
    "\n",
    "1.  **再次“翻译” 🗣️:** 对所有新生成的候选序列，再次使用**相同**的 ESM 模型（那个轻量级的）来计算它们的“数字指纹”（嵌入）。\n",
    "2.  **预测亮度 🔮:** 把这些新序列的指纹输入到我们训练好的随机森林模型中，让模型预测出每一条新序列的亮度值。\n",
    "3.  **对照“黑名单” 🚫:** 这是非常关键的一步！拿出 `Exclusion_List.csv`，检查我们预测出来的高分序列，**绝对不能**出现在这个名单里！如果在名单上，即使预测分数再高，也必须淘汰掉。\n",
    "4.  **排序选优 🏆:** 将所有通过“黑名单”检查的候选序列，按照预测的亮度值从高到低排序。\n",
    "\n",
    "经过这一轮，剩下的就是我们认为最有潜力、并且符合比赛规则的候选序列了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cfd0120-45d3-4f8c-a402-339e9a5fbc86",
   "metadata": {},
   "outputs": [
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_180",
     "meta": {},
     "name": "stdout",
     "output_type": "stream",
     "parent_header": {
      "date": "2025-04-27T14:30:06.462673Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_180",
      "msg_type": "stream",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     },
     "text": [
      "尝试加载用于预测的 ESM 模型: esm2_t30_150M_UR50D\n",
      "用于预测的 ESM 模型 'esm2_t30_150M_UR50D' 已加载到 cuda，耗时 2.48 秒。\n",
      "\n",
      "使用预测模型为候选序列生成 ESM 嵌入...\n",
      "检测到 GPU，使用 GPU 批次大小: 8\n",
      "正在为 500 个序列生成嵌入，共 63 个批次（批次大小: 8，设备: cuda）...\n",
      "  已处理批次 10/63... (耗时: 2.23 秒)\n",
      "  已处理批次 20/63... (耗时: 4.39 秒)\n",
      "  已处理批次 30/63... (耗时: 7.31 秒)\n",
      "  已处理批次 40/63... (耗时: 11.05 秒)\n",
      "  已处理批次 50/63... (耗时: 14.43 秒)\n",
      "  已处理批次 60/63... (耗时: 17.84 秒)\n",
      "  已处理批次 63/63... (耗时: 18.71 秒)\n",
      "嵌入生成在 18.71 秒内完成。\n",
      "平均每个序列耗时: 0.0374 秒\n",
      "候选序列嵌入的形状: (500, 640)\n",
      "\n",
      "正在为候选序列预测亮度...\n",
      "预测完成。\n",
      "\n",
      "根据排除列表 (739 个序列) 进行过滤...\n",
      "移除了 1 个在排除列表中的序列。\n",
      "\n",
      "预测出的 Top 6 个候选序列 (已排除):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>Mutations</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>PredictedBrightness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>V163T</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.514671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Candidate_2</td>\n",
       "      <td>E235T</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.490361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Candidate_3</td>\n",
       "      <td>F71L</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.485293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Candidate_4</td>\n",
       "      <td>V163D</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.483591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Candidate_5</td>\n",
       "      <td>L221T</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.482050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Candidate_6</td>\n",
       "      <td>E235S</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.481949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sequence ID Mutations                                           Sequence  \\\n",
       "328  Candidate_1     V163T  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "496  Candidate_2     E235T  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "479  Candidate_3      F71L  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "412  Candidate_4     V163D  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "179  Candidate_5     L221T  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "82   Candidate_6     E235S  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "\n",
       "     PredictedBrightness  \n",
       "328             3.514671  \n",
       "496             3.490361  \n",
       "479             3.485293  \n",
       "412             3.483591  \n",
       "179             3.482050  \n",
       "82              3.481949  "
      ]
     },
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_189",
     "meta": {},
     "metadata": {},
     "output_type": "display_data",
     "parent_header": {
      "date": "2025-04-27T14:30:27.565040Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_189",
      "msg_type": "display_data",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     }
    },
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_190",
     "meta": {},
     "name": "stdout",
     "output_type": "stream",
     "parent_header": {
      "date": "2025-04-27T14:30:27.650211Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_190",
      "msg_type": "stream",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     },
     "text": [
      "\n",
      "清理预测模型内存...\n",
      "清空CUDA缓存...\n",
      "预测模型清理完成。\n"
     ]
    },
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_191",
     "meta": {
      "dependencies_met": true,
      "engine": "91221703-bed9-4a3d-953a-9ea6a097dcc2",
      "started": "2025-04-27T14:30:06.241240Z",
      "status": "ok"
     },
     "output_type": "execute_reply",
     "parent_header": {
      "date": "2025-04-27T14:30:27.651551Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_191",
      "msg_type": "execute_reply",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     }
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import esm # 假设 esm 库已导入\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 假设 rf_model, candidate_list, mutation_list, exclusion_sequences,\n",
    "# TOP_N_SELECT, CPU_BATCH_SIZE, get_esm_embeddings (来自上一步) 已经定义好\n",
    "# 并且 avGFP_WT_sequence, candidate_position_pool_0based, MAX_MUTATIONS, N_CANDIDATES_TO_GENERATE 也已定义\n",
    "\n",
    "# --- 修正：定义用于预测的 ESM 模型名称 ---\n",
    "# !!! 关键：这里必须使用与训练 rf_model 时相同的 ESM 模型 !!!\n",
    "# 根据之前的日志，rf_model 是用 640 维嵌入训练的 (来自 esm2_t30_150M_UR50D)\n",
    "PREDICTION_ESM_MODEL_NAME = \"esm2_t30_150M_UR50D\" # <--- 确认这个模型与训练时一致\n",
    "\n",
    "print(f\"尝试加载用于预测的 ESM 模型: {PREDICTION_ESM_MODEL_NAME}\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # 加载指定用于预测的 ESM 模型和字母表\n",
    "    esm_model_pred, alphabet_pred = esm.pretrained.load_model_and_alphabet(PREDICTION_ESM_MODEL_NAME)\n",
    "    batch_converter_pred = alphabet_pred.get_batch_converter()\n",
    "    # 重新确定设备 (优先使用 GPU)\n",
    "    DEVICE_pred = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    esm_model_pred.to(DEVICE_pred)\n",
    "    esm_model_pred.eval() # 设置模型为评估模式 (关闭 dropout 等)\n",
    "    print(f\"用于预测的 ESM 模型 '{PREDICTION_ESM_MODEL_NAME}' 已加载到 {DEVICE_pred}，耗时 {time.time() - start_time:.2f} 秒。\")\n",
    "except Exception as e:\n",
    "    print(f\"加载 ESM 模型 {PREDICTION_ESM_MODEL_NAME} 时出错: {e}\")\n",
    "    print(\"请确保 'fair-esm' 已安装，模型名称正确，并且有足够的内存。\")\n",
    "    # 根据需要处理错误，例如退出\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 6.1 为候选序列生成 ESM 嵌入 ---\n",
    "print(\"\\n使用预测模型为候选序列生成 ESM 嵌入...\")\n",
    "candidate_embeddings_np = np.array([]) # 初始化为空 numpy 数组\n",
    "\n",
    "if candidate_list:\n",
    "    # 确定批次大小 (如果使用GPU，可以尝试更大的批次)\n",
    "    prediction_batch_size = CPU_BATCH_SIZE # 默认使用 CPU 批次大小 (来自之前的定义)\n",
    "    if DEVICE_pred == torch.device(\"cuda\"):\n",
    "        # 如果是GPU，可以尝试更大的批次，例如 8, 16 或 32，取决于GPU内存和模型大小\n",
    "        # 对于较大的模型如 150M，可能需要较小的批次大小\n",
    "        prediction_batch_size = 8 # 示例值 (根据你的 GPU 内存调整，与训练时用的GPU_BATCH_SIZE可以不同)\n",
    "        print(f\"检测到 GPU，使用 GPU 批次大小: {prediction_batch_size}\")\n",
    "    else:\n",
    "        print(f\"使用 CPU 批次大小: {prediction_batch_size}\")\n",
    "\n",
    "    # *** 修复点：使用正确的函数名 'get_esm_embeddings' ***\n",
    "    # 传递用于预测的模型、字母表、转换器和设备\n",
    "    candidate_embeddings_tensor = get_esm_embeddings( # <-- 使用正确的函数名\n",
    "        candidate_list,\n",
    "        esm_model_pred,         # 使用预测模型\n",
    "        alphabet_pred,        # 使用预测模型的字母表\n",
    "        batch_converter_pred, # 使用预测模型的转换器\n",
    "        DEVICE_pred,          # 使用预测模型所在的设备 (可能是 cuda 或 cpu)\n",
    "        batch_size=prediction_batch_size # 使用调整后的批次大小\n",
    "    )\n",
    "\n",
    "    # 将嵌入结果转换为 NumPy 数组以用于 scikit-learn 模型\n",
    "    # .cpu() 确保数据在 CPU 上，然后转换为 numpy\n",
    "    candidate_embeddings_np = candidate_embeddings_tensor.cpu().numpy()\n",
    "\n",
    "    print(f\"候选序列嵌入的形状: {candidate_embeddings_np.shape}\") # 检查维度是否正确 (应为 N x 640)\n",
    "\n",
    "    # 检查是否有 NaN (如果嵌入过程中出错)\n",
    "    if np.isnan(candidate_embeddings_np).any():\n",
    "        print(\"警告：在候选序列嵌入中发现 NaN 值。将移除相应的候选序列。\")\n",
    "        nan_mask = np.isnan(candidate_embeddings_np).any(axis=1)\n",
    "        # 需要同时过滤 candidate_list, mutation_list 和 embeddings\n",
    "        # 使用列表推导式进行过滤\n",
    "        original_count = len(candidate_list)\n",
    "        candidate_list = [seq for i, seq in enumerate(candidate_list) if not nan_mask[i]]\n",
    "        mutation_list = [mut for i, mut in enumerate(mutation_list) if not nan_mask[i]]\n",
    "        candidate_embeddings_np = candidate_embeddings_np[~nan_mask]\n",
    "        print(f\"因 NaN 嵌入移除了 {original_count - len(candidate_list)} 个候选序列。\")\n",
    "        print(f\"剩余候选序列数量: {len(candidate_list)}\")\n",
    "        print(f\"过滤后的候选序列嵌入形状: {candidate_embeddings_np.shape}\")\n",
    "\n",
    "else:\n",
    "    # candidate_embeddings_np 已经初始化为空数组\n",
    "    print(\"上一步没有生成候选序列，跳过预测。\")\n",
    "\n",
    "\n",
    "# --- 6.2 预测亮度 ---\n",
    "predicted_brightness = []\n",
    "# 确保我们有有效的嵌入和候选列表来进行预测\n",
    "# 并且嵌入的数量与候选列表的数量一致\n",
    "if candidate_embeddings_np.shape[0] > 0 and len(candidate_list) == candidate_embeddings_np.shape[0]:\n",
    "    print(\"\\n正在为候选序列预测亮度...\")\n",
    "    try:\n",
    "        # 使用加载的随机森林模型进行预测\n",
    "        predicted_brightness = rf_model.predict(candidate_embeddings_np)\n",
    "        print(\"预测完成。\")\n",
    "    except ValueError as ve: # 捕获更具体的 ValueError\n",
    "        print(f\"随机森林模型预测期间出错: {ve}\")\n",
    "        print(\"请确认用于预测的 ESM 模型生成的特征维度与训练 rf_model 时使用的维度一致。\")\n",
    "        # 如果预测失败，将结果列表清空，后续步骤会处理空结果\n",
    "        predicted_brightness = []\n",
    "    except Exception as e:\n",
    "        print(f\"随机森林模型预测期间发生未知错误: {e}\")\n",
    "        predicted_brightness = []\n",
    "else:\n",
    "    if not candidate_list:\n",
    "         print(\"没有可用的候选序列进行预测。\")\n",
    "    elif candidate_embeddings_np.shape[0] == 0 and candidate_list:\n",
    "         print(\"生成了候选序列，但未能生成有效的嵌入向量进行预测。\")\n",
    "    elif len(candidate_list) != candidate_embeddings_np.shape[0]:\n",
    "         print(\"错误：经过 NaN 过滤后，候选序列数量与嵌入向量数量不匹配。\")\n",
    "\n",
    "\n",
    "# --- 6.3 组合结果并筛选 ---\n",
    "final_candidates_formatted = pd.DataFrame() # 初始化为空 DataFrame\n",
    "\n",
    "# 只有在成功生成预测并且数量与候选列表匹配时才继续\n",
    "if len(candidate_list) > 0 and len(predicted_brightness) > 0 and len(candidate_list) == len(predicted_brightness):\n",
    "    # 创建包含序列、突变和预测值的 DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Sequence': candidate_list,\n",
    "        'Mutations': mutation_list, # 确保 mutation_list 与 candidate_list 保持同步\n",
    "        'PredictedBrightness': predicted_brightness\n",
    "    })\n",
    "\n",
    "    # 过滤掉排除列表中的序列\n",
    "    print(f\"\\n根据排除列表 ({len(exclusion_sequences)} 个序列) 进行过滤...\")\n",
    "    initial_candidate_count = len(results_df)\n",
    "    # 确保比较的是字符串类型\n",
    "    results_df = results_df[~results_df['Sequence'].astype(str).isin(exclusion_sequences)]\n",
    "    removed_count = initial_candidate_count - len(results_df)\n",
    "    if removed_count > 0:\n",
    "        print(f\"移除了 {removed_count} 个在排除列表中的序列。\")\n",
    "    else:\n",
    "        print(\"候选列表中的序列均不在排除列表中。\")\n",
    "\n",
    "    # 按预测亮度降序排序\n",
    "    results_df = results_df.sort_values(by='PredictedBrightness', ascending=False)\n",
    "\n",
    "    # 选择 Top N 个结果\n",
    "    final_candidates = results_df.head(TOP_N_SELECT).copy() # 使用 .copy() 避免 SettingWithCopyWarning\n",
    "\n",
    "    print(f\"\\n预测出的 Top {min(TOP_N_SELECT, len(final_candidates))} 个候选序列 (已排除):\") # 显示实际选出的数量\n",
    "\n",
    "    if not final_candidates.empty:\n",
    "        # 为了更清晰地展示，可以添加一个 ID 列\n",
    "        final_candidates.insert(0, 'Sequence ID', [f'Candidate_{i+1}' for i in range(len(final_candidates))])\n",
    "        # 调整列顺序以符合提交格式要求\n",
    "        final_candidates_formatted = final_candidates[['Sequence ID', 'Mutations', 'Sequence', 'PredictedBrightness']]\n",
    "        # 使用 display 或 print 显示 DataFrame\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(final_candidates_formatted) # 在 Jupyter 环境中友好显示\n",
    "        except ImportError:\n",
    "            print(final_candidates_formatted.to_string()) # 在非 IPython 环境中打印完整 DataFrame\n",
    "    else:\n",
    "        print(\"经过过滤和筛选后，没有剩余的候选序列。\")\n",
    "\n",
    "elif not candidate_list:\n",
    "     print(\"\\n没有生成候选序列或候选序列在预测前已被过滤掉。\")\n",
    "elif not predicted_brightness:\n",
    "     print(\"\\n预测步骤失败或没有产生结果。请检查之前的错误信息。\")\n",
    "else: # candidate_list 和 predicted_brightness 长度不匹配\n",
    "     print(\"\\n错误：最终候选序列数量与预测结果数量不匹配。无法继续处理。\")\n",
    "\n",
    "# --- 清理预测模型占用的内存 ---\n",
    "print(\"\\n清理预测模型内存...\")\n",
    "del esm_model_pred, alphabet_pred, batch_converter_pred\n",
    "if 'candidate_embeddings_tensor' in locals():\n",
    "    del candidate_embeddings_tensor\n",
    "if 'candidate_embeddings_np' in locals():\n",
    "    del candidate_embeddings_np\n",
    "if DEVICE_pred == torch.device(\"cuda\"):\n",
    "    print(\"清空CUDA缓存...\")\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"预测模型清理完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef106f37-c167-4796-87f9-ae024a2c06b1",
   "metadata": {},
   "source": [
    "---\n",
    "📄 **第七步：成果展示与未来展望！(输出与后续步骤)**\n",
    "\n",
    "太棒了！我们终于走完了整个流程！🎉\n",
    "\n",
    "1.  **最终名单 🏅:** 从上一轮筛选排序后的结果中，选出预测亮度最高的 **Top 6** 条序列。\n",
    "2.  **整理提交 ✍️:** 将这 6 条序列的信息（比如给它们起个名字 `Sequence ID`、记录清楚具体的突变 `Mutations`、以及完整的序列 `Sequence`）整理成比赛要求的 `.csv` 文件格式。\n",
    "3.  **下一步行动 🤔:** 这个教程只是一个起点！要在比赛中获得好成绩，你还需要：\n",
    "    *   **深入研究 📚:** 花更多时间优化你的“候选位点池”，让它更精准！\n",
    "    *   **平衡稳定性 🔥:** 这次我们主要关注了亮度，但比赛还要求**热稳定性**！你需要思考如何引入提高稳定性的策略（比如参考 Superfolder GFP 的突变，分析 PDB 结构寻找可以优化的点）。这可能是个多目标优化的挑战！\n",
    "    *   **升级装备 🚀:** 可以尝试不同的机器学习模型、更仔细地调整模型参数，甚至探索更高级的技术。\n",
    "    *   **打包文档 📦:** 别忘了按要求整理好你的代码 (`.zip`)，并写一份清晰的“设计思路”文档 (`.docx` 或 `.pdf`)，解释你是怎么做的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": " ",
   "id": "c4c1695e-85c9-4fe7-82f3-c058d201f856",
   "metadata": {},
   "outputs": [
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_105",
     "meta": {
      "dependencies_met": true,
      "engine": "91221703-bed9-4a3d-953a-9ea6a097dcc2",
      "started": "2025-04-27T14:16:06.076223Z",
      "status": "aborted"
     },
     "output_type": "execute_reply",
     "parent_header": {
      "date": "2025-04-27T14:16:06.076240Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_105",
      "msg_type": "execute_reply",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     }
    }
   ],
   "source": [
    "# --- 7.1 准备提交格式 (示例) ---\n",
    "# 比赛要求提交 CSV，包含 'Sequence ID', 'Mutations', 'Full Sequence'\n",
    "# 我们已经有了类似格式的 DataFrame 'final_candidates_formatted'\n",
    "\n",
    "# 如果需要保存为 CSV 文件：\n",
    "output_filename = \"my_top_brightness_candidates.csv\"\n",
    "if not final_candidates_formatted.empty:\n",
    "    # 选择需要的列\n",
    "    submission_df = final_candidates_formatted[['Sequence ID', 'Mutations', 'Sequence']].copy()\n",
    "    # 重命名列以完全匹配（如果需要）\n",
    "    # submission_df.rename(columns={'Sequence': 'Full Sequence'}, inplace=True) # 假设需要 'Full Sequence' 列名\n",
    "    submission_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nSuccessfully saved top {len(submission_df)} candidates to {output_filename}\")\n",
    "else:\n",
    "    print(\"\\nNo final candidates to save.\")\n",
    "\n",
    "\n",
    "# --- 7.2 后续步骤 --- \n",
    "print(\"\\n--- 教程已完成 ---\") \n",
    "print(\"后续可能的优化步骤：\") \n",
    "print(\"1.  **优化位置池：** 目前使用的数据量和批次非常小，您可以进行全面的文献/数据/结构分析，以创建更好的`候选位置池`。\") \n",
    "print(\"2.  **考虑稳定性：** 本教程仅关注亮度。你需要纳入预测/提高热稳定性的策略或模型（例如，使用PDB结构、已知的稳定突变）。这可能涉及多目标优化。\") \n",
    "print(\"3.  **改进模型：** 尝试不同的ESM模型、回归算法（例如梯度提升）、超参数调整，或更先进的技术，如微调ESM。\") \n",
    "print(\"4.  **代码打包：** 根据要求将代码整理成可运行的脚本或包（.zip）。包含一个README文件。\") \n",
    "print(\"5.  **设计原理：** 撰写一份清晰的文档，解释你的方法、选择特定位置/突变的原因以及所使用的方法。\") \n",
    "print(\"6.  **提交：** 准备最终的CSV文件，其中准确包含6条序列，确保它们不在排除列表中且符合突变限制。\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc0e68a-329d-45ba-b3c6-40097eeb330c",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8972765c-db3d-4639-a8f3-3eb1926e7cc1",
   "metadata": {},
   "source": [
    "以下演示了其它方案 使用Saprot模型用于向量嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776a8f4-625c-4192-b42b-889c3dedff91",
   "metadata": {},
   "source": [
    "---\n",
    "🧠 **(备选) 第三步 B：让 SaProt 读懂蛋白质语言！(特征工程 - SaProt 嵌入)**\n",
    "\n",
    "除了 ESM，还有其他强大的蛋白质语言模型，例如 SaProt。SaProt 通常基于 Transformer 架构，并通过 Hugging Face 的 `transformers` 库加载。\n",
    "\n",
    "与之前使用的轻量级 ESM 模型相比，SaProt 模型（如 `ECAS/SaProt_650M_AF2`）参数量可能更大，理论上可能捕捉更丰富的信息，但也需要更多的计算资源（内存和时间），尤其是在 CPU 上。\n",
    "\n",
    "下面我们将展示如何加载 SaProt 并用它来生成嵌入。你可以选择性地用这些嵌入替换前面 ESM 生成的嵌入，用于后续的模型训练和评估。\n",
    "\n",
    "**注意:** 下一步需要安装 `transformers` 库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da53f72c-bf19-4fe9-b768-fad94ce17e96",
   "metadata": {},
   "outputs": [
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_201",
     "meta": {},
     "name": "stdout",
     "output_type": "stream",
     "parent_header": {
      "date": "2025-04-27T14:30:37.706136Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_201",
      "msg_type": "stream",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     },
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: transformers in /opt/miniconda/lib/python3.7/site-packages (4.30.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/miniconda/lib/python3.7/site-packages (0.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda/lib/python3.7/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: importlib-metadata in /opt/miniconda/lib/python3.7/site-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/miniconda/lib/python3.7/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: requests in /opt/miniconda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/miniconda/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/miniconda/lib/python3.7/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /opt/miniconda/lib/python3.7/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/miniconda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/miniconda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/miniconda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n"
     ]
    },
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_207",
     "meta": {
      "dependencies_met": true,
      "engine": "91221703-bed9-4a3d-953a-9ea6a097dcc2",
      "started": "2025-04-27T14:30:35.774968Z",
      "status": "ok"
     },
     "output_type": "execute_reply",
     "parent_header": {
      "date": "2025-04-27T14:30:39.488146Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_207",
      "msg_type": "execute_reply",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     }
    }
   ],
   "source": [
    "# 安装 Hugging Face Transformers 库\n",
    "!pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eee8a40c-f568-43c1-a672-869e76a9bbe4",
   "metadata": {},
   "outputs": [
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_211",
     "meta": {},
     "name": "stdout",
     "output_type": "stream",
     "parent_header": {
      "date": "2025-04-27T14:30:40.035015Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_211",
      "msg_type": "stream",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     },
     "text": [
      "HF_ENDPOINT 未设置或非预期值。将尝试设置 Hugging Face 端点为镜像: https://hf-mirror.com\n",
      "当前脚本执行环境中 HF_ENDPOINT 已设置为: https://hf-mirror.com\n",
      "--- 开始 SaProt 工作流程 ---\n",
      "指定使用的 SaProt 模型 (来自镜像): westlake-repl/SaProt_35M_AF2\n",
      "检测到 CUDA GPU，将使用 GPU 进行 SaProt 计算。\n",
      "GPU 总显存: 7.86 GB\n",
      "最终使用的 SaProt 设备: cuda\n",
      "最终使用的 SaProt 模型: westlake-repl/SaProt_35M_AF2\n",
      "最终使用的 SaProt 批次大小: 32\n",
      "\n",
      "[步骤 1/9] 检查依赖变量/函数... 通过。\n",
      "\n",
      "[步骤 2/9] 正在加载 SaProt Tokenizer 和模型: westlake-repl/SaProt_35M_AF2 到 cuda...\n",
      "Some weights of the model checkpoint at westlake-repl/SaProt_35M_AF2 were not used when initializing EsmModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at westlake-repl/SaProt_35M_AF2 and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "SaProt 模型加载并在 cuda 上准备就绪，耗时 3.71 秒。\n",
      "\n",
      "[步骤 3/9] 正在定义 SaProt 嵌入函数...\n",
      "SaProt 嵌入函数定义完成。\n",
      "\n",
      "[步骤 4/9] 正在为训练数据生成 SaProt 嵌入...\n",
      "  训练数据量 (51715) 较大，将采样 5000 条用于嵌入和训练。\n",
      "  准备为 5000 个序列生成 SaProt 嵌入，共 157 批 (批大小: 32)...\n",
      "    已处理批次 15/157 (9.6%) - 耗时: 3.85 秒\n",
      "    已处理批次 30/157 (19.1%) - 耗时: 9.62 秒\n",
      "    已处理批次 45/157 (28.7%) - 耗时: 15.70 秒\n",
      "    已处理批次 60/157 (38.2%) - 耗时: 21.73 秒\n",
      "    已处理批次 75/157 (47.8%) - 耗时: 27.76 秒\n",
      "    已处理批次 90/157 (57.3%) - 耗时: 33.78 秒\n",
      "    已处理批次 105/157 (66.9%) - 耗时: 39.82 秒\n",
      "    已处理批次 120/157 (76.4%) - 耗时: 45.88 秒\n",
      "    已处理批次 135/157 (86.0%) - 耗时: 51.90 秒\n",
      "    已处理批次 150/157 (95.5%) - 耗时: 57.92 秒\n",
      "    已处理批次 157/157 (100.0%) - 耗时: 60.44 秒\n",
      "  SaProt 嵌入生成完成，总耗时: 60.44 秒。\n",
      "  平均每个序列耗时: 0.0121 秒。\n",
      "  生成的训练数据嵌入张量形状: torch.Size([5000, 480])\n",
      "  准备好的 X_saprot (嵌入) 形状: (5000, 480), y_saprot (亮度) 形状: (5000,)\n",
      "\n",
      "[步骤 5/9] 正在使用 SaProt 嵌入训练随机森林回归器...\n",
      "  已将数据分割为训练集 (4000) 和验证集 (1000)。\n",
      "  开始训练随机森林模型...\n",
      "  随机森林训练完成，耗时 0.43 秒。\n",
      "  基于 SaProt 的模型在【验证集】上的 R² 分数: -0.0043\n",
      "\n",
      "[步骤 6/9] 正在生成 500 个候选序列...\n",
      "  已生成 50/500 个唯一候选序列...\n",
      "  已生成 100/500 个唯一候选序列...\n",
      "  已生成 150/500 个唯一候选序列...\n",
      "  已生成 200/500 个唯一候选序列...\n",
      "  已生成 250/500 个唯一候选序列...\n",
      "  已生成 300/500 个唯一候选序列...\n",
      "  已生成 350/500 个唯一候选序列...\n",
      "  已生成 400/500 个唯一候选序列...\n",
      "  已生成 450/500 个唯一候选序列...\n",
      "  已生成 500/500 个唯一候选序列...\n",
      "  成功生成 500 个独特的候选序列。\n",
      "  候选序列生成耗时: 0.01 秒。\n",
      "\n",
      "[步骤 7/9] 正在为候选序列生成 SaProt 嵌入...\n",
      "  将为 500 个候选序列生成嵌入。\n",
      "  准备为 500 个序列生成 SaProt 嵌入，共 16 批 (批大小: 32)...\n",
      "    已处理批次 1/16 (6.2%) - 耗时: 0.25 秒\n",
      "    已处理批次 2/16 (12.5%) - 耗时: 0.50 秒\n",
      "    已处理批次 3/16 (18.8%) - 耗时: 0.76 秒\n",
      "    已处理批次 4/16 (25.0%) - 耗时: 1.06 秒\n",
      "    已处理批次 5/16 (31.2%) - 耗时: 1.44 秒\n",
      "    已处理批次 6/16 (37.5%) - 耗时: 1.85 秒\n",
      "    已处理批次 7/16 (43.8%) - 耗时: 2.28 秒\n",
      "    已处理批次 8/16 (50.0%) - 耗时: 2.71 秒\n",
      "    已处理批次 9/16 (56.2%) - 耗时: 3.13 秒\n",
      "    已处理批次 10/16 (62.5%) - 耗时: 3.55 秒\n",
      "    已处理批次 11/16 (68.8%) - 耗时: 3.98 秒\n",
      "    已处理批次 12/16 (75.0%) - 耗时: 4.38 秒\n",
      "    已处理批次 13/16 (81.2%) - 耗时: 4.79 秒\n",
      "    已处理批次 14/16 (87.5%) - 耗时: 5.20 秒\n",
      "    已处理批次 15/16 (93.8%) - 耗时: 5.60 秒\n",
      "    已处理批次 16/16 (100.0%) - 耗时: 5.85 秒\n",
      "  SaProt 嵌入生成完成，总耗时: 5.85 秒。\n",
      "  平均每个序列耗时: 0.0117 秒。\n",
      "  生成的候选序列嵌入张量形状: torch.Size([500, 480])\n",
      "\n",
      "[步骤 8/9] 正在使用基于 SaProt 的随机森林模型预测候选序列的亮度...\n",
      "  亮度预测完成，耗时 0.10 秒。\n",
      "  成功预测了 500 个候选序列的亮度。\n",
      "\n",
      "[步骤 9/9] 正在过滤、选择 Top N 候选序列并保存结果...\n",
      "  已创建包含 500 个候选序列及其预测亮度的 DataFrame。\n",
      "  正在根据排除列表 (739 条) 进行过滤...\n",
      "  移除了 1 个在排除列表中的序列。\n",
      "  过滤后剩余候选序列: 499 条。\n",
      "\n",
      "  筛选出的 Top 6 (最多 6) 个 SaProt 预测候选 (已排除):\n",
      "--- SaProt Top Candidates ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>Mutations</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>PredictedBrightness_SaProt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SaProt_Candidate_1</td>\n",
       "      <td>C70I</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>2.650868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>SaProt_Candidate_2</td>\n",
       "      <td>S30Q:N105Q:M153P:V163E:T203S:L221D</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVQGEGEGDATYGKLTLKF...</td>\n",
       "      <td>2.650868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>SaProt_Candidate_3</td>\n",
       "      <td>V68K:K101H:N105T:T203Y:L221V:H231E</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>2.650868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>SaProt_Candidate_4</td>\n",
       "      <td>S30F:F71G:K101M:N105S:M153T</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVFGEGEGDATYGKLTLKF...</td>\n",
       "      <td>2.650868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>SaProt_Candidate_5</td>\n",
       "      <td>Y145A</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>2.650868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>SaProt_Candidate_6</td>\n",
       "      <td>G10F:N105I:Y145T:S147Y:I171P</td>\n",
       "      <td>MSKGEELFTFVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>2.650868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sequence ID                           Mutations  \\\n",
       "0    SaProt_Candidate_1                                C70I   \n",
       "328  SaProt_Candidate_2  S30Q:N105Q:M153P:V163E:T203S:L221D   \n",
       "341  SaProt_Candidate_3  V68K:K101H:N105T:T203Y:L221V:H231E   \n",
       "340  SaProt_Candidate_4         S30F:F71G:K101M:N105S:M153T   \n",
       "339  SaProt_Candidate_5                               Y145A   \n",
       "338  SaProt_Candidate_6        G10F:N105I:Y145T:S147Y:I171P   \n",
       "\n",
       "                                              Sequence  \\\n",
       "0    MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "328  MSKGEELFTGVVPILVELDGDVNGHKFSVQGEGEGDATYGKLTLKF...   \n",
       "341  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "340  MSKGEELFTGVVPILVELDGDVNGHKFSVFGEGEGDATYGKLTLKF...   \n",
       "339  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "338  MSKGEELFTFVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "\n",
       "     PredictedBrightness_SaProt  \n",
       "0                      2.650868  \n",
       "328                    2.650868  \n",
       "341                    2.650868  \n",
       "340                    2.650868  \n",
       "339                    2.650868  \n",
       "338                    2.650868  "
      ]
     },
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_245",
     "meta": {},
     "metadata": {},
     "output_type": "display_data",
     "parent_header": {
      "date": "2025-04-27T14:31:51.223730Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_245",
      "msg_type": "display_data",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     }
    },
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_246",
     "meta": {},
     "name": "stdout",
     "output_type": "stream",
     "parent_header": {
      "date": "2025-04-27T14:31:51.227887Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_246",
      "msg_type": "stream",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     },
     "text": [
      "----------------------------\n",
      "\n",
      "  已成功将 Top 6 个 SaProt 候选序列保存到文件: my_top_saprot_candidates.csv\n",
      "\n",
      "--- SaProt 工作流程结束 ---\n"
     ]
    },
    {
     "id": "9c1f9c87-33eef1a094312e02050a72e7_374_247",
     "meta": {
      "dependencies_met": true,
      "engine": "91221703-bed9-4a3d-953a-9ea6a097dcc2",
      "started": "2025-04-27T14:30:39.757053Z",
      "status": "ok"
     },
     "output_type": "execute_reply",
     "parent_header": {
      "date": "2025-04-27T14:31:51.229008Z",
      "msg_id": "9c1f9c87-33eef1a094312e02050a72e7_374_247",
      "msg_type": "execute_reply",
      "session": "9c1f9c87-33eef1a094312e02050a72e7",
      "username": "username",
      "version": "5.3"
     }
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --- SaProt 完整工作流程单元 ---\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# === 在导入 transformers 之前设置 Hugging Face 镜像 ===\n",
    "# 检查环境变量是否已设置，如果未设置，则使用镜像\n",
    "# 这样做可以最大程度确保 transformers 库在初始化时就使用镜像地址\n",
    "hf_endpoint = os.environ.get('HF_ENDPOINT')\n",
    "mirror_endpoint = 'https://hf-mirror.com' # 指定镜像地址\n",
    "\n",
    "if hf_endpoint != mirror_endpoint: # 仅在未设置或设置不正确时进行设置\n",
    "    print(f\"HF_ENDPOINT 未设置或非预期值。将尝试设置 Hugging Face 端点为镜像: {mirror_endpoint}\")\n",
    "    os.environ['HF_ENDPOINT'] = mirror_endpoint\n",
    "    # 验证是否设置成功 (可能需要重启内核/环境才能完全生效，但在脚本内尽力设置)\n",
    "    current_endpoint = os.environ.get('HF_ENDPOINT')\n",
    "    if current_endpoint == mirror_endpoint:\n",
    "        print(f\"当前脚本执行环境中 HF_ENDPOINT 已设置为: {current_endpoint}\")\n",
    "    else:\n",
    "        print(f\"警告: 尝试设置 HF_ENDPOINT，但读取值仍为 {current_endpoint}。环境变量可能未完全生效。\")\n",
    "        # 如果脚本内设置无效，可能需要在外部环境（如启动脚本或系统环境变量）中设置\n",
    "else:\n",
    "    print(f\"检测到 HF_ENDPOINT 已设置为镜像: {hf_endpoint}，将继续使用此地址。\")\n",
    "\n",
    "# === 1. 导入库与基础设置 ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "# 尝试导入 transformers (现在应该会使用设置的 HF_ENDPOINT)\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "except ImportError:\n",
    "    print(\"错误: 无法导入 transformers 库。请确保已正确安装 (pip install transformers)。\")\n",
    "    sys.exit(1) # 无法导入则退出\n",
    "\n",
    "# 在Jupyter Notebook中优化DataFrame的显示\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    # 如果不在IPython环境中，定义一个简单的display函数\n",
    "    def display(x):\n",
    "        print(x)\n",
    "\n",
    "warnings.filterwarnings('ignore') # 忽略不影响结果的警告\n",
    "\n",
    "print(\"--- 开始 SaProt 工作流程 ---\")\n",
    "\n",
    "# === 2. 安装依赖 (注释掉，建议在环境级别管理) ===\n",
    "# print(\"\\n[步骤 1/9] 正在安装所需库 (transformers, sentencepiece)...\")\n",
    "# !pip install transformers sentencepiece -q # -q 表示静默安装\n",
    "# print(\"库安装检查完成。\")\n",
    "\n",
    "\n",
    "# === 3. SaProt 常量设置与设备检测 ===\n",
    "\n",
    "# --- 模型名称 (来自镜像) ---\n",
    "SAPROT_MODEL_NAME = \"westlake-repl/SaProt_35M_AF2\"\n",
    "print(f\"指定使用的 SaProt 模型 (来自镜像): {SAPROT_MODEL_NAME}\")\n",
    "\n",
    "# --- 批次大小设置 ---\n",
    "SAPROT_BATCH_SIZE_CPU = 4\n",
    "SAPROT_BATCH_SIZE_GPU = 32 # 35M 模型通常允许较大的批次\n",
    "\n",
    "# --- 设备检测 ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE_SAPROT = torch.device(\"cuda\")\n",
    "    SAPROT_BATCH_SIZE = SAPROT_BATCH_SIZE_GPU\n",
    "    print(\"检测到 CUDA GPU，将使用 GPU 进行 SaProt 计算。\")\n",
    "    try:\n",
    "        gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        print(f\"GPU 总显存: {gpu_mem_gb:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"无法获取GPU内存信息: {e}\")\n",
    "else:\n",
    "    DEVICE_SAPROT = torch.device(\"cpu\")\n",
    "    SAPROT_BATCH_SIZE = SAPROT_BATCH_SIZE_CPU\n",
    "    print(\"未检测到 CUDA GPU 或 CUDA 不可用，将使用 CPU 进行 SaProt 计算。\")\n",
    "\n",
    "# --- 最终确认打印 ---\n",
    "print(f\"最终使用的 SaProt 设备: {DEVICE_SAPROT}\")\n",
    "print(f\"最终使用的 SaProt 模型: {SAPROT_MODEL_NAME}\")\n",
    "print(f\"最终使用的 SaProt 批次大小: {SAPROT_BATCH_SIZE}\")\n",
    "\n",
    "\n",
    "# === 检查先前单元格定义的必要变量/函数是否存在 ===\n",
    "# 确保运行此单元格前，已成功运行定义这些变量和函数的单元格\n",
    "# !!! 注意: 请确保在运行此脚本/单元格之前，这些变量已经在您的环境中定义 !!!\n",
    "required_vars = ['avGFP_train_df', 'avGFP_WT_sequence', 'exclusion_sequences',\n",
    "                 'MAX_TRAIN_SAMPLES_FOR_EMBEDDING', 'SEED', 'candidate_position_pool_0based',\n",
    "                 'amino_acids', 'MAX_MUTATIONS', 'N_CANDIDATES_TO_GENERATE', 'TOP_N_SELECT',\n",
    "                 'generate_single_candidate']\n",
    "missing = [var for var in required_vars if not (var in locals() or var in globals())]\n",
    "if missing:\n",
    "    print(\"\\n错误: 未找到所有必需的变量或函数。\")\n",
    "    print(f\"请确保已首先运行定义了以下内容的单元格: {', '.join(missing)}\")\n",
    "    raise NameError(f\"缺少来自先前笔记本单元格的必需变量: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\n[步骤 1/9] 检查依赖变量/函数... 通过。\")\n",
    "\n",
    "\n",
    "# === 4. 加载 SaProt Tokenizer 和模型 ===\n",
    "# 现在调用 from_pretrained 时，应该使用脚本顶部设置的环境变量\n",
    "print(f\"\\n[步骤 2/9] 正在加载 SaProt Tokenizer 和模型: {SAPROT_MODEL_NAME} 到 {DEVICE_SAPROT}...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # 直接调用，依赖于顶部的环境变量设置\n",
    "    saprot_tokenizer = AutoTokenizer.from_pretrained(SAPROT_MODEL_NAME)\n",
    "    saprot_model = AutoModel.from_pretrained(SAPROT_MODEL_NAME)\n",
    "\n",
    "    # --- 优化: 尝试使用 torch.compile (保持不变) ---\n",
    "    use_torch_compile = False\n",
    "    if DEVICE_SAPROT == torch.device(\"cuda\") and hasattr(torch, 'compile'):\n",
    "        try:\n",
    "            print(\"  尝试使用 torch.compile 优化模型 (可能需要一些时间)...\")\n",
    "            # 'reduce-overhead' 模式通常适用于推理\n",
    "            saprot_model = torch.compile(saprot_model, mode=\"reduce-overhead\", fullgraph=True)\n",
    "            print(\"  torch.compile 应用成功！\")\n",
    "            use_torch_compile = True\n",
    "        except Exception as compile_err:\n",
    "            print(f\"  torch.compile 失败: {compile_err}。将使用未优化的模型。\")\n",
    "\n",
    "    # --- 模型设置与设备转移 (保持不变) ---\n",
    "    saprot_model.eval() # 设置为评估模式\n",
    "    saprot_model = saprot_model.to(DEVICE_SAPROT) # 将模型参数移动到 GPU 或 CPU\n",
    "\n",
    "    print(f\"SaProt 模型加载并在 {DEVICE_SAPROT} 上准备就绪，耗时 {time.time() - start_time:.2f} 秒。\")\n",
    "\n",
    "except OSError as e:\n",
    "    # 捕获 OSError，它通常包含连接或文件未找到的错误\n",
    "    print(f\"\\n!!! 加载模型时发生 OSError: {e}\")\n",
    "    print(\"这通常意味着：\")\n",
    "    print(\"  1. 网络连接问题：无法访问 HF_ENDPOINT 指定的地址。\")\n",
    "    print(f\"     - 检查镜像地址 '{os.environ.get('HF_ENDPOINT')}' 是否可访问。\")\n",
    "    print(f\"     - 检查你的网络连接和防火墙设置。\")\n",
    "    print(\"  2. 模型名称错误或镜像上不存在该模型。\")\n",
    "    print(f\"     - 确认模型 '{SAPROT_MODEL_NAME}' 在镜像站点上确实存在。\")\n",
    "    print(\"  3. 缓存问题：本地缓存可能已损坏或不完整。\")\n",
    "    print(\"     - 尝试清除 Hugging Face 缓存 (通常在 ~/.cache/huggingface/hub 或 ~/.cache/huggingface/transformers)。\")\n",
    "    print(\"  4. 环境变量未生效：如果看到错误仍然指向 huggingface.co，可能需要重启 Python 内核/环境或在外部设置环境变量。\")\n",
    "    # 可以选择停止执行\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    # 捕获其他可能的异常\n",
    "    print(f\"\\n!!! 加载 SaProt 模型时发生未知错误: {e}\")\n",
    "    print(f\"使用的模型标识符: {SAPROT_MODEL_NAME}\")\n",
    "    print(f\"使用的镜像端点: {os.environ.get('HF_ENDPOINT')}\")\n",
    "    raise # 重新引发异常，停止执行\n",
    "\n",
    "\n",
    "# === 5. 定义 SaProt 嵌入函数 ===\n",
    "print(\"\\n[步骤 3/9] 正在定义 SaProt 嵌入函数...\")\n",
    "def get_saprot_embeddings(sequences, model, tokenizer, device, batch_size, use_compile=False):\n",
    "    \"\"\"\n",
    "    使用 SaProt 模型为序列列表生成嵌入 (采用平均池化)。\n",
    "    针对 GPU 进行了优化，自动处理设备转移和批处理。\n",
    "    \"\"\"\n",
    "    embeddings = [] # 用于存储每个批次的嵌入结果\n",
    "    num_sequences = len(sequences)\n",
    "    if num_sequences == 0:\n",
    "        print(\"  输入序列列表为空，无需生成嵌入。\")\n",
    "        return torch.tensor([])\n",
    "\n",
    "    num_batches = (num_sequences + batch_size - 1) // batch_size\n",
    "    print(f\"  准备为 {num_sequences} 个序列生成 SaProt 嵌入，共 {num_batches} 批 (批大小: {batch_size})...\")\n",
    "    start_time_embed = time.time()\n",
    "\n",
    "    model.eval() # 再次确保是评估模式\n",
    "\n",
    "    with torch.no_grad(): # 优化推理速度和内存\n",
    "        for i in range(0, num_sequences, batch_size):\n",
    "            batch_seqs = sequences[i:i + batch_size]\n",
    "            batch_seqs_spaced = [\" \".join(list(s)) for s in batch_seqs] # SaProt 需要空格分隔\n",
    "            current_batch_num = i // batch_size + 1\n",
    "\n",
    "            try:\n",
    "                inputs = tokenizer(batch_seqs_spaced, add_special_tokens=True, padding=True, truncation=True, return_tensors='pt')\n",
    "                inputs = {key: val.to(device) for key, val in inputs.items()} # 数据移动到 GPU/CPU\n",
    "                outputs = model(**inputs) # 模型推理\n",
    "                last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "                # 平均池化 (考虑 attention mask)\n",
    "                mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "                sum_hidden_states = torch.sum(last_hidden_states * mask, dim=1)\n",
    "                sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "                mean_pooled_embeddings = sum_hidden_states / sum_mask\n",
    "\n",
    "                embeddings.append(mean_pooled_embeddings.cpu()) # 结果移回 CPU\n",
    "\n",
    "                # 打印进度\n",
    "                if current_batch_num % max(1, num_batches // 10) == 0 or current_batch_num == num_batches:\n",
    "                     progress_percent = (current_batch_num / num_batches) * 100\n",
    "                     elapsed = time.time() - start_time_embed\n",
    "                     print(f\"    已处理批次 {current_batch_num}/{num_batches} ({progress_percent:.1f}%) - 耗时: {elapsed:.2f} 秒\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                # 处理内存不足等运行时错误\n",
    "                if \"CUDA out of memory\" in str(e) and device == torch.device(\"cuda\"):\n",
    "                    print(f\"\\n    处理批次 {current_batch_num} 时发生 CUDA 内存不足错误!\")\n",
    "                    print(f\"    当前批次大小: {batch_size}。请尝试减小 'SAPROT_BATCH_SIZE_GPU'。\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    hidden_size = model.config.hidden_size if hasattr(model, 'config') else 768 # 尝试获取维度\n",
    "                    error_placeholder = torch.full((len(batch_seqs), hidden_size), float('nan'), device='cpu')\n",
    "                    embeddings.append(error_placeholder)\n",
    "                else:\n",
    "                    print(f\"    处理 SaProt 批次 {current_batch_num} 时发生运行时错误: {e}\")\n",
    "                    hidden_size = model.config.hidden_size if hasattr(model, 'config') else 768\n",
    "                    error_placeholder = torch.full((len(batch_seqs), hidden_size), float('nan'), device='cpu')\n",
    "                    embeddings.append(error_placeholder)\n",
    "            except Exception as e:\n",
    "                # 处理其他错误\n",
    "                print(f\"    处理 SaProt 批次 {current_batch_num} 时发生未知错误: {e}\")\n",
    "                hidden_size = model.config.hidden_size if hasattr(model, 'config') else 768\n",
    "                error_placeholder = torch.full((len(batch_seqs), hidden_size), float('nan'), device='cpu')\n",
    "                embeddings.append(error_placeholder)\n",
    "\n",
    "    total_embed_time = time.time() - start_time_embed\n",
    "    print(f\"  SaProt 嵌入生成完成，总耗时: {total_embed_time:.2f} 秒。\")\n",
    "    if num_sequences > 0:\n",
    "        print(f\"  平均每个序列耗时: {total_embed_time / num_sequences:.4f} 秒。\")\n",
    "\n",
    "    # 拼接嵌入结果\n",
    "    if not embeddings: return torch.tensor([])\n",
    "    try:\n",
    "        full_embeddings = torch.cat(embeddings, dim=0)\n",
    "    except Exception as cat_err:\n",
    "        print(f\"  拼接嵌入批次时出错: {cat_err}。尝试拼接有效部分...\")\n",
    "        valid_embeddings = [emb for emb in embeddings if isinstance(emb, torch.Tensor) and emb.ndim == 2 and not torch.isnan(emb).all()]\n",
    "        if valid_embeddings:\n",
    "             try:\n",
    "                 expected_dim = valid_embeddings[0].shape[1]\n",
    "                 valid_embeddings = [emb for emb in valid_embeddings if emb.shape[1] == expected_dim]\n",
    "                 if valid_embeddings:\n",
    "                     full_embeddings = torch.cat(valid_embeddings, dim=0)\n",
    "                 else: return torch.tensor([]) # 没有维度一致的\n",
    "             except Exception: return torch.tensor([]) # 拼接有效部分也失败\n",
    "        else: return torch.tensor([]) # 没有有效的\n",
    "\n",
    "    return full_embeddings\n",
    "\n",
    "print(\"SaProt 嵌入函数定义完成。\")\n",
    "\n",
    "\n",
    "# === 6. 为训练数据生成 SaProt 嵌入 ===\n",
    "print(f\"\\n[步骤 4/9] 正在为训练数据生成 SaProt 嵌入...\")\n",
    "# (采样逻辑保持不变)\n",
    "if len(avGFP_train_df) > MAX_TRAIN_SAMPLES_FOR_EMBEDDING:\n",
    "    print(f\"  训练数据量 ({len(avGFP_train_df)}) 较大，将采样 {MAX_TRAIN_SAMPLES_FOR_EMBEDDING} 条用于嵌入和训练。\")\n",
    "    sampled_train_df_saprot = avGFP_train_df.sample(n=MAX_TRAIN_SAMPLES_FOR_EMBEDDING, random_state=SEED)\n",
    "else:\n",
    "    print(f\"  使用全部 {len(avGFP_train_df)} 条训练数据进行嵌入。\")\n",
    "    sampled_train_df_saprot = avGFP_train_df.copy()\n",
    "\n",
    "train_sequences_saprot = sampled_train_df_saprot['full_sequence'].tolist()\n",
    "X_saprot = None # 初始化为 None\n",
    "y_saprot = None\n",
    "\n",
    "if train_sequences_saprot:\n",
    "    X_saprot_tensor = get_saprot_embeddings(\n",
    "        train_sequences_saprot, saprot_model, saprot_tokenizer, DEVICE_SAPROT,\n",
    "        batch_size=SAPROT_BATCH_SIZE, use_compile=use_torch_compile\n",
    "    )\n",
    "    if X_saprot_tensor.numel() > 0:\n",
    "        print(f\"  生成的训练数据嵌入张量形状: {X_saprot_tensor.shape}\")\n",
    "        X_saprot_np = X_saprot_tensor.numpy()\n",
    "        # 处理 NaN\n",
    "        nan_rows_mask_train = np.isnan(X_saprot_np).any(axis=1)\n",
    "        if np.any(nan_rows_mask_train):\n",
    "            num_nan_rows = nan_rows_mask_train.sum()\n",
    "            print(f\"  警告: 在训练嵌入中发现 {num_nan_rows} 行 NaN 值。将移除它们。\")\n",
    "            X_saprot_np = X_saprot_np[~nan_rows_mask_train]\n",
    "            sampled_train_df_saprot = sampled_train_df_saprot[~nan_rows_mask_train]\n",
    "            print(f\"  移除 NaN 后，剩余训练数据: {len(sampled_train_df_saprot)} 条\")\n",
    "\n",
    "        if X_saprot_np.shape[0] > 0 and X_saprot_np.shape[0] == len(sampled_train_df_saprot):\n",
    "            X_saprot = X_saprot_np\n",
    "            y_saprot = sampled_train_df_saprot['Brightness'].values\n",
    "            print(f\"  准备好的 X_saprot (嵌入) 形状: {X_saprot.shape}, y_saprot (亮度) 形状: {y_saprot.shape}\")\n",
    "        else:\n",
    "            print(\"  错误: 处理 NaN 或嵌入失败后，X 和 y 数据不匹配或为空。\")\n",
    "    else:\n",
    "        print(\"  警告: 未能成功为训练数据生成 SaProt 嵌入。\")\n",
    "else:\n",
    "    print(\"  没有找到用于生成嵌入的训练序列。\")\n",
    "\n",
    "if X_saprot is None or y_saprot is None:\n",
    "    print(\"!!! 错误: 未能准备 SaProt 训练数据 (X_saprot 或 y_saprot 为空)。后续步骤可能失败。\")\n",
    "\n",
    "\n",
    "# === 7. 使用 SaProt 嵌入训练随机森林回归模型 ===\n",
    "print(\"\\n[步骤 5/9] 正在使用 SaProt 嵌入训练随机森林回归器...\")\n",
    "rf_model_saprot = None # 初始化模型变量\n",
    "if X_saprot is not None and y_saprot is not None and X_saprot.shape[0] > 0:\n",
    "    if len(X_saprot) > 10:\n",
    "        X_train_saprot, X_val_saprot, y_train_saprot, y_val_saprot = train_test_split(\n",
    "            X_saprot, y_saprot, test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        print(f\"  已将数据分割为训练集 ({len(X_train_saprot)}) 和验证集 ({len(X_val_saprot)})。\")\n",
    "    else:\n",
    "        print(\"  数据集过小，无法进行验证分割，将使用所有数据进行训练。\")\n",
    "        X_train_saprot, y_train_saprot = X_saprot, y_saprot\n",
    "        X_val_saprot, y_val_saprot = None, None\n",
    "\n",
    "    rf_model_saprot = RandomForestRegressor(\n",
    "        n_estimators=100, random_state=SEED, n_jobs=-1, max_depth=20,\n",
    "        min_samples_leaf=3, oob_score=(X_val_saprot is None)\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    print(\"  开始训练随机森林模型...\")\n",
    "    rf_model_saprot.fit(X_train_saprot, y_train_saprot)\n",
    "    print(f\"  随机森林训练完成，耗时 {time.time() - start_time:.2f} 秒。\")\n",
    "\n",
    "    # 模型评估\n",
    "    if X_val_saprot is not None and y_val_saprot is not None:\n",
    "        y_pred_val_saprot = rf_model_saprot.predict(X_val_saprot)\n",
    "        r2_saprot_val = r2_score(y_val_saprot, y_pred_val_saprot)\n",
    "        print(f\"  基于 SaProt 的模型在【验证集】上的 R² 分数: {r2_saprot_val:.4f}\")\n",
    "    elif hasattr(rf_model_saprot, 'oob_score_') and rf_model_saprot.oob_score_:\n",
    "         print(f\"  基于 SaProt 的模型袋外 (OOB) R² 分数: {rf_model_saprot.oob_score_:.4f}\")\n",
    "    else:\n",
    "        y_pred_train_saprot = rf_model_saprot.predict(X_train_saprot)\n",
    "        r2_saprot_train = r2_score(y_train_saprot, y_pred_train_saprot)\n",
    "        print(f\"  基于 SaProt 的模型在【训练集】上的 R² 分数: {r2_saprot_train:.4f} (注意: 可能偏高)\")\n",
    "else:\n",
    "    print(\"  由于之前的步骤未能成功准备 X_saprot 和 y_saprot，跳过模型训练。\")\n",
    "\n",
    "\n",
    "# === 8. 生成候选序列 ===\n",
    "print(f\"\\n[步骤 6/9] 正在生成 {N_CANDIDATES_TO_GENERATE} 个候选序列...\")\n",
    "candidate_sequences_saprot = {}\n",
    "generated_count = 0\n",
    "attempts = 0\n",
    "max_attempts = N_CANDIDATES_TO_GENERATE * 10\n",
    "start_time_gen = time.time()\n",
    "\n",
    "# !!! 确保 generate_single_candidate 函数在此作用域可用 !!!\n",
    "if 'generate_single_candidate' not in globals() and 'generate_single_candidate' not in locals():\n",
    "     raise NameError(\"函数 'generate_single_candidate' 未定义。请确保它在之前的单元格中已运行。\")\n",
    "\n",
    "while generated_count < N_CANDIDATES_TO_GENERATE and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    try:\n",
    "        seq, mut_str = generate_single_candidate(\n",
    "            avGFP_WT_sequence, candidate_position_pool_0based, MAX_MUTATIONS\n",
    "        )\n",
    "        if seq not in candidate_sequences_saprot and seq != avGFP_WT_sequence:\n",
    "            candidate_sequences_saprot[seq] = mut_str\n",
    "            generated_count += 1\n",
    "            if generated_count % max(1, N_CANDIDATES_TO_GENERATE // 10) == 0 or generated_count == N_CANDIDATES_TO_GENERATE :\n",
    "                 print(f\"  已生成 {generated_count}/{N_CANDIDATES_TO_GENERATE} 个唯一候选序列...\")\n",
    "    except Exception as gen_err:\n",
    "        print(f\"生成候选序列时出错 (尝试 {attempts}): {gen_err}\")\n",
    "        # 可以选择继续或停止\n",
    "        # break\n",
    "\n",
    "gen_duration = time.time() - start_time_gen\n",
    "if generated_count < N_CANDIDATES_TO_GENERATE:\n",
    "    print(f\"  警告: 尝试 {attempts} 次后，仅生成了 {generated_count} 个独特的候选序列 (目标 {N_CANDIDATES_TO_GENERATE})。\")\n",
    "else:\n",
    "    print(f\"  成功生成 {generated_count} 个独特的候选序列。\")\n",
    "print(f\"  候选序列生成耗时: {gen_duration:.2f} 秒。\")\n",
    "\n",
    "candidate_list_saprot = list(candidate_sequences_saprot.keys())\n",
    "mutation_list_saprot = [candidate_sequences_saprot[seq] for seq in candidate_list_saprot]\n",
    "\n",
    "\n",
    "# === 9. 为候选序列生成 SaProt 嵌入 ===\n",
    "print(\"\\n[步骤 7/9] 正在为候选序列生成 SaProt 嵌入...\")\n",
    "candidate_embeddings_saprot_np = np.array([]) # 初始化\n",
    "if candidate_list_saprot:\n",
    "    print(f\"  将为 {len(candidate_list_saprot)} 个候选序列生成嵌入。\")\n",
    "    candidate_embeddings_saprot_tensor = get_saprot_embeddings(\n",
    "        candidate_list_saprot, saprot_model, saprot_tokenizer, DEVICE_SAPROT,\n",
    "        batch_size=SAPROT_BATCH_SIZE, use_compile=use_torch_compile\n",
    "    )\n",
    "    if candidate_embeddings_saprot_tensor.numel() > 0:\n",
    "        print(f\"  生成的候选序列嵌入张量形状: {candidate_embeddings_saprot_tensor.shape}\")\n",
    "        candidate_embeddings_saprot_np = candidate_embeddings_saprot_tensor.numpy()\n",
    "        # 处理 NaN\n",
    "        nan_mask_candidates = np.isnan(candidate_embeddings_saprot_np).any(axis=1)\n",
    "        if np.any(nan_mask_candidates):\n",
    "            num_nan_candidates = nan_mask_candidates.sum()\n",
    "            print(f\"  警告: 在候选嵌入中发现 {num_nan_candidates} 个 NaN 值。将移除它们。\")\n",
    "            original_count = len(candidate_list_saprot)\n",
    "            valid_indices = ~nan_mask_candidates\n",
    "            candidate_embeddings_saprot_np = candidate_embeddings_saprot_np[valid_indices]\n",
    "            # 确保列表也使用相同的布尔索引或等效逻辑进行过滤\n",
    "            candidate_list_saprot = [seq for i, seq in enumerate(candidate_list_saprot) if valid_indices[i]]\n",
    "            mutation_list_saprot = [mut for i, mut in enumerate(mutation_list_saprot) if valid_indices[i]]\n",
    "            print(f\"  移除了 {original_count - len(candidate_list_saprot)} 个因 NaN 嵌入被移除的候选序列。\")\n",
    "            print(f\"  剩余有效候选序列数量: {len(candidate_list_saprot)}\")\n",
    "            if candidate_embeddings_saprot_np.shape[0] > 0:\n",
    "                print(f\"  过滤后的候选嵌入形状: {candidate_embeddings_saprot_np.shape}\")\n",
    "            else:\n",
    "                print(\"  过滤后没有剩余的有效候选嵌入。\")\n",
    "    else:\n",
    "        print(\"  警告: 未能成功为候选序列生成 SaProt 嵌入。\")\n",
    "else:\n",
    "    print(\"  上一步未能生成任何候选序列，跳过嵌入步骤。\")\n",
    "\n",
    "if candidate_embeddings_saprot_np.shape[0] == 0 and candidate_list_saprot:\n",
    "    print(\"!!! 错误: 有候选序列但未能生成有效的嵌入。后续步骤将失败。\")\n",
    "\n",
    "\n",
    "# === 10. 预测候选序列的亮度 ===\n",
    "print(\"\\n[步骤 8/9] 正在使用基于 SaProt 的随机森林模型预测候选序列的亮度...\")\n",
    "predicted_brightness_saprot = [] # 保持初始化为列表\n",
    "if rf_model_saprot is not None and candidate_embeddings_saprot_np.shape[0] > 0:\n",
    "    if len(candidate_list_saprot) == candidate_embeddings_saprot_np.shape[0]:\n",
    "        try:\n",
    "            start_time_pred = time.time()\n",
    "            # .predict() 返回 NumPy array\n",
    "            predicted_brightness_saprot = rf_model_saprot.predict(candidate_embeddings_saprot_np)\n",
    "            pred_duration = time.time() - start_time_pred\n",
    "            print(f\"  亮度预测完成，耗时 {pred_duration:.2f} 秒。\")\n",
    "            print(f\"  成功预测了 {len(predicted_brightness_saprot)} 个候选序列的亮度。\")\n",
    "        except Exception as e:\n",
    "            print(f\"  使用 SaProt 随机森林模型进行预测时出错: {e}\")\n",
    "            predicted_brightness_saprot = [] # 出错时重置为空列表\n",
    "    else:\n",
    "        print(f\"  错误: 候选序列列表 ({len(candidate_list_saprot)}) 与其嵌入 ({candidate_embeddings_saprot_np.shape[0]}) 数量不匹配。无法预测。\")\n",
    "elif rf_model_saprot is None:\n",
    "     print(\"  随机森林模型未训练，无法预测。\")\n",
    "elif not candidate_list_saprot:\n",
    "     print(\"  没有候选序列可供预测。\")\n",
    "elif candidate_embeddings_saprot_np.shape[0] == 0:\n",
    "     print(\"  没有有效的候选序列 SaProt 嵌入可用于预测。\")\n",
    "\n",
    "\n",
    "# === 11. 过滤、选择 Top N 候选并保存结果 ===\n",
    "print(\"\\n[步骤 9/9] 正在过滤、选择 Top N 候选序列并保存结果...\")\n",
    "final_candidates_saprot_formatted = pd.DataFrame() # 初始化\n",
    "\n",
    "# --- FIX: 使用 len() 检查 predicted_brightness_saprot ---\n",
    "if candidate_list_saprot and len(predicted_brightness_saprot) > 0 and len(candidate_list_saprot) == len(predicted_brightness_saprot):\n",
    "# --- End FIX ---\n",
    "    results_saprot_df = pd.DataFrame({\n",
    "        'Sequence': candidate_list_saprot,\n",
    "        'Mutations': mutation_list_saprot,\n",
    "        'PredictedBrightness_SaProt': predicted_brightness_saprot # NumPy array 可以直接放入DataFrame\n",
    "    })\n",
    "    print(f\"  已创建包含 {len(results_saprot_df)} 个候选序列及其预测亮度的 DataFrame。\")\n",
    "\n",
    "    # 过滤排除列表\n",
    "    exclusion_set = set(exclusion_sequences)\n",
    "    initial_candidate_count = len(results_saprot_df)\n",
    "    print(f\"  正在根据排除列表 ({len(exclusion_set)} 条) 进行过滤...\")\n",
    "    results_saprot_df = results_saprot_df[~results_saprot_df['Sequence'].isin(exclusion_set)]\n",
    "    removed_count = initial_candidate_count - len(results_saprot_df)\n",
    "    if removed_count > 0: print(f\"  移除了 {removed_count} 个在排除列表中的序列。\")\n",
    "    else: print(\"  候选序列均不在排除列表中。\")\n",
    "    print(f\"  过滤后剩余候选序列: {len(results_saprot_df)} 条。\")\n",
    "\n",
    "    if not results_saprot_df.empty:\n",
    "        results_saprot_df = results_saprot_df.sort_values(by='PredictedBrightness_SaProt', ascending=False)\n",
    "        final_candidates_saprot = results_saprot_df.head(TOP_N_SELECT).copy()\n",
    "        num_selected = len(final_candidates_saprot)\n",
    "        print(f\"\\n  筛选出的 Top {num_selected} (最多 {TOP_N_SELECT}) 个 SaProt 预测候选 (已排除):\")\n",
    "\n",
    "        if num_selected > 0:\n",
    "            final_candidates_saprot.insert(0, 'Sequence ID', [f'SaProt_Candidate_{i+1}' for i in range(num_selected)])\n",
    "            final_candidates_saprot_formatted = final_candidates_saprot[['Sequence ID', 'Mutations', 'Sequence', 'PredictedBrightness_SaProt']]\n",
    "\n",
    "            print(\"--- SaProt Top Candidates ---\")\n",
    "            display(final_candidates_saprot_formatted)\n",
    "            print(\"----------------------------\")\n",
    "\n",
    "            submission_df_saprot = final_candidates_saprot_formatted[['Sequence ID', 'Mutations', 'Sequence']].copy()\n",
    "            output_filename_saprot = \"my_top_saprot_candidates.csv\"\n",
    "            try:\n",
    "                submission_df_saprot.to_csv(output_filename_saprot, index=False)\n",
    "                print(f\"\\n  已成功将 Top {len(submission_df_saprot)} 个 SaProt 候选序列保存到文件: {output_filename_saprot}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n  保存结果到 CSV 文件时出错: {e}\")\n",
    "        else:\n",
    "            print(\"  经过滤和排序后，没有符合条件的 Top N SaProt 候选序列。\")\n",
    "    else:\n",
    "        print(\"  根据排除列表过滤后，没有剩余的候选序列。\")\n",
    "\n",
    "# --- 更新这里的 elif 条件以匹配 if ---\n",
    "elif not candidate_list_saprot:\n",
    "     print(\"  由于没有生成或有效过滤候选序列，无法进行最终选择和保存。\")\n",
    "elif not (len(predicted_brightness_saprot) > 0): # 匹配 if 条件的检查方式\n",
    "     print(\"  由于 SaProt 亮度预测失败或未产生结果 (predicted_brightness_saprot 为空)，无法进行最终选择和保存。\")\n",
    "else: # 这个分支对应 len(candidate_list_saprot) != len(predicted_brightness_saprot)\n",
    "     print(\"  错误: 最终候选序列列表与预测结果数量不匹配。无法进行最终选择和保存。\")\n",
    "\n",
    "\n",
    "# === 12. 清理内存 (可选) ===\n",
    "# print(\"\\n--- 正在清理 SaProt 模型和相关张量以释放内存 ---\")\n",
    "# try:\n",
    "#     # Safely delete variables if they exist\n",
    "#     vars_to_delete = ['saprot_model', 'saprot_tokenizer', 'rf_model_saprot',\n",
    "#                       'X_saprot_tensor', 'X_saprot', 'y_saprot',\n",
    "#                       'X_train_saprot', 'y_train_saprot', 'X_val_saprot', 'y_val_saprot',\n",
    "#                       'candidate_embeddings_saprot_tensor', 'candidate_embeddings_saprot_np',\n",
    "#                       'results_saprot_df', 'final_candidates_saprot', 'final_candidates_saprot_formatted',\n",
    "#                       'submission_df_saprot']\n",
    "#     for var_name in vars_to_delete:\n",
    "#         # Use try-except for deletion as variables might be local or global\n",
    "#         try:\n",
    "#             if var_name in locals(): del locals()[var_name]\n",
    "#             if var_name in globals(): del globals()[var_name]\n",
    "#         except NameError:\n",
    "#             pass # Variable already deleted or never existed\n",
    "\n",
    "#     # 如果使用了 GPU，清空 PyTorch 的 CUDA 缓存\n",
    "#     if DEVICE_SAPROT == torch.device(\"cuda\"):\n",
    "#         print(\"  正在清空 CUDA 缓存...\")\n",
    "#         torch.cuda.empty_cache()\n",
    "#     print(\"内存清理完成。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"清理过程中出现错误: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- SaProt 工作流程结束 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e604ad8a-07b8-48d0-b745-67c6ac1bebc0",
   "metadata": {},
   "source": [
    "希望这个分步讲解能帮助大家理解整个流程！祝大家在蛋白质设计的探索中玩得开心，并在比赛中取得好成绩！✨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
