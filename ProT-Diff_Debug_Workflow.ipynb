{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77fa30d0",
   "metadata": {},
   "source": [
    "\n",
    "# ProT‑Diff Debug‑First Notebook (ipynb 工作流)\n",
    "\n",
    "这本 **ipynb** 设计成“先能跑‑再做强”的调试流程：先最小闭环（能采样出序列并存表），再逐步切换为论文设置。  \n",
    "> 两种模式：  \n",
    "> - **Quick Debug Mode**：不下载模型，几秒完成端到端（用伪嵌入器和小 U‑Net），确保环境 OK；  \n",
    "> - **Full Mode**：使用 **ProtT5‑XL‑UniRef50** 编码/解码 + 扩散模型，贴合论文（可用 CPU/GPU）。\n",
    "\n",
    "你可以先跑 Debug 模式通关，再切到 Full 模式进行正式训练/采样/筛选。  \n",
    "最后会输出 **`generated_seqs.csv`**（待筛选）与 **`logs/`、`checkpoints/`** 等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d22686",
   "metadata": {},
   "source": [
    "## 1) 环境准备（可重复执行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13520e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -U transformers einops numpy pandas scikit-learn tqdm matplotlib\n",
    "\n",
    "import os, math, json, random, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from transformers import T5EncoderModel, T5ForConditionalGeneration, AutoTokenizer\n",
    "    TRANSFORMERS_OK = True\n",
    "except Exception:\n",
    "    TRANSFORMERS_OK = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE, \"Transformers:\", TRANSFORMERS_OK)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318ff24",
   "metadata": {},
   "source": [
    "## 2) 配置（Debug/Full 两套参数一键切换）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b213c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DiffConfig:\n",
    "    max_len: int = 48\n",
    "    dim: int = 1024\n",
    "    train_steps: int = 2000\n",
    "    sample_steps: int = 200\n",
    "    predict_x0: bool = True\n",
    "    lr: float = 2e-4\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 2\n",
    "    seed: int = 42\n",
    "    uniform_noise_sampling: bool = False\n",
    "    save_every: int = 1\n",
    "\n",
    "    debug_mode: bool = True\n",
    "    debug_dim: int = 128\n",
    "    debug_epochs: int = 1\n",
    "    debug_sample_n: int = 32\n",
    "\n",
    "    sample_n: int = 200\n",
    "    ckpt_path: str = \"checkpoints/diffusion.pt\"\n",
    "\n",
    "cfg = DiffConfig()\n",
    "print(cfg)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d73498",
   "metadata": {},
   "source": [
    "## 3) 数据：占位与加载（把你的训练序列替换进来）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toy_train = [\"GIGKFLKKAKKFGKAFVKILKK\", \"LLKKLLKKLLKKLL\", \"VQGLLKALKKAL\", \"KKLLKLLKLLKLLK\"] * 64\n",
    "\n",
    "def load_txt_lines(path):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "user_train = load_txt_lines(\"data/train.txt\")\n",
    "if user_train:\n",
    "    train_seqs = user_train\n",
    "    print(f\"Loaded {len(train_seqs)} sequences from data/train.txt\")\n",
    "else:\n",
    "    train_seqs = toy_train\n",
    "    print(f\"Using toy set: {len(train_seqs)} sequences\")\n",
    "\n",
    "AA = set(list(\"ACDEFGHIKLMNPQRSTVWY\"))\n",
    "def clean_seq(s):\n",
    "    s = s.strip().upper()\n",
    "    return ''.join([c for c in s if c in AA])[:cfg.max_len]\n",
    "\n",
    "train_seqs = [clean_seq(s) for s in train_seqs if 5 <= len(clean_seq(s)) <= cfg.max_len]\n",
    "print(\"After clean:\", len(train_seqs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78074edb",
   "metadata": {},
   "source": [
    "## 4) 编码器/解码器：Debug 伪嵌入 vs ProtT5‑XL（Full）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956bf13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FakeEmbedder:\n",
    "    def __init__(self, dim=cfg.dim, max_len=cfg.max_len):\n",
    "        self.dim = dim; self.max_len = max_len\n",
    "    def encode(self, seqs):\n",
    "        embs = []\n",
    "        for s in seqs:\n",
    "            L = min(len(s), self.max_len)\n",
    "            rng = np.random.RandomState(abs(hash(s)) % (2**32))\n",
    "            e = rng.randn(L, self.dim).astype(np.float32) * 0.2\n",
    "            pos = np.linspace(0, 1, L, dtype=np.float32)[:, None]\n",
    "            e += pos * 0.1\n",
    "            pad = np.zeros((self.max_len - L, self.dim), dtype=np.float32)\n",
    "            embs.append(np.vstack([e, pad]))\n",
    "        return torch.tensor(np.stack(embs), dtype=torch.float32)\n",
    "\n",
    "    def decode(self, embs):\n",
    "        B, L, D = embs.shape\n",
    "        thr = 0.05\n",
    "        proto = torch.randn(20, D, device=embs.device)\n",
    "        idx2aa = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "        outs = []\n",
    "        for b in range(B):\n",
    "            x = embs[b]\n",
    "            mask = (x.norm(dim=-1) > thr)\n",
    "            x = x[mask]\n",
    "            if x.numel() == 0:\n",
    "                outs.append(\"G\")\n",
    "                continue\n",
    "            sim = torch.matmul(x, proto.T)  # L x 20\n",
    "            ids = sim.argmax(dim=-1).tolist()\n",
    "            outs.append(''.join(idx2aa[i] for i in ids))\n",
    "        return outs\n",
    "\n",
    "class ProtT5Wrapper:\n",
    "    def __init__(self, device=DEVICE):\n",
    "        self.device = device\n",
    "        from transformers import T5EncoderModel, T5ForConditionalGeneration, AutoTokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "        self.encoder = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\").to(device)\n",
    "        self.decoder = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\").to(device)\n",
    "        self.encoder.eval(); self.decoder.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, seqs):\n",
    "        toks = self.tokenizer(seqs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        toks = {k: v.to(self.device) for k, v in toks.items()}\n",
    "        out = self.encoder(**toks).last_hidden_state  # (B, L, 1024)\n",
    "        B, L, D = out.shape\n",
    "        if L > cfg.max_len:\n",
    "            out = out[:, :cfg.max_len, :]\n",
    "            L = cfg.max_len\n",
    "        if L < cfg.max_len:\n",
    "            pad = torch.zeros(B, cfg.max_len - L, D, device=out.device)\n",
    "            out = torch.cat([out, pad], dim=1)\n",
    "        return out.float().cpu()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode(self, embs):\n",
    "        from transformers.modeling_outputs import BaseModelOutput\n",
    "        B, L, D = embs.shape\n",
    "        x = embs.to(self.device)\n",
    "        mask = (x.norm(dim=-1) > 1e-4).float()\n",
    "        enc_out = BaseModelOutput(last_hidden_state=x)\n",
    "        ids = self.decoder.generate(encoder_outputs=enc_out, max_length=cfg.max_len+2)\n",
    "        return self.tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def build_embedder():\n",
    "    if cfg.debug_mode or (not TRANSFORMERS_OK):\n",
    "        print(\"Using FakeEmbedder (Debug Mode ON or transformers not available).\")\n",
    "        return FakeEmbedder()\n",
    "    try:\n",
    "        print(\"Loading ProtT5‑XL…\")\n",
    "        return ProtT5Wrapper()\n",
    "    except Exception as e:\n",
    "        print(\"ProtT5 load failed, fallback to FakeEmbedder:\", e)\n",
    "        return FakeEmbedder()\n",
    "\n",
    "embedder = build_embedder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd747c",
   "metadata": {},
   "source": [
    "## 5) 扩散模型：轻量 1D U‑Net（预测 x0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.GroupNorm(8, ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(ch, ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(ch, ch, 3, padding=1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)\n",
    "\n",
    "class TinyUNet1D(nn.Module):\n",
    "    # Input/Output: (B, L, D) -> (B, L, D); internally transpose to (B, D, L)\n",
    "    def __init__(self, dim_in, dim_model, predict_x0=True):\n",
    "        super().__init__()\n",
    "        self.predict_x0 = predict_x0\n",
    "        C = dim_model\n",
    "        self.inp = nn.Conv1d(dim_in, C, 1)\n",
    "        self.down = nn.Sequential(\n",
    "            ResidualBlock(C),\n",
    "            nn.Conv1d(C, C, 2, stride=2),  # L//2\n",
    "            ResidualBlock(C),\n",
    "        )\n",
    "        self.bot = ResidualBlock(C)\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv1d(C, C, 3, padding=1),\n",
    "            ResidualBlock(C),\n",
    "        )\n",
    "        self.out = nn.Conv1d(C, dim_in, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # (B, D, L)\n",
    "        x = self.inp(x)\n",
    "        d = self.down(x)\n",
    "        b = self.bot(d)\n",
    "        u = self.up(b)\n",
    "        if u.size(-1) != x.size(-1):\n",
    "            u = F.interpolate(u, size=x.size(-1), mode=\"nearest\")\n",
    "        y = self.out(u).transpose(1, 2)  # (B, L, D)\n",
    "        return y\n",
    "\n",
    "def make_model():\n",
    "    dim_in = cfg.dim\n",
    "    dim_model = cfg.debug_dim if cfg.debug_mode else 256\n",
    "    return TinyUNet1D(dim_in, dim_model, predict_x0=cfg.predict_x0).to(DEVICE)\n",
    "\n",
    "model = make_model()\n",
    "sum_p = sum(p.numel() for p in model.parameters())\n",
    "print(\"Model params:\", sum_p/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569ea7e",
   "metadata": {},
   "source": [
    "## 6) 扩散过程工具（sqrt schedule / DDPM 采样）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sqrt_schedule(T):\n",
    "    t = torch.linspace(1e-4, 1.0, T)\n",
    "    betas = t**0.5 * 1e-2\n",
    "    betas = betas.clamp(1e-6, 0.02)\n",
    "    return betas\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, T, predict_x0=True, uniform_noise=False):\n",
    "        self.T = T\n",
    "        self.predict_x0 = predict_x0\n",
    "        self.betas = sqrt_schedule(T).to(DEVICE)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_bar = torch.cumprod(self.alphas, dim=0)\n",
    "        self.uniform = uniform_noise\n",
    "\n",
    "    def q_sample(self, x0, t):\n",
    "        a_bar = self.alphas_bar[t].view(-1, 1, 1)\n",
    "        if self.uniform:\n",
    "            eps = torch.empty_like(x0).uniform_(-1, 1)\n",
    "        else:\n",
    "            eps = torch.randn_like(x0)\n",
    "        return a_bar.sqrt()*x0 + (1 - a_bar).sqrt()*eps, eps\n",
    "\n",
    "    def p_losses(self, model, x0, t):\n",
    "        xt, eps = self.q_sample(x0, t)\n",
    "        pred = model(xt)\n",
    "        target = x0 if self.predict_x0 else eps\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape, steps=200):\n",
    "        B, L, D = shape\n",
    "        if self.uniform:\n",
    "            x = torch.empty(B, L, D, device=DEVICE).uniform_(-1, 1)\n",
    "        else:\n",
    "            x = torch.randn(B, L, D, device=DEVICE)\n",
    "        ts = torch.linspace(self.T-1, 0, steps).long()\n",
    "        for ti in tqdm(ts, desc=\"Sampling\", leave=False):\n",
    "            beta = self.betas[ti]\n",
    "            alpha = self.alphas[ti]\n",
    "            a_bar = self.alphas_bar[ti]\n",
    "            eps = model(x)\n",
    "            if self.predict_x0:\n",
    "                x0 = (x - (1 - a_bar).sqrt()*eps) / (a_bar.sqrt() + 1e-8)\n",
    "                mean = alpha.sqrt()*x0 + (1 - alpha).sqrt()*eps\n",
    "            else:\n",
    "                mean = (1/alpha.sqrt())*(x - beta/((1 - a_bar).sqrt()+1e-8)*eps)\n",
    "            if ti > 0:\n",
    "                z = torch.randn_like(x) if not self.uniform else torch.empty_like(x).uniform_(-1,1)\n",
    "                x = mean + self.betas[ti].sqrt()*z\n",
    "            else:\n",
    "                x = mean\n",
    "        return x\n",
    "\n",
    "diff = Diffusion(T=cfg.train_steps, predict_x0=cfg.predict_x0,\n",
    "                 uniform_noise=cfg.uniform_noise_sampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b26f6",
   "metadata": {},
   "source": [
    "## 7) 数据集封装：把序列 → (48, 1024) 嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcac412",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbedDataset(Dataset):\n",
    "    def __init__(self, seqs, embedder):\n",
    "        self.seqs = seqs\n",
    "        self.embedder = embedder\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx]\n",
    "\n",
    "def collate_embed(batch):\n",
    "    embs = embedder.encode(batch)  # (B, 48, 1024)\n",
    "    return embs\n",
    "\n",
    "ds = EmbedDataset(train_seqs, embedder)\n",
    "dl = DataLoader(ds, batch_size=(4 if cfg.debug_mode else cfg.batch_size),\n",
    "                shuffle=True, collate_fn=collate_embed, drop_last=True)\n",
    "print(\"Batches:\", len(dl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd097a",
   "metadata": {},
   "source": [
    "## 8) 训练（先 Debug 快跑一遍）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dl, epochs, ckpt):\n",
    "    model.train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
    "    step = 0\n",
    "    for ep in range(1, epochs+1):\n",
    "        pbar = tqdm(dl, desc=f\"Epoch {ep}/{epochs}\")\n",
    "        for x0 in pbar:\n",
    "            x0 = x0.to(DEVICE)\n",
    "            t = torch.randint(0, diff.T, (x0.size(0),), device=DEVICE)\n",
    "            loss = diff.p_losses(model, x0, t)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            step += 1\n",
    "            pbar.set_postfix(loss=float(loss))\n",
    "        if ep % cfg.save_every == 0:\n",
    "            torch.save({\"model\": model.state_dict(), \"cfg\": cfg.__dict__}, ckpt)\n",
    "            print(\"Saved:\", ckpt)\n",
    "\n",
    "if cfg.debug_mode:\n",
    "    print(\">>> Debug training …\")\n",
    "    train(model, dl, epochs=cfg.debug_epochs, ckpt=\"checkpoints/debug.pt\")\n",
    "else:\n",
    "    print(\">>> Full training …\")\n",
    "    train(model, dl, epochs=cfg.epochs, ckpt=cfg.ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb185d66",
   "metadata": {},
   "source": [
    "## 9) 采样 + 解码 → 待筛选序列 `generated_seqs.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def sample_and_decode(n=100):\n",
    "    model.eval()\n",
    "    B = n\n",
    "    shape = (B, cfg.max_len, cfg.dim)\n",
    "    x = diff.p_sample_loop(model, shape, steps=(50 if cfg.debug_mode else cfg.sample_steps))\n",
    "    if DEVICE != \"cpu\":\n",
    "        x = x.float().cpu()\n",
    "    seqs = embedder.decode(x)\n",
    "    return seqs\n",
    "\n",
    "N = (cfg.debug_sample_n if cfg.debug_mode else cfg.sample_n)\n",
    "gen = sample_and_decode(N)\n",
    "df = pd.DataFrame({\"seq\": gen})\n",
    "df.to_csv(\"generated_seqs.csv\", index=False)\n",
    "print(\"Saved generated_seqs.csv with\", len(df), \"rows\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ee6b2",
   "metadata": {},
   "source": [
    "## 10) 快速体检过滤（长度/重复/净电荷/R+K占比等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pka = {\"K\":10.5, \"R\":12.5, \"H\":6.0, \"D\":3.9, \"E\":4.1}\n",
    "def net_charge(seq, ph=7.4):\n",
    "    pos = sum(1/(1+10**(ph-pka.get(aa, 0))) for aa in seq if aa in \"KRH\")\n",
    "    neg = sum(1/(1+10**(pka.get(aa, 14)-ph)) for aa in seq if aa in \"DE\")\n",
    "    return pos - neg\n",
    "\n",
    "def has_long_repeat(seq, k=6):\n",
    "    for aa in set(seq):\n",
    "        if aa*k in seq:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def rk_ratio(seq):\n",
    "    if not seq: return 0.0\n",
    "    rk = sum(aa in \"RK\" for aa in seq)\n",
    "    return rk/len(seq)\n",
    "\n",
    "df = pd.read_csv(\"generated_seqs.csv\")\n",
    "df[\"len\"] = df.seq.str.len()\n",
    "df[\"charge\"] = df.seq.apply(net_charge)\n",
    "df[\"rk_ratio\"] = df.seq.apply(rk_ratio)\n",
    "df[\"long_repeat\"] = df.seq.apply(has_long_repeat)\n",
    "filtered = df[\n",
    "    (df[\"len\"].between(5, 48)) &\n",
    "    (~df[\"long_repeat\"]) &\n",
    "    (df[\"charge\"] > 0) &\n",
    "    (df[\"rk_ratio\"] <= 0.40)\n",
    "].copy()\n",
    "\n",
    "filtered.to_csv(\"generated_seqs.filtered.csv\", index=False)\n",
    "print(\"After quick filters:\", len(filtered), \"kept; saved -> generated_seqs.filtered.csv\")\n",
    "filtered.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad97fb1",
   "metadata": {},
   "source": [
    "\n",
    "## 11) 切到 Full 模式（贴论文设置）\n",
    "\n",
    "1. 在 **配置** 单元格把 `debug_mode=False`，并调整 `epochs`、`batch_size`、`sample_n`；  \n",
    "2. 确保安装 `transformers` 并可以拉取 **Rostlab/prot_t5_xl_uniref50**；  \n",
    "3. 把你的训练集写到 `data/train.txt`（每行一个序列，长度 5–48）；  \n",
    "4. 从 **第 4 节** 开始依次重新运行到 **第 10 节**，得到 `generated_seqs.filtered.csv`；  \n",
    "5. 后续把该 CSV 喂给判别器 + 完整理化筛选（另附 notebook）。\n",
    "\n",
    "> 论文关键点（用于参数对齐）：“固定到 `(48,1024)` 的 latent；训练 2000 步；采样下采样为 200 步；\n",
    "> 解码前根据行范数阈值去掉接近 0 的 padding 行”。\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
