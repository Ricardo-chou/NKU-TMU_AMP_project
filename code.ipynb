{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16673d24-4303-4edf-b310-5ac0d911a754",
   "metadata": {},
   "source": [
    "# AMP筛选脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e4b954d4b0ad1",
   "metadata": {},
   "source": [
    "# 阶段 1：导入依赖与环境配置\n",
    "\n",
    "在本研究的实验准备阶段，我们首先构建了稳定、可复现的运行环境，以支持后续的大规模蛋白序列生成与结构预测任务。具体而言，除了**常用的`np,pd,fair-esm`等**依赖我们基于 Python 环境安装并升级了以下核心依赖：\n",
    "\n",
    "- **`transformers==4.33.3`**：用于加载和调用 HuggingFace 提供的蛋白语言模型，包括 Tranception 和 ESMFold；\n",
    "- **`safetensors==0.4.2`**：用于加载安全高效的模型权重格式，避免传统 `.bin` 文件在大模型加载过程中的内存碎片问题；\n",
    "- **`Biopython`**：用于蛋白序列的解析与生物信息处理，例如突变标签提取和 BLOSUM62 评分。\n",
    "\n",
    "为确保依赖版本的正确性，代码中调用了 `print(transformers.__version__)` 和 `print(safetensors.__version__)` 进行版本验证。针对模型运行的硬件支持，进一步通过 `torch.version.cuda` 和 `torch.cuda.is_available()` 检查了本地环境中 CUDA 的可用性与 PyTorch 版本（目标为 2.0.0）。该检查确认了当前系统是否支持 GPU 加速，这对于后续执行结构预测模型（如 ESMFold 和 AlphaFold）至关重要，尤其在面对大批量高复杂度序列时，GPU 的存在可显著提升推理效率与任务完成速度。\n",
    "\n",
    "本阶段的配置保证了模型加载、运行与推理的稳定性，是整个蛋白设计与评估流程的基础。通过该步骤的依赖确认与环境搭建，后续模型训练、序列生成、多模型打分与结构预测等任务得以在兼容、高效的计算环境中顺利开展。"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac73ec49-5ed7-4779-9071-b9d89403916b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:28:32.142654Z",
     "start_time": "2025-07-11T15:28:25.291418Z"
    }
   },
   "source": [
    "!pip install fair-esm openpyxl scikit-learn pandas numpy torch xlrd datasets\n",
    "!pip install accelerate"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fair-esm\r\n",
      "  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\r\n",
      "Collecting openpyxl\r\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Requirement already satisfied: scikit-learn in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (1.7.0)\r\n",
      "Requirement already satisfied: pandas in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: numpy in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (2.1.2)\r\n",
      "Requirement already satisfied: torch in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (2.5.1+cu121)\r\n",
      "Collecting xlrd\r\n",
      "  Downloading xlrd-2.0.2-py2.py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: datasets in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (3.6.0)\r\n",
      "Collecting et-xmlfile (from openpyxl)\r\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: filelock in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (3.1.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (20.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (0.33.2)\r\n",
      "Requirement already satisfied: packaging in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\r\n",
      "Requirement already satisfied: idna>=2.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\r\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\r\n",
      "Downloading xlrd-2.0.2-py2.py3-none-any.whl (96 kB)\r\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\r\n",
      "Installing collected packages: fair-esm, xlrd, et-xmlfile, openpyxl\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4/4\u001B[0m [openpyxl]3/4\u001B[0m [openpyxl]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed et-xmlfile-2.0.0 fair-esm-2.0.0 openpyxl-3.1.5 xlrd-2.0.2\r\n",
      "Collecting accelerate\r\n",
      "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from accelerate) (2.1.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from accelerate) (25.0)\r\n",
      "Requirement already satisfied: psutil in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from accelerate) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from accelerate) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from accelerate) (0.33.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from accelerate) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2024.6.1)\r\n",
      "Requirement already satisfied: requests in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.12.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\r\n",
      "Requirement already satisfied: networkx in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.1.105)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zhaojunjie/miniconda3/envs/pytorch_env/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\r\n",
      "Downloading accelerate-1.8.1-py3-none-any.whl (365 kB)\r\n",
      "Installing collected packages: accelerate\r\n",
      "Successfully installed accelerate-1.8.1\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0ff68f-c77d-4b7c-988e-cccb2cededdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting transformers==4.33.3\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/98/46/f6a79f944d5c7763a9bc13b2aa6ac72daf43a6551f5fb03bccf0a9c2fec1/transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.6/7.6 MB\u001B[0m \u001B[31m64.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (23.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/86/44/2101cc0890c3621b90365c9ee8d7291a597c0722ad66eccd6ffa7f1bcc09/regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m780.9/780.9 KB\u001B[0m \u001B[31m47.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (1.24.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/d6/27/07a337087dd507170a1b20fed3bbf8da81401185a7130a6e74e440c52040/tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m73.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: safetensors>=0.3.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (0.32.3)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (3.12.2)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.3) (4.6.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.3) (1.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.3) (2025.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from requests->transformers==4.33.3) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from requests->transformers==4.33.3) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from requests->transformers==4.33.3) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from requests->transformers==4.33.3) (3.4)\n",
      "Installing collected packages: tokenizers, regex, transformers\n",
      "Successfully installed regex-2024.11.6 tokenizers-0.13.3 transformers-4.33.3\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0mLooking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting Biopython\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/80/5a/6ba0066b7f38b9e7a085f2fc4c171a25ebfa64202aab0965961621f561e1/biopython-1.85-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m74.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from Biopython) (1.24.2)\n",
      "Installing collected packages: Biopython\n",
      "Successfully installed Biopython-1.85\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers==4.33.3\n",
    "!pip install Biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e5b54a-d921-4322-ac25-fe1203e40c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.33.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d4db39-6a30-438d-ad18-1458c11b6468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: transformers==4.33.3 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (4.33.3)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (0.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (0.5.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (0.13.3)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (3.12.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (23.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from transformers==4.33.3) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.3) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.3) (4.6.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.3) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from requests->transformers==4.33.3) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from requests->transformers==4.33.3) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from requests->transformers==4.33.3) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from requests->transformers==4.33.3) (1.26.16)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0mLooking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: Biopython in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (1.85)\n",
      "Requirement already satisfied: numpy in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from Biopython) (1.24.2)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0mLooking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting safetensors==0.4.2\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/aa/9a/723fc6eed972b28bbb24241b246005093b3c27340bc8f7b7606d75a92834/safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m26.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: safetensors\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.5.3\n",
      "    Uninstalling safetensors-0.5.3:\n",
      "      Successfully uninstalled safetensors-0.5.3\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "accelerate 1.7.0 requires safetensors>=0.4.3, but you have safetensors 0.4.2 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed safetensors-0.4.2\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m4.33.3\n",
      "0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers==4.33.3\n",
    "!pip install Biopython\n",
    "!pip install --upgrade safetensors==0.4.2\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "import safetensors\n",
    "print(safetensors.__version__)  # 应输出 0.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eddb1d76-3beb-49ef-9a6c-fc063e21aa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n",
      "True\n",
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)  # 应输出 2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90bf85d9-cc83-4f10-9faa-ab99bcf7e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.0.0+cu117\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, triton, typing-extensions\n",
      "Required-by: accelerate, torchaudio, torchvision, triton\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561aac4c-82c2-4319-8c66-d473d7d74a81",
   "metadata": {},
   "source": [
    "**导入常用依赖库**\n",
    "\n",
    "```python\n",
    "import pandas as pd import numpy as np import torch import esm import os import random import re import warnings\n",
    "```\n",
    "\n",
    "此外，导入了 `sklearn.model_selection.train_test_split` 和 `sklearn.metrics.r2_score`，用于后续亮度预测器模型的训练集划分和性能评估。\n",
    "\n",
    "------\n",
    "\n",
    "**输入数据文件路径定义**\n",
    "\n",
    "这部分定义了项目中关键的输入数据文件路径：\n",
    "\n",
    "- `GFP_data.xlsx`：主数据集，包含蛋白序列与亮度等标签；\n",
    "- `AAseqs of 4 GFP proteins.txt`：野生型序列（WT）信息，作为突变设计的参考基础；\n",
    "- `Exclusion_List.csv`：排除列表，含不合法或不希望使用的序列（如实验失败或毒性序列），将在后续生成中剔除。\n",
    "\n",
    "------\n",
    "\n",
    "**超参数设置**\n",
    "\n",
    "- `ESM_MODEL_NAME`：选择中等规模的预训练蛋白语言模型，兼顾预测精度与计算成本；\n",
    "- `MAX_MUTATIONS`：最大突变位点数，符合竞赛规则；\n",
    "- `N_CANDIDATES_TO_GENERATE`：每轮候选序列生成数量；\n",
    "- `TOP_N_SELECT`：最终入选的候选序列数量（如用于 AlphaFold 验证或实验）。\n",
    "\n",
    "------\n",
    "\n",
    "**硬件环境检查与打印**\n",
    "\n",
    "------\n",
    "\n",
    "**全局随机种子设置（可复现性）**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6771f530-01d6-488d-ba54-a5a0be261d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # 处理表格数据（如 Excel、CSV）\n",
    "import numpy as np   # 数学计算库\n",
    "import torch         # PyTorch，主要用于模型加载与运行\n",
    "import esm           # Facebook 的蛋白语言模型库 ESM（用于提取蛋白序列嵌入）\n",
    "import os            # 操作文件路径\n",
    "import random        # 设置随机种子\n",
    "from sklearn.model_selection import train_test_split  # 训练集测试集划分\n",
    "from sklearn.metrics import r2_score                  # 模型性能评估指标\n",
    "import re           # 正则表达式，用于处理突变信息\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略警告信息（不影响主逻辑）\n",
    "\n",
    "# 常量定义\n",
    "TRAIN_DATA_FILE = os.path.join('GFP_data.xlsx')\n",
    "# 包含亮度、突变信息等训练数据的 Excel 文件\n",
    "WT_SEQ_FILE = os.path.join('AAseqs of 4 GFP proteins.txt')\n",
    "# 4 个 GFP 蛋白的氨基酸序列（wild-type 序列），后续将基于这些序列设计突变\n",
    "EXCLUSION_FILE = os.path.join('Exclusion_List.csv')\n",
    "# 不允许的序列清单，可能是失败序列、毒性序列等（用于过滤）\n",
    "\n",
    "\n",
    "# --- 模型与生成参数 ---\n",
    "ESM_MODEL_NAME = \"esm2_t12_35M_UR50D\" # 选择一个中等大小的ESM模型，平衡速度和性能\n",
    "MAX_MUTATIONS = 6 # 比赛规则：最多6个突变\n",
    "N_CANDIDATES_TO_GENERATE = 500 # 生成候选序列的数量（可调整）\n",
    "TOP_N_SELECT = 10 # 最终选择的序列数量\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 设置随机种子以便结果可复现（可选）\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f143d-9aca-4f20-830f-29df29b407ea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " # 阶段 2：数据加载与预处理\n",
    "该阶段的目标是从多个数据源加载原始信息，并将其处理为可用于模型输入和突变设计的结构化格式，主要包括亮度标签、野生型序列、突变标签以及排除项。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3e7c5-cea9-4f0a-9deb-d5ecf25f55a7",
   "metadata": {},
   "source": [
    "## 2.1 加载训练数据集（GFP_data.xlsx）\n",
    "首先通过 pandas 读取 GFP_data.xlsx 文件中的 'brightness' 工作表，作为训练数据来源。该表格包含多个 GFP 蛋白的突变序列、突变标签（如 aaMutations）、完整蛋白序列以及其对应的亮度实验结果（如 Brightness 或 log Brightness），为亮度预测器模型的训练提供了监督标签。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "770c4590-f020-4215-82e3-ca159e546a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loaded 147950 rows from GFP_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training data...\")\n",
    "try:\n",
    "    xsl = pd.ExcelFile(TRAIN_DATA_FILE,engine='openpyxl') # 假设亮度数据在名为 'brightness' 的 sheet\n",
    "    gfp_df = xsl.parse('brightness')\n",
    "    print(f\"Loaded {len(gfp_df)} rows from {TRAIN_DATA_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Training data file not found at {TRAIN_DATA_FILE}\")\n",
    "    exit() # 如果文件不存在，可能需要停止执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931e6ad-2b02-4daa-8ff9-7244d3f70ae9",
   "metadata": {},
   "source": [
    "## 2.2 读取野生型蛋白序列（WT_SEQ_FILE）\n",
    "随后从 AAseqs of 4 GFP proteins.txt 文件中解析出 avGFP 的野生型序列。该文件采用类似 FASTA 的格式，使用 > 开头的 header 来标识序列名称，后面接氨基酸序列本体。程序通过关键字（如 “avGFP”）定位目标序列并提取其主序列，用于后续的突变生成参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "329667ad-b658-4f6c-aca1-3da1fcc577bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading avGFP WT sequence...\n",
      "Found avGFP WT sequence (Length: 238).\n",
      "MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading avGFP WT sequence...\")\n",
    "avGFP_WT_sequence = None\n",
    "try:\n",
    "    with open(WT_SEQ_FILE, 'r') as f:\n",
    "        # 假设文件格式是 >Header \\n Sequence \\n >Header2...\n",
    "        # 我们需要找到 avGFP 的序列\n",
    "        header = \"\"\n",
    "        seq_lines = []\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                # 如果找到了上一个序列，并且是avGFP，保存它\n",
    "                if \"avGFP\" in header and seq_lines:\n",
    "                    avGFP_WT_sequence = \"\".join(seq_lines).strip()\n",
    "                    break # 找到后退出循环\n",
    "                # 开始新的序列记录\n",
    "                header = line.strip()\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line.strip())\n",
    "        # 处理文件最后一个序列的情况\n",
    "        if avGFP_WT_sequence is None and \"avGFP\" in header and seq_lines:\n",
    "             avGFP_WT_sequence = \"\".join(seq_lines).strip()\n",
    "\n",
    "    if avGFP_WT_sequence:\n",
    "        print(f\"Found avGFP WT sequence (Length: {len(avGFP_WT_sequence)}).\")\n",
    "        print(avGFP_WT_sequence) #查看序列\n",
    "    else:\n",
    "        print(\"Error: avGFP WT sequence not found in\", WT_SEQ_FILE)\n",
    "        #没找到手动设置一个默认值或停止执行\n",
    "        avGFP_WT_sequence = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\" \n",
    "        print(\"Using default WT sequence.\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: WT sequence file not found at {WT_SEQ_FILE}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb136c-587f-424b-8e43-e43cbf35feae",
   "metadata": {},
   "source": [
    "## 2.3 加载排除列表（Exclusion_List.csv）\n",
    "程序还加载了一个名为 Exclusion_List.csv 的排除列表数据，用于过滤掉某些不合法或不推荐的序列。这些序列可能来源于实验失败、毒性反应等历史记录。通过后续的比对操作，程序会确保生成的候选序列不会与这些“黑名单”重合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f44e77e-71b5-4e37-a3f1-b2cef297f733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exclusion list...\n",
      "Loaded 739 sequences into exclusion list.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading exclusion list...\")\n",
    "try:\n",
    "    exclusion_df = pd.read_csv(EXCLUSION_FILE)\n",
    "    # 排除序列在名为 'sequences-not-submit' 的列中\n",
    "    exclusion_sequences = set(exclusion_df['sequences-not-submit'].astype(str))\n",
    "    print(f\"Loaded {len(exclusion_sequences)} sequences into exclusion list.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Exclusion list file not found at {EXCLUSION_FILE}\")\n",
    "    exclusion_sequences = set() # 如果文件不存在，创建一个空集合\n",
    "    print(\"Warning: Proceeding without an exclusion list.\")\n",
    "except KeyError:\n",
    "    print(f\"Error: Column 'sequences-not-submit' not found in {EXCLUSION_FILE}\")\n",
    "    exclusion_sequences = set()\n",
    "    print(\"Warning: Proceeding without an exclusion list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3a331-90bc-4e82-afb9-dd29d34a4827",
   "metadata": {},
   "source": [
    "## 2.4 预处理训练数据\n",
    "#### 参数说明：\n",
    "\n",
    "- `mutation_str`：突变描述字符串，例如 `\"WT\"` 或 `\"A12B:C34D\"`；\n",
    "- `wt_sequence`：avGFP 的野生型氨基酸序列。\n",
    "\n",
    "#### 逻辑说明：\n",
    "\n",
    "1. 若为 `WT`（wild-type，野生型），直接返回原序列。\n",
    "2. 否则，将突变字符串按 `:` 拆分为多个突变（支持多点突变）。\n",
    "3. 用正则表达式匹配每个突变的格式，如 `G101A`（即第101位的G被换成了A）。\n",
    "4. 验证突变位置是否合法，是否在序列长度范围内。\n",
    "5. 检查突变位置的原始氨基酸是否与实际匹配（建议，但可跳过）。\n",
    "6. 如果突变为 `*`（终止密码子），视为无效突变，返回 `None`。\n",
    "7. 如果突变为 `.`，表示保持原氨基酸不变。\n",
    "8. 将突变后的字符替换到序列中。\n",
    "9. 最终返回突变后的完整序列字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28132c01-8e29-4020-a3a0-073cac446057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Filtered down to 54025 avGFP entries.\n",
      "Removed 2310 rows due to invalid sequences or brightness.\n",
      "Final training set size: 51715\n",
      "\n",
      "Sample of processed training data:\n",
      "                     aaMutations  Brightness  \\\n",
      "0                             WT    3.719212   \n",
      "1                          A109D    1.301030   \n",
      "2  A109D:N145D:I187V:M232T:L235P    1.301031   \n",
      "3        A109D:Y142N:H147L:E221G    1.301189   \n",
      "4                          A109G    3.708478   \n",
      "5        A109G:K139M:R167C:L235P    3.582764   \n",
      "6  A109G:K155E:F164S:L193Q:L194P    1.499573   \n",
      "7                    A109G:K157R    3.659013   \n",
      "8  A109G:K157R:I160V:I187V:T224S    3.573855   \n",
      "9  A109P:K112R:G173S:L177P:S201G    1.301031   \n",
      "\n",
      "                                       full_sequence  \n",
      "0  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "1  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "2  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "3  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "4  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "5  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "6  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "7  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "8  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "9  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
      "Processed training data saved to 'processed_training_data.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing training data...\")\n",
    "# 筛选 avGFP 数据\n",
    "avGFP_train_df = gfp_df[gfp_df['GFP type'] == 'avGFP'].copy()\n",
    "print(f\"Filtered down to {len(avGFP_train_df)} avGFP entries.\")\n",
    "\n",
    "# 定义函数：根据突变字符串生成完整序列\n",
    "def generate_mutated_sequence(mutation_str, wt_sequence):\n",
    "    \"\"\"\n",
    "    根据突变描述字符串和野生型序列生成突变后的完整序列。\n",
    "    mutation_str: e.g., \"WT\", \"G101A\", \"A12B:C34D\"\n",
    "    wt_sequence: 野生型氨基酸序列字符串\n",
    "    \"\"\"\n",
    "    if not isinstance(mutation_str, str) or not wt_sequence:\n",
    "        return None\n",
    "    if mutation_str.strip().upper() == 'WT':\n",
    "        return wt_sequence\n",
    "\n",
    "    sequence = list(wt_sequence)\n",
    "    mutations = mutation_str.split(':') # 支持多个突变，以冒号分隔\n",
    "    valid_mutation_count = 0\n",
    "    try:\n",
    "        for mut in mutations:\n",
    "            match = re.match(r'([A-Z])(\\d+)([A-Z*.])$', mut.strip(), re.IGNORECASE) # 匹配 G101A, T203*, V163.\n",
    "            if match:\n",
    "                original_aa, pos, new_aa = match.groups()\n",
    "                pos = int(pos)# 转换为 0-based index\n",
    "\n",
    "                # 检查位置是否有效\n",
    "                if pos < 0 or pos >= len(sequence):\n",
    "                    # print(f\"Warning: Invalid position {pos+1} in mutation '{mut}' for sequence length {len(sequence)}. Skipping mutation.\")\n",
    "                    continue # 跳过无效位置的突变\n",
    "\n",
    "                # 检查原始氨基酸是否匹配\n",
    "                if sequence[pos].upper() != original_aa.upper():\n",
    "                    print(f\"Warning: Original AA mismatch at position {pos+1} in mutation '{mut}'. Expected {sequence[pos]}, got {original_aa}. Applying mutation anyway.\")\n",
    "                    pass # 允许不匹配，但打印警告\n",
    "\n",
    "                # 处理特殊字符\n",
    "                if new_aa == '*': # 终止密码子 - 通常不希望出现在中间\n",
    "                    # print(f\"Warning: Stop codon '*' mutation '{mut}' encountered. Treating as deletion or invalid sequence for this tutorial.\")\n",
    "                    # 对于亮度预测，终止密码子通常导致无功能蛋白，返回None\n",
    "                    return None \n",
    "                elif new_aa == '.': # 表示与原氨基酸相同 (无突变)\n",
    "                    new_aa = sequence[pos] # 保持不变\n",
    "\n",
    "                sequence[pos] = new_aa.upper()\n",
    "                valid_mutation_count += 1\n",
    "            else:\n",
    "                print(f\"Warning: Could not parse mutation '{mut}'. Skipping.\")\n",
    "                pass # 跳过无法解析的突变格式\n",
    "        # 如果没有成功应用任何突变（可能是格式问题），返回None\n",
    "        if valid_mutation_count == 0 and mutations:\n",
    "            return None\n",
    "        return \"\".join(sequence)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing mutation string '{mutation_str}': {e}\")\n",
    "        return None # 返回 None 表示序列生成失败\n",
    "\n",
    "# 应用函数生成序列\n",
    "avGFP_train_df['full_sequence'] = avGFP_train_df['aaMutations'].apply(\n",
    "    lambda x: generate_mutated_sequence(x, avGFP_WT_sequence)\n",
    ")\n",
    "\n",
    "# 清理数据：移除序列生成失败或亮度无效的行\n",
    "original_len = len(avGFP_train_df)\n",
    "avGFP_train_df.dropna(subset=['full_sequence', 'Brightness'], inplace=True)\n",
    "# 确保亮度是数值类型\n",
    "avGFP_train_df['Brightness'] = pd.to_numeric(avGFP_train_df['Brightness'], errors='coerce')\n",
    "avGFP_train_df.dropna(subset=['Brightness'], inplace=True)\n",
    "\n",
    "print(f\"Removed {original_len - len(avGFP_train_df)} rows due to invalid sequences or brightness.\")\n",
    "print(f\"Final training set size: {len(avGFP_train_df)}\")\n",
    "\n",
    "# 查看处理后的数据\n",
    "print(\"\\nSample of processed training data:\")\n",
    "print(avGFP_train_df[['aaMutations', 'Brightness', 'full_sequence']].head(10))\n",
    "avGFP_train_df.to_excel('processed_training_data.xlsx', index=False) # 保存处理后的数据\n",
    "print(\"Processed training data saved to 'processed_training_data.xlsx'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76083b57-dabf-4e23-b106-42c6bc98c7d1",
   "metadata": {},
   "source": [
    "# 阶段3：Mutation Library构建策略\n",
    "\n",
    "------\n",
    "\n",
    "## 阶段 1：引导式小批量生成（高质量种子）\n",
    "\n",
    "**与上一届选用位点枚举得到的library的优点**：\n",
    "\n",
    "- 控制生成质量，避免全空间暴力枚举带来的大量“死序列”\n",
    "- 强化已知高亮度区域的探索深度，利于模型迭代与多目标打分\n",
    "- 适合结合你已有的亮度训练数据使用 VAE / Transformer 进行生成\n",
    "\n",
    "### 细化点：\n",
    "\n",
    "| 项目                           | 建议                                                         |\n",
    "| ------------------------------ | ------------------------------------------------------------ |\n",
    "| **VAE/Transformer 解码器采样** | 使用 `Top-k` 或 `nucleus sampling` 策略（p=0.9），避免低概率垃圾序列 |\n",
    "| **突变控制**                   | 在采样输出中限制最多只变异 ≤6 个位点                         |\n",
    "| **高频突变位点控制**           | 从亮度数据集以及文献参考中统计频率高的突变位点，例如 F64, S65, M153, T203 等优先设为候选位置 |\n",
    "| **隐空间子采样（亮度引导）**   | 将训练集中亮度最高的序列映射到 latent space，进行“高亮区域附近”采样（引导生成更亮的候选） |\n",
    "| **筛选机制**                   | 添加简单模型筛选器（如 MLP/Ridge），快速过滤极端无效序列（如带 stop codon、极端保守位点突变） |\n",
    "\n",
    "###  输出目标：\n",
    "\n",
    "- **5,000 ~ 50,000 条序列**：均为结构合理、突变点分布良好、分布接近训练集中高亮度区域\n",
    "\n",
    "------\n",
    "\n",
    "##  阶段 2：结构化扩展（百万级突变空间）\n",
    "\n",
    "- 用结构信息 +进化启发约束搜索空间，比盲目组合更科学\n",
    "- 可结合 MPNN、RFdiffusion、AlphaFold 等工具构建“热稳定+折叠可行”的序列变异空间\n",
    "\n",
    "###  细化点：\n",
    "\n",
    "| 策略                                      | 建议                                                         |\n",
    "| ----------------------------------------- | ------------------------------------------------------------ |\n",
    "| **多点突变扩展**                          | 对种子序列进行局部扩展，每次突变 1~2 个新位点（从候选位点集中选） |\n",
    "| **结构启发**                              | 利用 AlphaFold pLDDT 分布或结构热核分析，避开核心β桶区域、保守氢键骨架 |\n",
    "| **遗传算法 / CbAS / Simulated Annealing** |                                                              |\n",
    "\n",
    "- 可对亮度预测模型建立适应度函数（fitness）<br>\n",
    "- 结合变异概率控制多样性/稳定性之间平衡\n",
    "   | **RFdiffusion 指导结构一致性** | 用 avGFP 的 backbone（或 AlphaFold预测结构）作为参考，生成结构一致的突变体 |\n",
    "   | **热稳定性预测模块** | 加入 ΔΔG 预测（FoldX / DeepDDG）或温度诱导变异打分（如 FireProt）做双向过滤 |\n",
    "\n",
    "### 输出目标：\n",
    "\n",
    "- **1M~100M 条序列**（非冗余、结构合理、未出现在训练集中的变体）\n",
    "- 可根据预测亮度/热稳打分排序选 top-k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd41da-27c5-45d1-83ee-67c3d26d3ca4",
   "metadata": {},
   "source": [
    "#### 3.1.1 基于突变频率筛选 top-60 位点\n",
    "\n",
    "为了限制变异空间并提升模型学习效率，我们首先对训练集中已有的蛋白突变信息进行统计，选取突变频率最高的 60 个氨基酸位点作为候选突变位点集合。这一策略具有如下意义：\n",
    "\n",
    "- **降维作用**：在全长 200 多个残基的 GFP 序列中，任意突变组合构成的空间极其庞大。通过聚焦频繁突变区域，有效减少搜索空间维度，使后续生成模型聚焦于功能相关区域。\n",
    "- **生物学动因**：突变频率高的位点往往是亮度改造过程中的“热点位点”，可能已在历史实验中被多次验证对亮度有显著影响，因此具有更高的信息密度。\n",
    "- **稳定建模基础**：由于这些位点在训练集中覆盖度高，有助于模型稳定学习突变-功能之间的关系，避免稀疏数据带来的建模困难。\n",
    "\n",
    "具体方法如下：\n",
    "\n",
    "- 对 `aaMutations` 字段中的每条突变标签（如 `\"G101A:T203F\"`）进行正则解析，提取所有突变发生的位置编号（如 101, 203）；\n",
    "- 累加所有训练集中突变位点的出现频率，形成位点频数分布；\n",
    "- 对分布排序，取频率最高的前 60 个位点，作为后续突变控制与编码输入的索引模板；\n",
    "- 该集合也将作为后续构建 mutation matrix 的列索引，确保候选序列的突变受控且可比较。\n",
    "\n",
    "该 top-60 位点集合最终以 `top_positions.csv` 保存，用于后续模型训练和生成阶段调用。### 3.1.1 根据频率进行筛选top60点位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8081a689-b6e1-4884-bc4f-9b9bda886570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mut_encoded matrix and index is saved\n",
      "    Position\n",
      "0        164\n",
      "1        229\n",
      "2        121\n",
      "3        128\n",
      "4         99\n",
      "5        144\n",
      "6        233\n",
      "7        198\n",
      "8        214\n",
      "9        153\n",
      "10       159\n",
      "11       113\n",
      "12       158\n",
      "13         8\n",
      "14       171\n",
      "15       170\n",
      "16       165\n",
      "17       149\n",
      "18       156\n",
      "19        79\n",
      "20       107\n",
      "21       146\n",
      "22       130\n",
      "23       166\n",
      "24       223\n",
      "25       105\n",
      "26        46\n",
      "27        23\n",
      "28        84\n",
      "29       161\n",
      "30       135\n",
      "31        71\n",
      "32       167\n",
      "33       185\n",
      "34        47\n",
      "35       209\n",
      "36        52\n",
      "37       101\n",
      "38       100\n",
      "39       212\n",
      "40       140\n",
      "41        88\n",
      "42       126\n",
      "43       131\n",
      "44       136\n",
      "45       162\n",
      "46        78\n",
      "47         3\n",
      "48        26\n",
      "49        41\n",
      "50       218\n",
      "51       123\n",
      "52        45\n",
      "53        68\n",
      "54       188\n",
      "55       114\n",
      "56        98\n",
      "57       176\n",
      "58        83\n",
      "59        62\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# 配置\n",
    "INPUT_LEN = 60\n",
    "AA_TOKENS = list(\"ACDEFGHIKLMNPQRSTVWY*.\")\n",
    "AA_TO_IDX = {aa: i+1 for i, aa in enumerate(AA_TOKENS)}  # 0 for no mutation\n",
    "\n",
    "# 加载训练数据\n",
    "df = pd.read_excel(\"processed_training_data.xlsx\",engine='openpyxl')\n",
    "\n",
    "# 提取高频突变位点\n",
    "def extract_positions(mutation_str):\n",
    "    if mutation_str.strip().upper() == \"WT\":\n",
    "        return []\n",
    "    positions = []\n",
    "    for mut in mutation_str.split(\":\"):\n",
    "        match = re.match(r\"[A-Z](\\d+)[A-Z*\\.]\", mut.strip())\n",
    "        if match:\n",
    "            positions.append(int(match.group(1))+1)\n",
    "    return positions\n",
    "\n",
    "mutation_pos_counter = Counter()\n",
    "for muts in df['aaMutations']:\n",
    "    mutation_pos_counter.update(extract_positions(muts))\n",
    "top_positions = [pos for pos, _ in mutation_pos_counter.most_common(INPUT_LEN)]\n",
    "\n",
    "# 构建突变编码矩阵\n",
    "mutation_matrix = np.zeros((len(df), INPUT_LEN), dtype=int)\n",
    "for i, muts in enumerate(df['aaMutations']):\n",
    "    if muts.strip().upper() == \"WT\":\n",
    "        continue\n",
    "    for mut in muts.split(\":\"):\n",
    "        match = re.match(r\"([A-Z])(\\d+)([A-Z*\\.])\", mut.strip())\n",
    "        if match:\n",
    "            _, pos, to_aa = match.groups()\n",
    "            pos = int(pos)\n",
    "            if pos in top_positions:\n",
    "                col_idx = top_positions.index(pos)\n",
    "                mutation_matrix[i, col_idx] = AA_TO_IDX.get(to_aa.upper(), 0)\n",
    "\n",
    "# 保存突变编码矩阵\n",
    "np.save(\"mutation_matrix.npy\", mutation_matrix)\n",
    "mut_matrix_index=pd.DataFrame(top_positions, columns=[\"Position\"])\n",
    "mut_matrix_index.to_csv(\"top_positions.csv\", index=False)\n",
    "print(\"mut_encoded matrix and index is saved\")\n",
    "print(mut_matrix_index)\n",
    "print(mutation_matrix)\n",
    "print(top_positions[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2db13-4fe5-4bdb-a810-f4719eac4782",
   "metadata": {},
   "source": [
    "### 3.1.2 结合 PDB 结构信息识别功能相关位点\n",
    "\n",
    "为提高突变策略的生物学合理性与功能导向性，我们不仅从突变频率出发选取高频突变位点，还进一步结合 avGFP 的三维结构信息，识别靠近荧光核心区域（chromophore）以及形成结构核心的氨基酸残基。这一策略基于如下考虑：\n",
    "\n",
    "- **荧光团周围残基对亮度影响显著**，直接参与电子跃迁环境的形成；\n",
    "- **空间接近性决定了突变对结构稳定性或功能性的干扰程度**；\n",
    "- **利用结构邻接矩阵可支持后续图神经网络、接触映射或空间约束设计**。\n",
    "\n",
    "------\n",
    "\n",
    "#### 加载 avGFP 的结构模型（PDB 文件）\n",
    "\n",
    "使用 Bio.PDB 模块载入 AlphaFold2 模型预测的 avGFP 结构（`AF-P42212-F1-model_v4.pdb`）。从该文件中提取第一个模型与链（默认单链），遍历所有残基并提取主链 α-碳（CA）原子的坐标。\n",
    "\n",
    "------\n",
    "\n",
    "#### 构建结构信息表\n",
    "\n",
    "对每一个有效残基记录：\n",
    "\n",
    "- 残基编号 `Residue_ID`\n",
    "- 三字母氨基酸名称（可选）\n",
    "- 主链 CA 原子的三维坐标\n",
    "\n",
    "构建为 `df_structure`，用于后续几何计算。\n",
    "\n",
    "------\n",
    "\n",
    "#### 计算残基与荧光团之间的空间距离\n",
    "\n",
    "- 选取荧光团关键残基编号（如 Tyr64, Gly65, Ser66）；\n",
    "- 提取这三个残基的 CA 坐标，并计算其质心位置，作为 **chromophore center**；\n",
    "- 对所有其他残基，计算其与荧光团中心的欧几里得距离；\n",
    "- 若该距离小于 10 Å，则认为“靠近荧光团”。\n",
    "\n",
    "该信息存入字段 `Dist_to_Chromophore` 和 `Nearby_Chromophore`，用于标注与功能核心的空间关系。\n",
    "\n",
    "------\n",
    "\n",
    "#### 构建残基邻接矩阵\n",
    "\n",
    "- 提取所有残基的 CA 坐标；\n",
    "- 计算任意两个残基之间的欧几里得距离；\n",
    "- 若距离小于 8.0 Å，则认为两者在三维结构中“相邻”；\n",
    "- 构建 0-1 的邻接矩阵 `adj_matrix`，表示残基之间的直接接触关系。\n",
    "\n",
    "这一邻接矩阵将在后续结构引导的突变策略（如限制仅突变结构邻近区）或结构感知模型（如 GNN）中发挥作用。\n",
    "\n",
    "------\n",
    "\n",
    "#### 数据保存\n",
    "\n",
    "- `df_structure` 保存为 Excel 文件，记录每个残基的编号、坐标、距离及是否靠近荧光团；\n",
    "- `distance_matrix.npy` 和 `adjacency_matrix.npy` 作为结构参考图保存，用于后续建模。### 3.1.2 PDB结构点位筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae612423-2bd1-48ec-98f1-bff1f0f240f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: biopython in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (1.85)\n",
      "Requirement already satisfied: numpy in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from biopython) (1.24.2)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m    Residue_ID Residue_Name                   CA_Coord  Dist_to_Chromophore  \\\n",
      "0            1          MET    [9.345, 3.276, -20.995]            23.483324   \n",
      "1            2          SER    [9.859, 6.183, -18.551]            22.154364   \n",
      "2            3          LYS   [13.126, 6.773, -16.595]            23.009256   \n",
      "3            4          GLY   [11.065, 6.289, -13.396]            19.327999   \n",
      "4            5          GLU    [10.499, 2.585, -14.37]            19.062248   \n",
      "5            6          GLU   [14.205, 1.858, -13.541]            21.406456   \n",
      "6            7          LEU     [13.33, 2.541, -9.837]            18.751091   \n",
      "7            8          PHE   [10.872, -0.455, -9.823]            16.751165   \n",
      "8            9          THR  [13.057, -3.255, -11.346]            19.922075   \n",
      "9           10          GLY   [13.411, -4.927, -7.895]            19.264715   \n",
      "10          11          VAL   [11.765, -5.139, -4.468]            16.971924   \n",
      "11          12          VAL   [11.189, -1.605, -3.109]            15.061995   \n",
      "12          13          PRO    [10.541, -0.858, 0.611]            14.283325   \n",
      "13          14          ILE        [7.254, 1.0, 1.266]            10.937656   \n",
      "14          15          LEU      [6.248, 3.388, 4.048]            11.073471   \n",
      "15          16          VAL      [2.599, 4.509, 4.457]             8.662785   \n",
      "16          17          GLU      [1.557, 7.274, 6.873]            10.925240   \n",
      "17          18          LEU      [-2.09, 8.319, 7.353]            10.829897   \n",
      "18          19          ASP     [-3.608, 11.02, 9.549]            14.159023   \n",
      "19          20          GLY    [-7.394, 10.604, 9.711]            14.592321   \n",
      "20          21          ASP   [-10.438, 12.43, 11.115]            17.742678   \n",
      "21          22          VAL   [-13.895, 10.81, 10.576]            18.200293   \n",
      "22          23          ASN    [-16.817, 12.64, 12.29]            21.931238   \n",
      "23          24          GLY   [-14.195, 14.113, 14.75]            22.887243   \n",
      "24          25          HIS  [-12.786, 10.638, 15.608]            21.229961   \n",
      "25          26          LYS    [-9.05, 11.302, 15.093]            19.723705   \n",
      "26          27          PHE    [-6.746, 8.389, 14.246]            17.168442   \n",
      "27          28          SER    [-3.322, 7.668, 12.724]            15.145935   \n",
      "28          29          VAL    [-2.079, 4.607, 10.784]            12.430024   \n",
      "29          30          SER     [1.503, 3.559, 10.003]            12.393362   \n",
      "30          31          GLY      [2.129, 0.945, 7.291]            10.216804   \n",
      "31          32          GLU     [5.362, -0.836, 6.329]            11.884653   \n",
      "32          33          GLY      [6.287, -3.489, 3.77]            12.140051   \n",
      "33          34          GLU     [7.453, -4.009, 0.193]            12.398718   \n",
      "34          35          GLY    [6.364, -3.807, -3.444]            11.469245   \n",
      "35          36          ASP    [7.478, -5.776, -6.509]            14.262819   \n",
      "36          37          ALA    [6.197, -4.095, -9.686]            14.058687   \n",
      "37          38          THR     [7.509, -7.0, -11.873]            17.585272   \n",
      "38          39          TYR   [4.753, -9.238, -10.419]            16.528776   \n",
      "39          40          GLY    [2.388, -6.373, -9.505]            12.999534   \n",
      "40          41          LYS     [2.698, -7.51, -5.851]            12.083776   \n",
      "41          42          LEU     [2.319, -5.58, -2.566]             9.473211   \n",
      "42          43          THR     [2.911, -6.979, 0.956]            11.093336   \n",
      "43          44          LEU     [1.984, -4.453, 3.692]             9.672593   \n",
      "44          45          LYS     [1.265, -4.393, 7.464]            11.722281   \n",
      "45          46          PHE    [-0.778, -1.442, 8.793]            10.922720   \n",
      "46          47          ILE   [-1.087, -0.519, 12.496]            14.171150   \n",
      "47          48          CYS    [-3.465, 2.002, 14.077]            15.360450   \n",
      "48          49          THR     [-1.01, 4.011, 16.248]            17.810307   \n",
      "49          50          THR    [-3.806, 5.876, 18.143]            19.832771   \n",
      "\n",
      "   Nearby_Chromophore  \n",
      "0                  No  \n",
      "1                  No  \n",
      "2                  No  \n",
      "3                  No  \n",
      "4                  No  \n",
      "5                  No  \n",
      "6                  No  \n",
      "7                  No  \n",
      "8                  No  \n",
      "9                  No  \n",
      "10                 No  \n",
      "11                 No  \n",
      "12                 No  \n",
      "13                 No  \n",
      "14                 No  \n",
      "15                Yes  \n",
      "16                 No  \n",
      "17                 No  \n",
      "18                 No  \n",
      "19                 No  \n",
      "20                 No  \n",
      "21                 No  \n",
      "22                 No  \n",
      "23                 No  \n",
      "24                 No  \n",
      "25                 No  \n",
      "26                 No  \n",
      "27                 No  \n",
      "28                 No  \n",
      "29                 No  \n",
      "30                 No  \n",
      "31                 No  \n",
      "32                 No  \n",
      "33                 No  \n",
      "34                 No  \n",
      "35                 No  \n",
      "36                 No  \n",
      "37                 No  \n",
      "38                 No  \n",
      "39                 No  \n",
      "40                 No  \n",
      "41                Yes  \n",
      "42                 No  \n",
      "43                Yes  \n",
      "44                 No  \n",
      "45                 No  \n",
      "46                 No  \n",
      "47                 No  \n",
      "48                 No  \n",
      "49                 No  \n",
      "\n",
      "邻接矩阵维度: (238, 238)\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython\n",
    "\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# --- 1. 载入 PDB 文件 ---\n",
    "pdb_file = \"AF-P42212-F1-model_v4.pdb\"\n",
    "\n",
    "# --- 2. 读取结构 ---\n",
    "parser = PDB.PDBParser(QUIET=True)\n",
    "structure = parser.get_structure(\"avGFP\", pdb_file)\n",
    "model = structure[0]\n",
    "chain = list(model.get_chains())[0]\n",
    "\n",
    "# --- 3. 提取残基信息 ---\n",
    "residue_info = []\n",
    "ca_coords = []\n",
    "res_indices = []\n",
    "\n",
    "for residue in chain:\n",
    "    res_id = residue.get_id()\n",
    "    if res_id[0] == \" \":  # 排除水分子/杂原子\n",
    "        resseq = res_id[1]\n",
    "        resname = residue.get_resname()\n",
    "        ca = residue[\"CA\"] if \"CA\" in residue else None\n",
    "        coord = ca.coord if ca is not None else None  # ✅ 显式判断\n",
    "        residue_info.append((resseq, resname, coord))\n",
    "        if coord is not None:\n",
    "            ca_coords.append(coord)\n",
    "            res_indices.append(resseq)\n",
    "\n",
    "# --- 4. 构建 DataFrame ---\n",
    "df_structure = pd.DataFrame(residue_info, columns=[\"Residue_ID\", \"Residue_Name\", \"CA_Coord\"])\n",
    "\n",
    "# --- 修复点 1：确保 Residue_ID 为整数 ---\n",
    "df_structure[\"Residue_ID\"] = pd.to_numeric(df_structure[\"Residue_ID\"], errors=\"coerce\")\n",
    "\n",
    "# --- 修复点 2：正确提取荧光团坐标 ---\n",
    "chromophore_residues = [64, 65, 66]\n",
    "chromo_rows = df_structure[df_structure[\"Residue_ID\"].isin(chromophore_residues)]\n",
    "\n",
    "# 只保留有有效坐标的行\n",
    "chromophore_coords = [np.array(coord) for coord in chromo_rows[\"CA_Coord\"] if isinstance(coord, np.ndarray) and coord.shape == (3,)]\n",
    "\n",
    "# --- 5. 计算荧光团中心 ---\n",
    "chromophore_center = np.mean(chromophore_coords, axis=0) if len(chromophore_coords) == 3 else None\n",
    "\n",
    "# --- 修复点 3：计算距离函数 robust 化 ---\n",
    "def compute_distance(coord):\n",
    "    if isinstance(coord, str):  # 有些 DataFrame 会存成字符串\n",
    "        coord = np.fromstring(coord.strip(\"[]\"), sep=\" \")\n",
    "    if chromophore_center is None or coord is None or len(coord) != 3:\n",
    "        return np.nan\n",
    "    return np.linalg.norm(coord - chromophore_center)\n",
    "\n",
    "# --- 6. 应用距离计算 ---\n",
    "df_structure[\"Dist_to_Chromophore\"] = df_structure[\"CA_Coord\"].apply(compute_distance)\n",
    "\n",
    "# --- 7. 标注是否靠近荧光团（<10 Å） ---\n",
    "df_structure[\"Nearby_Chromophore\"] = df_structure[\"Dist_to_Chromophore\"].apply(\n",
    "    lambda d: \"Yes\" if pd.notna(d) and d < 10 else \"No\"\n",
    ")\n",
    "\n",
    "# --- 8. 生成邻接矩阵 ---\n",
    "ca_array = np.array(ca_coords)\n",
    "distance_matrix = cdist(ca_array, ca_array)  # 欧几里得距离\n",
    "threshold = 8.0\n",
    "adj_matrix = (distance_matrix < threshold).astype(int)\n",
    "\n",
    "# --- 9. 保存结构信息和邻接矩阵 ---\n",
    "df_structure.to_excel(\"GFP_PDB_structure_enhanced.xlsx\", index=False)\n",
    "np.save(\"GFP_CA_distance_matrix.npy\", distance_matrix)\n",
    "np.save(\"GFP_CA_adjacency_matrix.npy\", adj_matrix)\n",
    "\n",
    "# --- 10. 打印摘要 ---\n",
    "print(df_structure.head(50))\n",
    "print(\"\\n邻接矩阵维度:\", adj_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18802a3-9da7-44e9-be4d-9f25421451b3",
   "metadata": {},
   "source": [
    "## 3.1.3 DPSS：近似结构分析筛选突变位点\n",
    "\n",
    "该策略不依赖昂贵的全原子能量计算或表面积算法，而是利用结构邻接矩阵和荧光团距离近似评估残基的**可突变性（mutation eligibility）**。其设计逻辑包括：\n",
    "\n",
    "------\n",
    "\n",
    "### 获取基础结构数据\n",
    "\n",
    "- 从先前保存的结构表格 `GFP_PDB_structure_enhanced.xlsx` 中加载所有残基的三维坐标、距离荧光团中心等；\n",
    "- 同时加载残基之间的邻接矩阵 `adj_matrix` 和欧几里得距离矩阵 `distance_matrix`，这两者用于近似估计每个残基的结构环境与暴露度。\n",
    "\n",
    "------\n",
    "\n",
    "### 计算每个残基的邻接数（degree）\n",
    "\n",
    "- 每个残基的邻接数即为它与多少其他残基“相邻”（距离 < 8 Å）；\n",
    "- 数值越大，表示该残基埋藏在内部、结构紧密；\n",
    "- 数值越小，则可能位于表面或缺乏结构约束。\n",
    "\n",
    "该信息存入字段 `Contact_Degree`。\n",
    "\n",
    "------\n",
    "\n",
    "### 判断是否“暴露”（Approx_Exposure）\n",
    "\n",
    "- 设置经验阈值：`Contact_Degree <= 6` 即判定为暴露（Exposed）；\n",
    "- 否则判定为埋藏（Buried）；\n",
    "- 该标准为经验规则，尽管粗略但在无溶剂可接触面积数据时是合理替代。\n",
    "\n",
    "------\n",
    "\n",
    "### 结合荧光团距离进一步过滤\n",
    "\n",
    "- 仅考虑 `Nearby_Chromophore == \"No\"` 的位点（即距离荧光团超过 10 Å）；\n",
    "- 避免突变影响荧光活性中心区域，保障功能不被破坏；\n",
    "- 联合判断是否暴露与远离功能核心，两者均满足才标记为 `Mutation_Eligible = True`。\n",
    "\n",
    "------\n",
    "\n",
    "### 输出推荐突变位点清单\n",
    "\n",
    "最终筛选出所有满足条件的突变位点，排序策略为：\n",
    "\n",
    "1. **优先远离荧光团（Dist_to_Chromophore 高）**\n",
    "2. **优先低接触度（Contact_Degree 小）**\n",
    "\n",
    "即优先推荐**最暴露且最远离功能核心的表面残基**，可认为它们既“安全”，又“对整体结构和功能干扰最小”，适合用于采样更多样化的突变组合。\n",
    "\n",
    "结果保存为 Excel 文件 `\"推荐突变位点（近似结构分析）.xlsx\"`，用于 VAE 突变位点选择控制。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a924890-24ce-4eb6-b26b-3c255e4c2850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 结构信息 + 邻接矩阵 + 距离矩阵\n",
    "df_structure = pd.read_excel(\"GFP_PDB_structure_enhanced.xlsx\",engine='openpyxl')\n",
    "adj_matrix = np.load(\"GFP_CA_adjacency_matrix.npy\")\n",
    "distance_matrix = np.load(\"GFP_CA_distance_matrix.npy\")\n",
    "\n",
    "# 每个残基的连接数（degree）\n",
    "degree = adj_matrix.sum(axis=1)\n",
    "df_structure[\"Contact_Degree\"] = degree\n",
    "\n",
    "# 近似暴露度判断：连接数小于等于6视为暴露\n",
    "df_structure[\"Approx_Exposure\"] = df_structure[\"Contact_Degree\"].apply(\n",
    "    lambda x: \"Exposed\" if x <= 6 else \"Buried\"\n",
    ")\n",
    "\n",
    "# 综合打分策略：仅保留远离chromophore + 暴露位点\n",
    "df_structure[\"Mutation_Eligible\"] = df_structure.apply(\n",
    "    lambda row: row[\"Nearby_Chromophore\"] == \"No\" and row[\"Approx_Exposure\"] == \"Exposed\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 提取推荐突变位点\n",
    "eligible_sites = df_structure[df_structure[\"Mutation_Eligible\"] == True][\n",
    "    [\"Residue_ID\", \"Residue_Name\", \"Dist_to_Chromophore\", \"Contact_Degree\"]\n",
    "].sort_values(by=[\"Dist_to_Chromophore\", \"Contact_Degree\"], ascending=[False, True])\n",
    "eligible_sites.to_excel(\"推荐突变位点（近似结构分析）.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf8ae466d6b112",
   "metadata": {},
   "source": [
    "## 3.1.4 多信息源整合突变位点筛选与AvoidRegion排查\n",
    "\n",
    "此方法集成了三类突变位点来源：\n",
    "\n",
    "| 来源类别         | 说明                                                      |\n",
    "| ---------------- | --------------------------------------------------------- |\n",
    "| **频率统计**     | 来自训练集中突变频次最高的位点（数据驱动）                |\n",
    "| **结构启发**     | 靠近蛋白质表面且远离荧光团的暴露残基（结构生物学启发）    |\n",
    "| **文献经典位点** | 来自公开文献中已知对 GFP 功能有调控作用的位点（领域知识） |\n",
    "\n",
    "### 残基标注：来源标识与区域排除\n",
    "\n",
    "在结构残基表中为每一个 Residue_ID 添加三列布尔字段：\n",
    "\n",
    "- `From_Literature`: 是否来源于文献；\n",
    "- `From_Frequency`: 是否为突变高频位点；\n",
    "- `From_Exposure`: 是否为暴露远离功能区的残基。\n",
    "\n",
    "同时添加排除标记 `Avoid_Region`：例如 chromophore（荧光团核心）残基（65–67）将被打上负面标签，避免进入突变区域。\n",
    "背景：\n",
    "\n",
    "- S65、Y66、G67 是 GFP 荧光团的核心，突变它们：\n",
    "  - 很容易破坏发光结构；\n",
    "  - 失去功能或荧光消失；\n",
    "  - 非常“危险”，除非有特别目的（如红移或蓝移发光谱）。\n",
    "\n",
    "#### 所以：\n",
    "\n",
    "要避免在这些高度关键区域“乱动”，除非你做的是染色体调节或已知工程突变（如 S65T 是 sfGFP 的经典突变）。\n",
    "\n",
    "------\n",
    "\n",
    "###  多因素综合打分机制\n",
    "\n",
    "定义如下线性加权策略：\n",
    "\n",
    "```\n",
    "得分 = 1.5 × 文献支持 + 1.0 × 频率支持 + 1.5 × 暴露结构支持\n",
    "```\n",
    "\n",
    "- 文献和结构依据加权更高，体现“可信生物学指导”；\n",
    "- 频率维度作为数据驱动支持，用于提升建模覆盖面；\n",
    "- 被标记为 `Avoid_Region` 的残基直接赋值为 -1，强行排除。\n",
    "\n",
    "该打分策略简单、可解释，能够在有限空间内聚焦那些“既被使用频繁、又安全且潜在有效”的位点。\n",
    "\n",
    "最终以这些来源为特征，构建简单加权评分体系，输出 Top-K 位点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ce7e31-d6d2-4bc9-8553-e27ec79d9ada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:00:21.465953Z",
     "start_time": "2025-06-01T08:00:18.604969Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GFP_PDB_structure_enhanced.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# 加载输入文件\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m df_struct \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_excel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGFP_PDB_structure_enhanced.xlsx\u001B[39m\u001B[38;5;124m\"\u001B[39m,engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopenpyxl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      8\u001B[0m df_exposure \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_excel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m推荐突变位点（近似结构分析）.xlsx\u001B[39m\u001B[38;5;124m\"\u001B[39m,engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopenpyxl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      9\u001B[0m df_lit \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGFP_经典突变位点表.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001B[0m, in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001B[0m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[0;32m    494\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 495\u001B[0m     io \u001B[38;5;241m=\u001B[39m ExcelFile(\n\u001B[0;32m    496\u001B[0m         io,\n\u001B[0;32m    497\u001B[0m         storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    498\u001B[0m         engine\u001B[38;5;241m=\u001B[39mengine,\n\u001B[0;32m    499\u001B[0m         engine_kwargs\u001B[38;5;241m=\u001B[39mengine_kwargs,\n\u001B[0;32m    500\u001B[0m     )\n\u001B[0;32m    501\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[0;32m    502\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    503\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    505\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1567\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001B[0m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine \u001B[38;5;241m=\u001B[39m engine\n\u001B[0;32m   1565\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstorage_options \u001B[38;5;241m=\u001B[39m storage_options\n\u001B[1;32m-> 1567\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engines[engine](\n\u001B[0;32m   1568\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_io,\n\u001B[0;32m   1569\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   1570\u001B[0m     engine_kwargs\u001B[38;5;241m=\u001B[39mengine_kwargs,\n\u001B[0;32m   1571\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:553\u001B[0m, in \u001B[0;36mOpenpyxlReader.__init__\u001B[1;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001B[0m\n\u001B[0;32m    541\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    542\u001B[0m \u001B[38;5;124;03mReader using openpyxl engine.\u001B[39;00m\n\u001B[0;32m    543\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    550\u001B[0m \u001B[38;5;124;03m    Arbitrary keyword arguments passed to excel engine.\u001B[39;00m\n\u001B[0;32m    551\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    552\u001B[0m import_optional_dependency(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenpyxl\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 553\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    554\u001B[0m     filepath_or_buffer,\n\u001B[0;32m    555\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    556\u001B[0m     engine_kwargs\u001B[38;5;241m=\u001B[39mengine_kwargs,\n\u001B[0;32m    557\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:563\u001B[0m, in \u001B[0;36mBaseExcelReader.__init__\u001B[1;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m IOHandles(\n\u001B[0;32m    560\u001B[0m     handle\u001B[38;5;241m=\u001B[39mfilepath_or_buffer, compression\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmethod\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mNone\u001B[39;00m}\n\u001B[0;32m    561\u001B[0m )\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(filepath_or_buffer, (ExcelFile, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workbook_class)):\n\u001B[1;32m--> 563\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[0;32m    564\u001B[0m         filepath_or_buffer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m, storage_options\u001B[38;5;241m=\u001B[39mstorage_options, is_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    565\u001B[0m     )\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workbook_class):\n\u001B[0;32m    568\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\io\\common.py:882\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    874\u001B[0m             handle,\n\u001B[0;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    879\u001B[0m         )\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m--> 882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n\u001B[0;32m    883\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[0;32m    885\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'GFP_PDB_structure_enhanced.xlsx'"
     ]
    }
   ],
   "source": [
    "# 整合所有突变信息来源，构建 Top-20 位点选择完整流程代码\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 加载输入文件\n",
    "df_struct = pd.read_excel(\"GFP_PDB_structure_enhanced.xlsx\",engine='openpyxl')\n",
    "df_exposure = pd.read_excel(\"推荐突变位点（近似结构分析）.xlsx\",engine='openpyxl')\n",
    "df_lit = pd.read_csv(\"GFP_经典突变位点表.csv\")\n",
    "df_freq = pd.read_csv(\"top_positions.csv\")\n",
    "\n",
    "# 处理文献位点（如 F64）提取编号\n",
    "def extract_position(val):\n",
    "    match = re.search(r'(\\d+)', str(val))\n",
    "    return int(match.group(1)) if match else None\n",
    "df_lit[\"Residue_ID\"] = df_lit[\"位点\"].apply(extract_position)\n",
    "\n",
    "# 提取残基编号\n",
    "lit_positions = df_lit[\"Residue_ID\"].dropna().astype(int).tolist()\n",
    "freq_positions = df_freq[\"Position\"].tolist()\n",
    "exposure_positions = df_exposure[\"Residue_ID\"].tolist()\n",
    "\n",
    "# 标记信息来源\n",
    "df_struct[\"From_Literature\"] = df_struct[\"Residue_ID\"].apply(lambda x: int(x) in lit_positions)\n",
    "df_struct[\"From_Frequency\"] = df_struct[\"Residue_ID\"].apply(lambda x: int(x) in freq_positions)\n",
    "df_struct[\"From_Exposure\"] = df_struct[\"Residue_ID\"].apply(lambda x: int(x) in exposure_positions)\n",
    "\n",
    "# 排除 chromophore 核心\n",
    "df_struct[\"Avoid_Region\"] = df_struct[\"Residue_ID\"].apply(lambda x: x in [65, 66, 67])\n",
    "\n",
    "# 打分函数\n",
    "def score(row):\n",
    "    if row[\"Avoid_Region\"]:\n",
    "        return -1\n",
    "    return 1.5 * row[\"From_Literature\"] + 1.0 * row[\"From_Frequency\"] + 1.5 * row[\"From_Exposure\"]\n",
    "\n",
    "# 计算得分并排序\n",
    "df_struct[\"Mutation_Score\"] = df_struct.apply(score, axis=1)\n",
    "top20_mutation_sites = df_struct.sort_values(by=\"Mutation_Score\", ascending=False)\n",
    "top20_mutation_sites = top20_mutation_sites[top20_mutation_sites[\"Mutation_Score\"] > 0].head(20)\n",
    "\n",
    "# 保存输出\n",
    "output_path = \"Top20_Mutation_Sites.csv\"\n",
    "top20_mutation_sites.to_csv(output_path, index=False)\n",
    "print('Top20_Mutation_Sites.csv is saved')\n",
    "print(top20_mutation_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40a8a7-61ea-412b-b215-90dd2ead2e20",
   "metadata": {},
   "source": [
    "## 表格列含义解释\n",
    "\n",
    "| 列名                    | 含义                                       | 解释                                                         |\n",
    "| ----------------------- | ------------------------------------------ | ------------------------------------------------------------ |\n",
    "| **Residue_ID**          | 残基编号                                   | GFP 蛋白中该氨基酸的序号，对应 `avGFP` 的标准编号            |\n",
    "| **Residue_Name**        | 三字母残基名称                             | 例如：`TYR`, `GLN`, `VAL`，即 Tyr, Gln, Val 等               |\n",
    "| **CA_Coord**            | α-碳坐标                                   | 残基的三维空间坐标（可用于结构绘图或建模）                   |\n",
    "| **Dist_to_Chromophore** | 距离 chromophore 的空间距离                | 从当前残基到 chromophore 中心（S65-Y66-G67）的欧几里得距离，单位 Å |\n",
    "| **Nearby_Chromophore**  | 是否靠近 chromophore（<10Å）               | Yes/No 标记；用于避免核心光团周围不宜突变的位点              |\n",
    "| **From_Literature**     | 来自文献支持                               | True/False：是否被 sfGFP/EGFP 等文献确认为重要突变位点       |\n",
    "| **From_Frequency**      | 来自训练数据统计                           | True/False：是否在 `GFP_data.xlsx` 中高频出现且关联高亮度    |\n",
    "| **From_Exposure**       | 来自近似结构暴露分析                       | True/False：是否是暴露残基（邻接数低），适合突变             |\n",
    "| **Avoid_Region**        | 是否是 chromophore 核心区域（S65-Y66-Y67） | True 表示禁止突变，已排除                                    |\n",
    "| **Mutation_Score**      | 综合打分                                   | 评分规则如下                                                |\n",
    "\n",
    "\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82fb4c-a9b1-4456-86fd-301bb3d54ca9",
   "metadata": {},
   "source": [
    "## 阶段 3.2：基于多策略的突变残基候选推荐\n",
    "\n",
    "### 目标\n",
    "\n",
    "为每个已确定的突变位点，推荐一组**可能保留蛋白功能、结构更合理的突变残基集合**，用于构建最终的突变组合（Mutation Library）并输入到生成模型（如 VAE）或筛选系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96912a8c-5656-4733-9583-404e6541bf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>位点</th>\n",
       "      <th>野生型残基</th>\n",
       "      <th>经典突变</th>\n",
       "      <th>功能类别</th>\n",
       "      <th>功能说明</th>\n",
       "      <th>文献来源</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F64</td>\n",
       "      <td>Phe</td>\n",
       "      <td>F64L</td>\n",
       "      <td>折叠 &amp; 热稳增强</td>\n",
       "      <td>改善37°C表达、稳定性更高</td>\n",
       "      <td>Cormack et al., 1996; Heim et al., 1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S65</td>\n",
       "      <td>Ser</td>\n",
       "      <td>S65T</td>\n",
       "      <td>亮度提升</td>\n",
       "      <td>激发波长从395 → 488nm，量子产率提升</td>\n",
       "      <td>Heim et al., 1995; Tsien, 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y66</td>\n",
       "      <td>Tyr</td>\n",
       "      <td>Y66H/F/W</td>\n",
       "      <td>色调调节</td>\n",
       "      <td>改变荧光团性质，调节颜色（蓝光/红移）</td>\n",
       "      <td>Ormö et al., 1996; Tsien, 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T203</td>\n",
       "      <td>Thr</td>\n",
       "      <td>T203Y/F</td>\n",
       "      <td>发射峰红移 &amp; 亮度增强</td>\n",
       "      <td>与荧光团 π-π stacking，提高亮度</td>\n",
       "      <td>Zhang et al., 2006; Tsien, 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M153</td>\n",
       "      <td>Met</td>\n",
       "      <td>M153T</td>\n",
       "      <td>折叠 &amp; 热稳增强</td>\n",
       "      <td>Superfolder GFP 中的核心突变</td>\n",
       "      <td>Pédelacq et al., 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>V163</td>\n",
       "      <td>Val</td>\n",
       "      <td>V163A</td>\n",
       "      <td>热稳定性提升</td>\n",
       "      <td>疏水核区域，小残基更利于折叠</td>\n",
       "      <td>Pédelacq et al., 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A206</td>\n",
       "      <td>Ala</td>\n",
       "      <td>A206K</td>\n",
       "      <td>抑制二聚化</td>\n",
       "      <td>阻止 GFP 在高浓度下形成二聚体</td>\n",
       "      <td>Zacharias et al., 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Y145</td>\n",
       "      <td>Tyr</td>\n",
       "      <td>Y145F</td>\n",
       "      <td>结构稳定性</td>\n",
       "      <td>内部氢键优化，提高桶结构稳定性</td>\n",
       "      <td>Jung et al., 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>N149</td>\n",
       "      <td>Asn</td>\n",
       "      <td>N149K</td>\n",
       "      <td>可溶性增强</td>\n",
       "      <td>表面极性变化，改善表达</td>\n",
       "      <td>Shaner et al., 2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S30</td>\n",
       "      <td>Ser</td>\n",
       "      <td>S30R</td>\n",
       "      <td>热稳增强</td>\n",
       "      <td>桶口桥联，防止展开</td>\n",
       "      <td>Pédelacq et al., 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>V68</td>\n",
       "      <td>Val</td>\n",
       "      <td>V68L/A</td>\n",
       "      <td>折叠增强</td>\n",
       "      <td>连接光团的支架，增强折叠</td>\n",
       "      <td>Baird et al., 2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>E222</td>\n",
       "      <td>Glu</td>\n",
       "      <td>E222G/Q</td>\n",
       "      <td>光谱调节</td>\n",
       "      <td>影响质子传递路径（pKa 调控）</td>\n",
       "      <td>Elsliger et al., 1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      位点 野生型残基      经典突变          功能类别                     功能说明  \\\n",
       "0    F64   Phe      F64L     折叠 & 热稳增强           改善37°C表达、稳定性更高   \n",
       "1    S65   Ser      S65T          亮度提升  激发波长从395 → 488nm，量子产率提升   \n",
       "2    Y66   Tyr  Y66H/F/W          色调调节      改变荧光团性质，调节颜色（蓝光/红移）   \n",
       "3   T203   Thr   T203Y/F  发射峰红移 & 亮度增强   与荧光团 π-π stacking，提高亮度   \n",
       "4   M153   Met     M153T     折叠 & 热稳增强   Superfolder GFP 中的核心突变   \n",
       "5   V163   Val     V163A        热稳定性提升           疏水核区域，小残基更利于折叠   \n",
       "6   A206   Ala     A206K         抑制二聚化        阻止 GFP 在高浓度下形成二聚体   \n",
       "7   Y145   Tyr     Y145F         结构稳定性          内部氢键优化，提高桶结构稳定性   \n",
       "8   N149   Asn     N149K         可溶性增强              表面极性变化，改善表达   \n",
       "9    S30   Ser      S30R          热稳增强                桶口桥联，防止展开   \n",
       "10   V68   Val    V68L/A          折叠增强             连接光团的支架，增强折叠   \n",
       "11  E222   Glu   E222G/Q          光谱调节         影响质子传递路径（pKa 调控）   \n",
       "\n",
       "                                       文献来源  \n",
       "0   Cormack et al., 1996; Heim et al., 1995  \n",
       "1            Heim et al., 1995; Tsien, 1998  \n",
       "2            Ormö et al., 1996; Tsien, 1998  \n",
       "3           Zhang et al., 2006; Tsien, 1998  \n",
       "4                     Pédelacq et al., 2006  \n",
       "5                     Pédelacq et al., 2006  \n",
       "6                    Zacharias et al., 2002  \n",
       "7                         Jung et al., 2005  \n",
       "8                       Shaner et al., 2004  \n",
       "9                     Pédelacq et al., 2006  \n",
       "10                       Baird et al., 2000  \n",
       "11                    Elsliger et al., 1999  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取经典突变位点参考表\n",
    "classic_mutation_path = \"GFP_经典突变位点表.csv\"\n",
    "classic_mut_df = pd.read_csv(classic_mutation_path)\n",
    "\n",
    "# 查看数据结构\n",
    "classic_mut_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b5885d-7138-41e9-a9eb-7594d1d62712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{149: ['ASN'],\n",
       " 153: ['MET'],\n",
       " 214: ['LYS'],\n",
       " 212: ['ASN'],\n",
       " 68: ['VAL'],\n",
       " 64: ['PHE'],\n",
       " 222: ['GLU'],\n",
       " 30: ['SER'],\n",
       " 145: ['TYR'],\n",
       " 206: ['ALA'],\n",
       " 203: ['THR'],\n",
       " 163: ['VAL'],\n",
       " 130: ['PHE'],\n",
       " 128: ['ILE'],\n",
       " 136: ['ILE'],\n",
       " 131: ['LYS'],\n",
       " 135: ['ASN'],\n",
       " 121: ['ASN'],\n",
       " 140: ['LYS'],\n",
       " 126: ['LYS']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载 Top20 位点信息\n",
    "top20_path = \"Top20_Mutation_Sites.csv\"\n",
    "top20_df = pd.read_csv(top20_path)\n",
    "\n",
    "# 建立突变位点到候选残基的映射\n",
    "mutation_candidates = {}\n",
    "\n",
    "# 建立经典突变位点映射表（便于查找）\n",
    "classic_mut_map = classic_mut_df.groupby(\"位点\")[\"经典突变\"].apply(list).to_dict()\n",
    "classic_wt_map = classic_mut_df.groupby(\"位点\")[\"野生型残基\"].first().to_dict()\n",
    "\n",
    "for _, row in top20_df.iterrows():\n",
    "    pos = int(row[\"Residue_ID\"])\n",
    "    wt = row[\"Residue_Name\"]\n",
    "    candidates = set()\n",
    "    candidates.add(wt)  # 保留野生型残基\n",
    "\n",
    "    # 如果该位点出现在经典突变表中\n",
    "    if pos in classic_mut_map:\n",
    "        for mut in classic_mut_map[pos]:\n",
    "            if len(mut) >= 3 and mut[:len(str(pos))*(-1)] == wt:\n",
    "                alt = mut[len(wt):-len(str(pos))]\n",
    "            else:\n",
    "                alt = mut[len(wt):-len(str(pos))] if mut.startswith(wt) else mut[-1]\n",
    "            if alt.isalpha() and len(alt) == 1:\n",
    "                candidates.add(alt)\n",
    "\n",
    "    mutation_candidates[pos] = sorted(list(candidates))\n",
    "\n",
    "mutation_candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95f1b98bbf0053",
   "metadata": {},
   "source": [
    "### 3.2.1 BLOSUM62 相似性筛选\n",
    "\n",
    "#### 方法概述\n",
    "\n",
    "BLOSUM62 是基于保守性统计构建的替换矩阵，得分高表示某个氨基酸替换在自然进化中较为常见，因此保守性高、破坏性小。\n",
    "\n",
    "#### 实现步骤：\n",
    "\n",
    "- 对于每个位点，取 WT 残基（如 F64）；\n",
    "- 在 BLOSUM62 中寻找对 WT 残基打分 ≥ 1 的残基，认为它们在演化上相似；\n",
    "- 最终形成：`{Residue_ID: [WT, AA1, AA2, ..., AAk]}` 的突变残基列表；\n",
    "- 输出为 `\"候选突变残基推荐表.xlsx\"`，便于查阅和用于突变控制。\n",
    "\n",
    "#### 优势：\n",
    "\n",
    "- 有生物学合理性，能减少突变导致的功能丧失；\n",
    "- 可作为“保守突变策略”的基础突变集合；\n",
    "- 可与结构或 PLM 模型互补（避免纯模型生成的偏差）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3271b2b9-7bb5-4937-8cac-ff123880d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "候选突变残基推荐表已保存。\n",
      "    Residue_ID   WT             候选残基（三字母）\n",
      "0          149  ASN  [ASN, ASP, HIS, SER]\n",
      "1          153  MET  [ILE, LEU, MET, VAL]\n",
      "2          214  LYS  [ARG, GLN, GLU, LYS]\n",
      "3          212  ASN  [ASN, ASP, HIS, SER]\n",
      "4           68  VAL  [ILE, LEU, MET, VAL]\n",
      "5           64  PHE       [PHE, TRP, TYR]\n",
      "6          222  GLU  [ASP, GLN, GLU, LYS]\n",
      "7           30  SER  [ALA, ASN, SER, THR]\n",
      "8          145  TYR  [HIS, PHE, TRP, TYR]\n",
      "9          206  ALA            [ALA, SER]\n",
      "10         203  THR            [SER, THR]\n",
      "11         163  VAL  [ILE, LEU, MET, VAL]\n",
      "12         130  PHE       [PHE, TRP, TYR]\n",
      "13         128  ILE  [ILE, LEU, MET, VAL]\n",
      "14         136  ILE  [ILE, LEU, MET, VAL]\n",
      "15         131  LYS  [ARG, GLN, GLU, LYS]\n",
      "16         135  ASN  [ASN, ASP, HIS, SER]\n",
      "17         121  ASN  [ASN, ASP, HIS, SER]\n",
      "18         140  LYS  [ARG, GLN, GLU, LYS]\n",
      "19         126  LYS  [ARG, GLN, GLU, LYS]\n"
     ]
    }
   ],
   "source": [
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "# 加载 BLOSUM62\n",
    "blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
    "\n",
    "# 所有20种标准氨基酸缩写\n",
    "amino_acids = [\n",
    "    'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY',\n",
    "    'HIS', 'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER',\n",
    "    'THR', 'TRP', 'TYR', 'VAL'\n",
    "]\n",
    "\n",
    "# 将三字母氨基酸缩写转换为一字母\n",
    "aa_three_to_one = {\n",
    "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
    "    'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "    'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
    "    'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "}\n",
    "\n",
    "aa_one_to_three = {v: k for k, v in aa_three_to_one.items()}\n",
    "\n",
    "# 扩展每个位点的候选残基（基于 BLOSUM62 相似度）\n",
    "expanded_mutation_candidates = {}\n",
    "\n",
    "for pos, residues in mutation_candidates.items():\n",
    "    wt_three = residues[0]  # 第一位是WT残基\n",
    "    wt_one = aa_three_to_one.get(wt_three.upper(), None)\n",
    "    if wt_one is None:\n",
    "        continue\n",
    "\n",
    "    similar_aa = set([wt_three])  # 包含WT残基\n",
    "\n",
    "    # 查找 BLOSUM62 中打分较高的氨基酸\n",
    "    for aa in amino_acids:\n",
    "        aa_one = aa_three_to_one[aa]\n",
    "        key1 = (wt_one, aa_one)\n",
    "        key2 = (aa_one, wt_one)\n",
    "        score = blosum62.get((wt_one, aa_one), blosum62.get((aa_one, wt_one), None))\n",
    "        if score is not None and score >= 1:  # 选择相似度较高的\n",
    "            similar_aa.add(aa)\n",
    "\n",
    "    expanded_mutation_candidates[pos] = sorted(list(similar_aa))\n",
    "\n",
    "import json\n",
    "p_d = pd.DataFrame([\n",
    "        {\"Residue_ID\": k, \"WT\": mutation_candidates[k][0], \"候选残基（三字母）\": v}\n",
    "        for k, v in expanded_mutation_candidates.items()\n",
    "    ])\n",
    "p_d.to_excel(\"候选突变残基推荐表.xlsx\", index=False)\n",
    "print(\"候选突变残基推荐表已保存。\")\n",
    "print(p_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aabd05846971e4",
   "metadata": {},
   "source": [
    "### 3.2.2 基于蛋白语言模型（PLM）的突变倾向预测\n",
    "\n",
    "#### 方法概述\n",
    "\n",
    "使用预训练蛋白语言模型（如 ESM-1b）对野生型序列进行逐位预测，提取每个位点的残基概率分布（log softmax），推荐得分前 5 的残基作为突变候选。\n",
    "\n",
    "#### 实现步骤：\n",
    "\n",
    "1. 加载 avGFP 的 WT 序列；\n",
    "2. 加载 Top20 位点清单；\n",
    "3. 使用 `esm1b_t33_650M_UR50S` 加载模型和字典；\n",
    "4. 对每个位点：\n",
    "   - 提取模型对该位点的残基 logits；\n",
    "   - 计算 log softmax 得分；\n",
    "   - 获取 Top5 残基 + 其对应得分；\n",
    "5. 保存为 `\"PLM_突变候选推荐表.xlsx\"`。\n",
    "\n",
    "#### 输出格式（示例）：\n",
    "\n",
    "| Residue_ID | WT   | Top1 | LogP1 | Top2 | LogP2 | ...  |\n",
    "| ---------- | ---- | ---- | ----- | ---- | ----- | ---- |\n",
    "| 64         | F    | Y    | -1.23 | W    | -1.55 | ...  |\n",
    "\n",
    "\n",
    "\n",
    "#### 优势：\n",
    "\n",
    "- 利用上下文学习蛋白的“语言模式”，能够预测当前位点最自然的变体；\n",
    "- 拥有大规模预训练知识，尤其在缺乏结构或功能注释时依然有效；\n",
    "- 能为“更自由”的突变生成提供支持（激进突变的合理性评估）；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb41f1b-5e09-4c1e-9107-530450caecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 已完成预测，保存为 PLM_突变候选推荐表.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Residue_ID</th>\n",
       "      <th>WT</th>\n",
       "      <th>Top1</th>\n",
       "      <th>LogP1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>LogP2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>LogP3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>LogP4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>LogP5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>ASN</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>E</td>\n",
       "      <td>-3.261</td>\n",
       "      <td>T</td>\n",
       "      <td>-3.314</td>\n",
       "      <td>K</td>\n",
       "      <td>-3.350</td>\n",
       "      <td>D</td>\n",
       "      <td>-3.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153</td>\n",
       "      <td>MET</td>\n",
       "      <td>M</td>\n",
       "      <td>-1.187</td>\n",
       "      <td>E</td>\n",
       "      <td>-2.623</td>\n",
       "      <td>T</td>\n",
       "      <td>-2.722</td>\n",
       "      <td>S</td>\n",
       "      <td>-2.938</td>\n",
       "      <td>D</td>\n",
       "      <td>-2.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214</td>\n",
       "      <td>LYS</td>\n",
       "      <td>K</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>L</td>\n",
       "      <td>-3.658</td>\n",
       "      <td>I</td>\n",
       "      <td>-3.817</td>\n",
       "      <td>P</td>\n",
       "      <td>-3.861</td>\n",
       "      <td>G</td>\n",
       "      <td>-3.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>212</td>\n",
       "      <td>ASN</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>K</td>\n",
       "      <td>-3.586</td>\n",
       "      <td>E</td>\n",
       "      <td>-3.854</td>\n",
       "      <td>S</td>\n",
       "      <td>-3.866</td>\n",
       "      <td>D</td>\n",
       "      <td>-3.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68</td>\n",
       "      <td>VAL</td>\n",
       "      <td>V</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>K</td>\n",
       "      <td>-3.254</td>\n",
       "      <td>G</td>\n",
       "      <td>-3.294</td>\n",
       "      <td>E</td>\n",
       "      <td>-3.313</td>\n",
       "      <td>D</td>\n",
       "      <td>-3.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>PHE</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>D</td>\n",
       "      <td>-2.911</td>\n",
       "      <td>G</td>\n",
       "      <td>-3.272</td>\n",
       "      <td>T</td>\n",
       "      <td>-3.348</td>\n",
       "      <td>S</td>\n",
       "      <td>-3.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>222</td>\n",
       "      <td>GLU</td>\n",
       "      <td>E</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>K</td>\n",
       "      <td>-3.469</td>\n",
       "      <td>G</td>\n",
       "      <td>-3.545</td>\n",
       "      <td>S</td>\n",
       "      <td>-3.612</td>\n",
       "      <td>D</td>\n",
       "      <td>-3.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>SER</td>\n",
       "      <td>S</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>K</td>\n",
       "      <td>-3.257</td>\n",
       "      <td>T</td>\n",
       "      <td>-3.368</td>\n",
       "      <td>E</td>\n",
       "      <td>-3.443</td>\n",
       "      <td>V</td>\n",
       "      <td>-3.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>145</td>\n",
       "      <td>TYR</td>\n",
       "      <td>Y</td>\n",
       "      <td>-0.818</td>\n",
       "      <td>E</td>\n",
       "      <td>-2.617</td>\n",
       "      <td>P</td>\n",
       "      <td>-2.905</td>\n",
       "      <td>K</td>\n",
       "      <td>-2.961</td>\n",
       "      <td>G</td>\n",
       "      <td>-3.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>206</td>\n",
       "      <td>ALA</td>\n",
       "      <td>A</td>\n",
       "      <td>-0.634</td>\n",
       "      <td>I</td>\n",
       "      <td>-2.974</td>\n",
       "      <td>L</td>\n",
       "      <td>-3.027</td>\n",
       "      <td>F</td>\n",
       "      <td>-3.175</td>\n",
       "      <td>V</td>\n",
       "      <td>-3.180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Residue_ID   WT Top1  LogP1 Top2  LogP2 Top3  LogP3 Top4  LogP4 Top5  LogP5\n",
       "0         149  ASN    N -0.489    E -3.261    T -3.314    K -3.350    D -3.500\n",
       "1         153  MET    M -1.187    E -2.623    T -2.722    S -2.938    D -2.967\n",
       "2         214  LYS    K -0.316    L -3.658    I -3.817    P -3.861    G -3.891\n",
       "3         212  ASN    N -0.279    K -3.586    E -3.854    S -3.866    D -3.932\n",
       "4          68  VAL    V -0.519    K -3.254    G -3.294    E -3.313    D -3.395\n",
       "5          64  PHE    L -0.552    D -2.911    G -3.272    T -3.348    S -3.368\n",
       "6         222  GLU    E -0.367    K -3.469    G -3.545    S -3.612    D -3.693\n",
       "7          30  SER    S -0.319    K -3.257    T -3.368    E -3.443    V -3.950\n",
       "8         145  TYR    Y -0.818    E -2.617    P -2.905    K -2.961    G -3.070\n",
       "9         206  ALA    A -0.634    I -2.974    L -3.027    F -3.175    V -3.180"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import esm\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. 加载 avGFP WT 序列 ===\n",
    "wt_sequence = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "\n",
    "# === 2. 加载 Top20 位点表格 ===\n",
    "df = pd.read_csv(\"Top20_Mutation_Sites.csv\")  # 确保文件在当前路径\n",
    "df = df[[\"Residue_ID\", \"Residue_Name\"]].copy()\n",
    "df.columns = [\"Residue_ID\", \"WT\"]\n",
    "df[\"Residue_ID\"] = df[\"Residue_ID\"].astype(int)\n",
    "\n",
    "# === 3. 加载 ESM 模型 ===\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "model.eval()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# === 4. 准备序列输入 ===\n",
    "data = [(\"GFP\", wt_sequence)]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "# === 5. 推理：获取所有位置的 logits ===\n",
    "with torch.no_grad():\n",
    "    logits = model(batch_tokens)[\"logits\"][0]  # [seq_len, vocab_size]\n",
    "\n",
    "# === 6. 分析每个位点 ===\n",
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    res_id = row[\"Residue_ID\"]\n",
    "    wt_res = row[\"WT\"]\n",
    "    \n",
    "    pos = res_id - 1  # Python 0-based\n",
    "    log_probs = torch.nn.functional.log_softmax(logits[pos + 1], dim=-1)  # +1 for BOS token\n",
    "\n",
    "    topk = torch.topk(log_probs, k=5)\n",
    "    aa_indices = topk.indices.tolist()\n",
    "    scores = topk.values.tolist()\n",
    "\n",
    "    aa_letters = [alphabet.get_tok(i) for i in aa_indices]\n",
    "    results.append({\n",
    "        \"Residue_ID\": res_id,\n",
    "        \"WT\": wt_res,\n",
    "        \"Top1\": aa_letters[0],\n",
    "        \"LogP1\": round(scores[0], 3),\n",
    "        \"Top2\": aa_letters[1],\n",
    "        \"LogP2\": round(scores[1], 3),\n",
    "        \"Top3\": aa_letters[2],\n",
    "        \"LogP3\": round(scores[2], 3),\n",
    "        \"Top4\": aa_letters[3],\n",
    "        \"LogP4\": round(scores[3], 3),\n",
    "        \"Top5\": aa_letters[4],\n",
    "        \"LogP5\": round(scores[4], 3),\n",
    "    })\n",
    "\n",
    "# === 7. 保存结果 ===\n",
    "out_df = pd.DataFrame(results)\n",
    "out_df.to_excel(\"PLM_突变候选推荐表.xlsx\", index=False)\n",
    "print(\" 已完成预测，保存为 PLM_突变候选推荐表.xlsx\")\n",
    "out_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bac088-7131-439f-a69a-f1cada3f8f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM候选残基已保存\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Residue_ID</th>\n",
       "      <th>WT</th>\n",
       "      <th>候选残基（一字母）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>ASN</td>\n",
       "      <td>D, E, K, N, T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153</td>\n",
       "      <td>MET</td>\n",
       "      <td>D, E, M, S, T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214</td>\n",
       "      <td>LYS</td>\n",
       "      <td>G, I, K, L, P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>212</td>\n",
       "      <td>ASN</td>\n",
       "      <td>D, E, K, N, S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68</td>\n",
       "      <td>VAL</td>\n",
       "      <td>D, E, G, K, V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>PHE</td>\n",
       "      <td>D, G, L, S, T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>222</td>\n",
       "      <td>GLU</td>\n",
       "      <td>D, E, G, K, S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>SER</td>\n",
       "      <td>E, K, S, T, V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>145</td>\n",
       "      <td>TYR</td>\n",
       "      <td>E, G, K, P, Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>206</td>\n",
       "      <td>ALA</td>\n",
       "      <td>A, F, I, L, V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Residue_ID   WT      候选残基（一字母）\n",
       "0         149  ASN  D, E, K, N, T\n",
       "1         153  MET  D, E, M, S, T\n",
       "2         214  LYS  G, I, K, L, P\n",
       "3         212  ASN  D, E, K, N, S\n",
       "4          68  VAL  D, E, G, K, V\n",
       "5          64  PHE  D, G, L, S, T\n",
       "6         222  GLU  D, E, G, K, S\n",
       "7          30  SER  E, K, S, T, V\n",
       "8         145  TYR  E, G, K, P, Y\n",
       "9         206  ALA  A, F, I, L, V"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 环境重置后需重新导入库\n",
    "import pandas as pd\n",
    "\n",
    "# 重新加载上传的 PLM 突变残基推荐表\n",
    "plm_file = \"PLM_突变候选推荐表.xlsx\"\n",
    "plm_df = pd.read_excel(plm_file,engine=\"openpyxl\")\n",
    "\n",
    "# 为每个位点构建推荐突变残基集合（包含WT + Top5 推荐）\n",
    "plm_mutation_candidates = {}\n",
    "\n",
    "for _, row in plm_df.iterrows():\n",
    "    pos = int(row[\"Residue_ID\"])\n",
    "    candidates = set()\n",
    "\n",
    "    for i in range(1, 6):  # Top1 to Top5\n",
    "        aa = row[f\"Top{i}\"]\n",
    "        if isinstance(aa, str):\n",
    "            candidates.add(aa)\n",
    "\n",
    "    # 保存为列表并排序\n",
    "    plm_mutation_candidates[pos] = sorted(candidates)\n",
    "\n",
    "# 以表格形式展示\n",
    "candidate_df=pd.DataFrame([\n",
    "        {\"Residue_ID\": k, \"WT\": plm_df.loc[plm_df[\"Residue_ID\"] == k, \"WT\"].values[0],\n",
    "         \"候选残基（一字母）\": \", \".join(v)}\n",
    "        for k, v in plm_mutation_candidates.items()\n",
    "    ])\n",
    "candidate_df.to_excel(\"PLM候选突变残基（按位点）.xlsx\",index=False)\n",
    "print(\"PLM候选残基已保存\")\n",
    "candidate_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949391c2ea0838e",
   "metadata": {},
   "source": [
    "## 阶段 3.2.3 多源突变残基候选集整合（并集策略）\n",
    "\n",
    "### 目标\n",
    "\n",
    "对每个突变位点，将以下两个来源的突变残基集合合并：\n",
    "\n",
    "- **PLM 模型推荐的残基**（上下文预测结果）\n",
    "- **BLOSUM62 相似残基**（进化保守性替代）\n",
    "\n",
    "以确保每个位点的突变空间兼顾：\n",
    "\n",
    "- 生物学合理性（不破坏结构/功能）\n",
    "- 多样性与模型探索潜力（语言模型给出新的高概率残基）\n",
    "\n",
    "------\n",
    "\n",
    "### 实现步骤解析\n",
    "\n",
    "#### 加载输入数据\n",
    "\n",
    "- `PLM候选突变残基（按位点）.xlsx`：记录每个位点语言模型打分 top-k 残基；\n",
    "- `候选突变残基推荐表.xlsx`：记录 BLOSUM62 打分 ≥ 1 的保守残基（由三字母编码表示）。\n",
    "\n",
    "#### 三字母转换为一字母表示\n",
    "\n",
    "- 将 BLOSUM 的候选残基字段从 `[\"ARG\", \"ILE\", \"GLN\"]` 转换为 `[\"R\", \"I\", \"Q\"]`；\n",
    "- 使用标准的三字母 → 一字母映射表 `aa_three_to_one`。\n",
    "\n",
    "#### 合并残基集合（按 Residue_ID 分组）\n",
    "\n",
    "- 对每一个 Residue_ID：\n",
    "  - 初始化为 PLM 候选集合；\n",
    "  - 如果 BLOSUM 中也包含该位点，合并两个集合；\n",
    "  - 若 PLM 未覆盖但 BLOSUM 有记录，也会单独加入该位点。\n",
    "\n",
    "#### 输出格式标准化\n",
    "\n",
    "- 每个位点对应的残基候选列表转换为逗号分隔的一字母串；\n",
    "- 构建为 DataFrame 并输出为 `并集PLM筛选后点位.xlsx`，供后续 VAE 或筛选阶段使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6442a-54ba-491f-bcdd-bc0b9fdef65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Residue_ID</th>\n",
       "      <th>候选残基（一字母）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>D, E, H, K, N, S, T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153</td>\n",
       "      <td>D, E, I, L, M, S, T, V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214</td>\n",
       "      <td>E, G, I, K, L, P, Q, R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>212</td>\n",
       "      <td>D, E, H, K, N, S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68</td>\n",
       "      <td>D, E, G, I, K, L, M, V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>D, F, G, L, S, T, W, Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>222</td>\n",
       "      <td>D, E, G, K, Q, S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>A, E, K, N, S, T, V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>145</td>\n",
       "      <td>E, F, G, H, K, P, W, Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>206</td>\n",
       "      <td>A, F, I, L, S, V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Residue_ID               候选残基（一字母）\n",
       "0         149     D, E, H, K, N, S, T\n",
       "1         153  D, E, I, L, M, S, T, V\n",
       "2         214  E, G, I, K, L, P, Q, R\n",
       "3         212        D, E, H, K, N, S\n",
       "4          68  D, E, G, I, K, L, M, V\n",
       "5          64  D, F, G, L, S, T, W, Y\n",
       "6         222        D, E, G, K, Q, S\n",
       "7          30     A, E, K, N, S, T, V\n",
       "8         145  E, F, G, H, K, P, W, Y\n",
       "9         206        A, F, I, L, S, V"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plm_candidates_path = \"PLM候选突变残基（按位点）.xlsx\"\n",
    "blosum_candidates_path = \"候选突变残基推荐表.xlsx\"\n",
    "\n",
    "plm_df = pd.read_excel(plm_candidates_path,engine='openpyxl')\n",
    "blosum_df = pd.read_excel(blosum_candidates_path,engine='openpyxl')\n",
    "# 三字母转一字母映射\n",
    "aa_three_to_one = {\n",
    "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
    "    'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "    'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
    "    'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "}\n",
    "\n",
    "# 转换 BLOSUM 残基为一字母\n",
    "blosum_df[\"候选残基（一字母）\"] = blosum_df[\"候选残基（三字母）\"].apply(\n",
    "    lambda x: [aa_three_to_one.get(res.strip(), '') for res in eval(x)]\n",
    ")\n",
    "\n",
    "# 创建 Residue_ID 映射：合并 PLM 和 BLOSUM 的残基集合\n",
    "combined_candidates = {}\n",
    "\n",
    "for _, row in plm_df.iterrows():\n",
    "    rid = int(row[\"Residue_ID\"])\n",
    "    plm_set = set(row[\"候选残基（一字母）\"].replace(\" \", \"\").split(\",\"))\n",
    "    combined_candidates[rid] = plm_set\n",
    "\n",
    "for _, row in blosum_df.iterrows():\n",
    "    rid = int(row[\"Residue_ID\"])\n",
    "    blosum_set = set(row[\"候选残基（一字母）\"])\n",
    "    if rid in combined_candidates:\n",
    "        combined_candidates[rid].update(blosum_set)\n",
    "    else:\n",
    "        combined_candidates[rid] = blosum_set\n",
    "\n",
    "# 构建表格展示\n",
    "combined_df = pd.DataFrame([\n",
    "    {\"Residue_ID\": rid, \"候选残基（一字母）\": \", \".join(sorted(residues))}\n",
    "    for rid, residues in combined_candidates.items()\n",
    "])\n",
    "\n",
    "combined_df.to_excel(\"并集PLM筛选后点位.xlsx\", index=False, engine='openpyxl')\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4b89c-b443-49d0-bfb9-73a6634b30f6",
   "metadata": {},
   "source": [
    "## 阶段 3.3：基于变分自编码器（VAE）生成突变序列库\n",
    "\n",
    "### 阶段目标\n",
    "\n",
    "通过训练 VAE 模型，在高亮度蛋白序列数据的分布中建立潜在表示，进而从潜在空间采样并解码生成新的序列，这些序列满足以下设计要求：\n",
    "\n",
    "- 与高亮度蛋白结构接近；\n",
    "- 控制突变数量；\n",
    "- 限制突变位点与突变残基集合；\n",
    "- 生成序列多样性高、结构合理、有亮度潜力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238d7fd-43be-4e27-a88c-aa5903d61cfa",
   "metadata": {},
   "source": [
    "### 提取字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "711e0efd-5643-4af1-9715-b470bfc3edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "wt_sequence = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "\n",
    "# 1. 加载 avGFP WT 序列\n",
    "wt_seq = wt_sequence\n",
    "\n",
    "# 2. 读取并集突变位点文件\n",
    "mut_df = pd.read_excel(\"并集PLM筛选后点位.xlsx\",engine='openpyxl')\n",
    "residue_ids = mut_df[\"Residue_ID\"].tolist()\n",
    "residue_aa_options = {\n",
    "    rid: r.split(\", \") for rid, r in zip(mut_df[\"Residue_ID\"], mut_df[\"候选残基（一字母）\"])\n",
    "}\n",
    "\n",
    "# 3. 编码映射\n",
    "aa_to_id = {'-': 0, 'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7,\n",
    "            'G': 8, 'H': 9, 'I': 10, 'L': 11, 'K': 12, 'M': 13, 'F': 14,\n",
    "            'P': 15, 'S': 16, 'T': 17, 'W': 18, 'Y': 19, 'V': 20}\n",
    "id_to_aa = {v: k for k, v in aa_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce72402c-8289-44fc-a383-ce2035d3caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "[149, 153, 214, 212, 68, 64, 222, 30, 145, 206, 203, 163, 130, 128, 136, 131, 135, 121, 140, 126]\n"
     ]
    }
   ],
   "source": [
    "print(len(wt_seq))  # → 应为 238\n",
    "print(residue_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6357a7-8f7a-45b7-9dfb-88b1f562fff1",
   "metadata": {},
   "source": [
    "## 阶段 3.3.1 构造并训练 VAE（变分自编码器）\n",
    "\n",
    "### 本阶段目标\n",
    "\n",
    "基于你在前一阶段（3.1~3.2）构建的突变位点集合与候选残基集合，本阶段训练一个变分自编码器模型，使其能够从已有突变样本中学习分布规律，并在潜在空间中生成符合突变策略、具备亮度潜力的序列候选"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e969b3baf0d447",
   "metadata": {},
   "source": [
    "### 数据生成与编码：自由突变样本构造\n",
    "\n",
    "通过 `generate_mutation_encoding_freedom()` 函数，你构建了一个自定义突变样本采样器，其关键特性包括：\n",
    "\n",
    "- **突变位点自由组合**：从已筛选的 `allowed_positions` 中随机选取 2 ~ `max_mut` 个位点；\n",
    "- **突变残基控制**：每个位点的残基仅从允许集合 `residue_aa_options` 中采样；\n",
    "- **去重机制**：使用序列标识 `key_tuple` 保证每一条突变组合唯一；\n",
    "- **编码为整数 token**：WT/突变后的氨基酸统一映射为索引 ID，用于模型训练；\n",
    "- **支持记录突变信息**：可输出标准突变描述（如 `F64Y:T203S`），用于追踪与筛选。\n",
    "\n",
    "该步骤生成的是训练 VAE 所需的样本集，形状为 `[N, L]` 的整数矩阵，其中每一行为一个突变片段序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac285406-80e3-4d71-87dc-88d489a5a910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7, 13, 12,  3, 20, 11,  7, 16,  9,  1, 17, 20, 14, 10, 10, 12,  3,  3,\n",
       "          12, 12],\n",
       "         [ 3, 13,  6,  4, 20, 11,  7, 16, 19,  1, 17, 20, 14, 10, 10, 12,  3,  1,\n",
       "          12, 12],\n",
       "         [ 3, 13, 12,  3, 20, 11,  6, 16, 19,  1, 17, 20, 14, 10, 10, 12,  3,  3,\n",
       "          12, 12],\n",
       "         [ 3, 13, 12,  3, 20, 11, 16, 16, 19,  1, 17, 20, 14, 10, 10, 12,  3,  3,\n",
       "          12, 12],\n",
       "         [17, 13, 12,  3, 20, 11,  7,  7, 18,  1, 17, 20, 10, 10, 10, 12,  3,  3,\n",
       "          12, 12]]),\n",
       " ['N149E:Y145H',\n",
       "  'K214Q:N212D:N121A',\n",
       "  'E222Q:S30S',\n",
       "  'E222S:N121N',\n",
       "  'N149T:S30E:Y145W:F130I:I128I:I136I'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重构后的自由突变样本生成函数（增强版）\n",
    "def generate_mutation_encoding_freedom(\n",
    "    n_samples=2000,\n",
    "    max_mut=6,\n",
    "    allowed_positions=None,\n",
    "    seed=None,\n",
    "    return_mutinfo=True\n",
    "):\n",
    "    \"\"\"\n",
    "    自由突变采样器（支持热点控制、去重、记录、随机种子）。\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    if allowed_positions is None:\n",
    "        allowed_positions = residue_ids.copy()\n",
    "\n",
    "    seen = set()\n",
    "    samples = []\n",
    "    mutation_records = []\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        mut_pos = tuple(sorted(random.sample(allowed_positions, k=random.randint(2, min(max_mut, len(allowed_positions))))))\n",
    "        aa_seq = []\n",
    "        record = []\n",
    "        key = []\n",
    "\n",
    "        for rid in allowed_positions:\n",
    "            if rid in mut_pos:\n",
    "                mut_aa = random.choice(residue_aa_options[rid])\n",
    "                aa_seq.append(mut_aa)\n",
    "                record.append(f\"{wt_seq[rid - 1]}{rid}{mut_aa}\")\n",
    "                key.append((rid, mut_aa))\n",
    "            else:\n",
    "                aa_seq.append(wt_seq[rid - 1])\n",
    "                key.append((rid, wt_seq[rid - 1]))\n",
    "\n",
    "        # 去重判断（基于编码键）\n",
    "        key_tuple = tuple(key)\n",
    "        if key_tuple in seen:\n",
    "            continue\n",
    "        seen.add(key_tuple)\n",
    "\n",
    "        # 编码为 token ID\n",
    "        token_ids = [aa_to_id.get(aa, 0) for aa in aa_seq]\n",
    "        samples.append(token_ids)\n",
    "        mutation_records.append(\":\".join(record))\n",
    "\n",
    "    tensor = torch.tensor(samples)\n",
    "    return (tensor, mutation_records) if return_mutinfo else tensor\n",
    "\n",
    "# 测试生成\n",
    "freedom_tensor_v2, freedom_records_v2 = generate_mutation_encoding_freedom(\n",
    "    n_samples=5, seed=42\n",
    ")\n",
    "\n",
    "freedom_tensor_v2, freedom_records_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9944a1ca58a02",
   "metadata": {},
   "source": [
    "### 模型结构设计：MutationVAE\n",
    "\n",
    "#### 网络结构说明：\n",
    "\n",
    "- **嵌入层（Embedding）**：将每个残基 ID 映射为 32 维向量，支持空位填充（padding_idx=0）；\n",
    "- **编码器（Encoder）**：由全连接层 + Dropout + LayerNorm 构成，输出隐向量分布的均值 `mu` 和 log 方差 `logvar`；\n",
    "- **重参数化层**：使用 `reparameterize(mu, logvar)` 实现从潜在空间 `z ~ N(mu, σ²)` 中采样；\n",
    "- **解码器（Decoder）**：反向将潜在变量映射为 `[seq_len, vocab_size]` 的每位点 logits 分布；\n",
    "- **输出层**：softmax 前一层，输出每个位点的氨基酸概率。\n",
    "\n",
    "该模型可以理解为在**突变限制空间中学习一个压缩-重建的映射函数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "273786e0-8e82-4c92-9403-f3a37a6756ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MutationVAE(\n",
       "   (embed): Embedding(21, 32, padding_idx=0)\n",
       "   (encoder): Sequential(\n",
       "     (0): Linear(in_features=640, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Dropout(p=0.2, inplace=False)\n",
       "     (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "     (4): ReLU()\n",
       "     (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (fc_mu): Linear(in_features=128, out_features=16, bias=True)\n",
       "   (fc_logvar): Linear(in_features=128, out_features=16, bias=True)\n",
       "   (decoder_fc): Sequential(\n",
       "     (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Dropout(p=0.2, inplace=False)\n",
       "     (3): Linear(in_features=128, out_features=640, bias=True)\n",
       "     (4): ReLU()\n",
       "   )\n",
       "   (output_layernorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "   (output): Linear(in_features=32, out_features=21, bias=True)\n",
       " ),\n",
       " 'Total trainable parameters: 287541')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MutationVAE(nn.Module):\n",
    "    def __init__(self, input_len=20, vocab_size=21, emb_dim=32, latent_dim=16):\n",
    "        super(MutationVAE, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_len * emb_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128)\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, input_len * emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_layernorm = nn.LayerNorm(emb_dim)\n",
    "        self.output = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.embed(x).view(x.size(0), -1)\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_fc(z).view(-1, self.input_len, self.emb_dim)\n",
    "        h = self.output_layernorm(h)\n",
    "        return self.output(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        logits = self.decode(z)\n",
    "        return logits, mu, logvar\n",
    "model = MutationVAE()\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model, f\"Total trainable parameters: {total_params}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e0539445c77a2",
   "metadata": {},
   "source": [
    "### 训练流程设计\n",
    "\n",
    "你采用了标准的 VAE 训练范式，包括重构损失和 KL 散度损失的联合优化：\n",
    "\n",
    "- **重构损失**（CrossEntropy）：衡量模型生成序列与原突变序列的一致性；\n",
    "- **KL 散度损失**：约束潜在空间的分布接近于标准正态分布，提升生成稳定性与多样性；\n",
    "- **KL 退火策略**：训练初期使用较小 KL 权重 `β`，逐步增长至 `max_beta=0.01`，避免模式塌陷（posterior collapse）；\n",
    "- **训练细节**：\n",
    "  - `60 epochs` 总训练轮次；\n",
    "  - `gradient clipping` 防止爆炸；\n",
    "  - 使用 Adam 优化器，`lr=1e-3`；\n",
    "  - 验证集（10%）实时监控 `val_loss`，确保泛化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa8b48-0a7c-4295-90f6-eeb45e53a461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss=3.1484, KL=5.0701, Beta=0.0000, Val Loss=2.7473\n",
      "[Epoch 2] Train Loss=2.8077, KL=9.7811, Beta=0.0002, Val Loss=2.4714\n",
      "[Epoch 3] Train Loss=2.5562, KL=11.4941, Beta=0.0003, Val Loss=2.2402\n",
      "[Epoch 4] Train Loss=2.3425, KL=13.1990, Beta=0.0005, Val Loss=2.0646\n",
      "[Epoch 5] Train Loss=2.1639, KL=14.8054, Beta=0.0007, Val Loss=1.9207\n",
      "[Epoch 6] Train Loss=2.0161, KL=16.3150, Beta=0.0008, Val Loss=1.8018\n",
      "[Epoch 7] Train Loss=1.8917, KL=17.6927, Beta=0.0010, Val Loss=1.7047\n",
      "[Epoch 8] Train Loss=1.7860, KL=18.9327, Beta=0.0012, Val Loss=1.6194\n",
      "[Epoch 9] Train Loss=1.6972, KL=20.0140, Beta=0.0013, Val Loss=1.5481\n",
      "[Epoch 10] Train Loss=1.6182, KL=20.9194, Beta=0.0015, Val Loss=1.4867\n",
      "[Epoch 11] Train Loss=1.5537, KL=21.6190, Beta=0.0017, Val Loss=1.4343\n",
      "[Epoch 12] Train Loss=1.4962, KL=22.0949, Beta=0.0018, Val Loss=1.3897\n",
      "[Epoch 13] Train Loss=1.4470, KL=22.3288, Beta=0.0020, Val Loss=1.3497\n",
      "[Epoch 14] Train Loss=1.4034, KL=22.3015, Beta=0.0022, Val Loss=1.3153\n",
      "[Epoch 15] Train Loss=1.3659, KL=22.0282, Beta=0.0023, Val Loss=1.2850\n",
      "[Epoch 16] Train Loss=1.3329, KL=21.5284, Beta=0.0025, Val Loss=1.2588\n",
      "[Epoch 17] Train Loss=1.3038, KL=20.8090, Beta=0.0027, Val Loss=1.2356\n",
      "[Epoch 18] Train Loss=1.2786, KL=19.9134, Beta=0.0028, Val Loss=1.2156\n",
      "[Epoch 19] Train Loss=1.2562, KL=18.8658, Beta=0.0030, Val Loss=1.1981\n",
      "[Epoch 20] Train Loss=1.2358, KL=17.6818, Beta=0.0032, Val Loss=1.1826\n",
      "[Epoch 21] Train Loss=1.2185, KL=16.4276, Beta=0.0033, Val Loss=1.1684\n",
      "[Epoch 22] Train Loss=1.2027, KL=15.1405, Beta=0.0035, Val Loss=1.1559\n",
      "[Epoch 23] Train Loss=1.1890, KL=13.8873, Beta=0.0037, Val Loss=1.1453\n",
      "[Epoch 24] Train Loss=1.1757, KL=12.7132, Beta=0.0038, Val Loss=1.1348\n",
      "[Epoch 25] Train Loss=1.1640, KL=11.6686, Beta=0.0040, Val Loss=1.1241\n",
      "[Epoch 26] Train Loss=1.1538, KL=10.7530, Beta=0.0042, Val Loss=1.1152\n",
      "[Epoch 27] Train Loss=1.1429, KL=9.9944, Beta=0.0043, Val Loss=1.1076\n",
      "[Epoch 28] Train Loss=1.1334, KL=9.3336, Beta=0.0045, Val Loss=1.0998\n",
      "[Epoch 29] Train Loss=1.1238, KL=8.7939, Beta=0.0047, Val Loss=1.0914\n",
      "[Epoch 30] Train Loss=1.1150, KL=8.3257, Beta=0.0048, Val Loss=1.0832\n",
      "[Epoch 31] Train Loss=1.1060, KL=7.9019, Beta=0.0050, Val Loss=1.0771\n",
      "[Epoch 32] Train Loss=1.0990, KL=7.4881, Beta=0.0052, Val Loss=1.0718\n",
      "[Epoch 33] Train Loss=1.0921, KL=7.0803, Beta=0.0053, Val Loss=1.0655\n",
      "[Epoch 34] Train Loss=1.0850, KL=6.6774, Beta=0.0055, Val Loss=1.0591\n",
      "[Epoch 35] Train Loss=1.0791, KL=6.2763, Beta=0.0057, Val Loss=1.0544\n",
      "[Epoch 36] Train Loss=1.0729, KL=5.8752, Beta=0.0058, Val Loss=1.0498\n",
      "[Epoch 37] Train Loss=1.0674, KL=5.4779, Beta=0.0060, Val Loss=1.0443\n",
      "[Epoch 38] Train Loss=1.0616, KL=5.0944, Beta=0.0062, Val Loss=1.0394\n",
      "[Epoch 39] Train Loss=1.0566, KL=4.7203, Beta=0.0063, Val Loss=1.0355\n",
      "[Epoch 40] Train Loss=1.0518, KL=4.3543, Beta=0.0065, Val Loss=1.0312\n",
      "[Epoch 41] Train Loss=1.0464, KL=4.0242, Beta=0.0067, Val Loss=1.0270\n",
      "[Epoch 42] Train Loss=1.0425, KL=3.7150, Beta=0.0068, Val Loss=1.0227\n",
      "[Epoch 43] Train Loss=1.0370, KL=3.4242, Beta=0.0070, Val Loss=1.0189\n",
      "[Epoch 44] Train Loss=1.0322, KL=3.1701, Beta=0.0072, Val Loss=1.0143\n",
      "[Epoch 45] Train Loss=1.0283, KL=2.9240, Beta=0.0073, Val Loss=1.0100\n",
      "[Epoch 46] Train Loss=1.0239, KL=2.7012, Beta=0.0075, Val Loss=1.0060\n",
      "[Epoch 47] Train Loss=1.0197, KL=2.5074, Beta=0.0077, Val Loss=1.0022\n",
      "[Epoch 48] Train Loss=1.0159, KL=2.3258, Beta=0.0078, Val Loss=1.0002\n",
      "[Epoch 49] Train Loss=1.0122, KL=2.1568, Beta=0.0080, Val Loss=0.9947\n",
      "[Epoch 50] Train Loss=1.0078, KL=2.0143, Beta=0.0082, Val Loss=0.9922\n",
      "[Epoch 51] Train Loss=1.0041, KL=1.8738, Beta=0.0083, Val Loss=0.9886\n",
      "[Epoch 52] Train Loss=1.0005, KL=1.7475, Beta=0.0085, Val Loss=0.9861\n",
      "[Epoch 53] Train Loss=0.9972, KL=1.6309, Beta=0.0087, Val Loss=0.9842\n",
      "[Epoch 54] Train Loss=0.9944, KL=1.5189, Beta=0.0088, Val Loss=0.9801\n",
      "[Epoch 55] Train Loss=0.9905, KL=1.4293, Beta=0.0090, Val Loss=0.9758\n",
      "[Epoch 56] Train Loss=0.9873, KL=1.3383, Beta=0.0092, Val Loss=0.9738\n",
      "[Epoch 57] Train Loss=0.9838, KL=1.2497, Beta=0.0093, Val Loss=0.9704\n",
      "[Epoch 58] Train Loss=0.9811, KL=1.1653, Beta=0.0095, Val Loss=0.9657\n",
      "[Epoch 59] Train Loss=0.9778, KL=1.0790, Beta=0.0097, Val Loss=0.9648\n",
      "[Epoch 60] Train Loss=0.9753, KL=1.0033, Beta=0.0098, Val Loss=0.9627\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "# KL系数退火函数\n",
    "def kl_annealing(epoch, total_epochs, max_beta=0.01):\n",
    "    return max_beta * (epoch / total_epochs)\n",
    "\n",
    "# 训练数据（保留10%验证）\n",
    "data_list = []\n",
    "for s in [1, 2, 3, 4, 5]:\n",
    "    data_batch, _ = generate_mutation_encoding_freedom(n_samples=2000, seed=s)\n",
    "    data_list.append(data_batch)\n",
    "data = torch.cat(data_list, dim=0)\n",
    "val_size = int(0.1 * len(data))\n",
    "train_data, val_data = data[val_size:], data[:val_size]\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 训练 60 epoch，带验证\n",
    "for epoch in range(60):\n",
    "    beta = kl_annealing(epoch, total_epochs=60)\n",
    "\n",
    "    model.train()\n",
    "    logits, mu, logvar = model(train_data)\n",
    "    ce = F.cross_entropy(logits.view(-1, model.vocab_size), train_data.view(-1), ignore_index=0)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / train_data.size(0)\n",
    "    total = ce + beta * kl\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total.backward()\n",
    "    utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # 验证集评估\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits, _, _ = model(val_data)\n",
    "        val_loss = F.cross_entropy(val_logits.view(-1, model.vocab_size), val_data.view(-1), ignore_index=0)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss={ce.item():.4f}, KL={kl.item():.4f}, Beta={beta:.4f}, Val Loss={val_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a086bc29ff931",
   "metadata": {},
   "source": [
    "## 阶段 3.3.2：VAE 生成函数构造与多策略突变控制\n",
    "\n",
    "### 目标\n",
    "\n",
    "从训练好的 VAE 模型中采样生成符合以下要求的大规模突变序列库：\n",
    "\n",
    "- 控制突变数量（如最多 6 个）；\n",
    "- 控制突变位置（如 PLM + 手工 Top 位点）；\n",
    "- 控制残基选择（如 3.2.3 中的残基并集）；\n",
    "- 多样性强、去重处理、合规性强；\n",
    "- 输出格式规范，便于后续筛选与结构评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9297b57650d82",
   "metadata": {},
   "source": [
    "### `generate_sequences_from_vae`：统一序列生成接口\n",
    "\n",
    "该函数完成整体 VAE 序列生成任务，支持：\n",
    "\n",
    "- **多种解码策略**（argmax / top-k / Gumbel softmax）；\n",
    "- **两类突变模式**：\n",
    "  - `aggressive`：最大数量突变，自由拓展；\n",
    "  - `conservative`：较少突变数量 + `mut_rate` 控制；\n",
    "- **批量采样生成**：每轮采样 `batch_size` 个潜在变量 `z`；\n",
    "- **突变位点限制**：支持 PLM + 文献手动组合筛选；\n",
    "- **结果去重**、统计突变数、支持保存为 CSV 文件。\n",
    "### 多种解码策略支持\n",
    "\n",
    "| 策略     | 方法                     | 特点                   |\n",
    "| -------- | ------------------------ | ---------------------- |\n",
    "| `argmax` | 每个位点取最大概率氨基酸 | 保守，低多样性         |\n",
    "| `topk`   | 从 top-k 残基中随机采样  | 平衡性能与多样性       |\n",
    "| `gumbel` | 加入随机噪声后 argmax    | 高多样性，模拟抽样扰动 |\n",
    "\n",
    "\n",
    "\n",
    "这些策略均作用于 VAE 的 decoder 输出的 `logits` 上。\n",
    "###  `mut_to_seq_aggressive` / `mut_to_seq_conservative`：两类突变控制策略\n",
    "\n",
    "#### Aggressive（激进策略）\n",
    "\n",
    "- 尽可能多地引入突变（最多 `max_mut`）；\n",
    "- 每个位点都被解码器覆盖，按 residue_id 顺序处理；\n",
    "- 只要突变残基不同于 WT 且不重复，就允许；\n",
    "- 适用于探索新功能区、高多样性筛选任务。\n",
    "\n",
    "#### Conservative（保守策略）\n",
    "\n",
    "- 使用突变概率 `mut_rate` 决定是否突变；\n",
    "- 控制总突变数在 2 ~ 3 之间；\n",
    "- 对突变位点和突变内容都更加谨慎；\n",
    "- 适用于保持亮度、结构稳定性较高的方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c78f719-c6ae-4e8b-8e3a-fa05cae9e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#VAE生成函数\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "def generate_sequences_from_vae(\n",
    "    model,\n",
    "    wt_seq,\n",
    "    id_to_aa,\n",
    "    residue_ids,\n",
    "    batch_size=1000,\n",
    "    total=100000,\n",
    "    decode_strategy=\"topk\",\n",
    "    k=5,\n",
    "    max_mut=6,\n",
    "    mut_rate=0.3,\n",
    "    mutation_mode=\"aggressive\",  # [\"aggressive\", \"conservative\"]\n",
    "    allowed_positions=None,      # plm选择\n",
    "    device=\"cpu\",\n",
    "    save_path=None,\n",
    "    seed=None\n",
    "):\n",
    "    \"\"\"\n",
    "    统一突变序列生成接口，支持激进/保守突变策略。\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    all_sequences = []\n",
    "    all_tags = []\n",
    "    seen = set()\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(total // batch_size), desc=\"Generating\"):\n",
    "            while len(all_sequences) < total:\n",
    "                z = torch.randn(batch_size, model.latent_dim).to(device)\n",
    "                logits = model.decode(z).to(\"cpu\")\n",
    "\n",
    "            # 解码\n",
    "            if decode_strategy == \"argmax\":\n",
    "                sampled_ids = torch.argmax(logits, dim=-1)\n",
    "            elif decode_strategy == \"topk\":\n",
    "                sampled_ids = top_k_sampling(logits, k=k)\n",
    "            elif decode_strategy == \"gumbel\":\n",
    "                sampled_ids = gumbel_softmax_sample(logits)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported decode strategy\")\n",
    "\n",
    "            # 选择突变策略\n",
    "            if mutation_mode == \"aggressive\":\n",
    "                pairs = mut_to_seq_aggressive(\n",
    "                        sampled_ids, residue_ids, wt_seq, id_to_aa, max_mut=max_mut, allowed_positions=allowed_positions\n",
    "                        )\n",
    "            elif mutation_mode == \"conservative\":\n",
    "                pairs = mut_to_seq_conservative(\n",
    "                        sampled_ids, residue_ids, wt_seq, id_to_aa, max_mut=max_mut, mut_rate=mut_rate, allowed_positions=allowed_positions\n",
    "                        )\n",
    "            else:\n",
    "                raise ValueError(\"mutation_mode must be 'aggressive' or 'conservative'\")\n",
    "\n",
    "            for seq, tag in pairs:\n",
    "                if seq not in seen:\n",
    "                    seen.add(seq)\n",
    "                    all_sequences.append(seq)\n",
    "                    all_tags.append(tag)\n",
    "                    pbar.update(1)\n",
    "                if len(all_sequences) >= total:\n",
    "                    break\n",
    "        pbar.close()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"mutated_sequence\": all_sequences,\n",
    "        \"mutation_tag\": all_tags,\n",
    "        \"mutation_count\": [tag.count(\":\") + 1 if tag else 0 for tag in all_tags]\n",
    "    })\n",
    "\n",
    "    if save_path:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def top_k_sampling(logits, k=5):\n",
    "    \"\"\"\n",
    "    对每个位点在 top-k 中进行随机采样（多批次）。\n",
    "    logits: Tensor [batch, seq_len, vocab]\n",
    "    返回: Tensor [batch, seq_len]\n",
    "    \"\"\"\n",
    "    sampled_ids = []\n",
    "    for row in logits:  # row: [seq_len, vocab]\n",
    "        sampled_row = []\n",
    "        for token_logits in row:\n",
    "            topk_vals, topk_idx = torch.topk(token_logits, k)\n",
    "            probs = torch.softmax(topk_vals, dim=0)\n",
    "            idx = torch.multinomial(probs, 1).item()\n",
    "            sampled_row.append(topk_idx[idx].item())\n",
    "        sampled_ids.append(sampled_row)\n",
    "    return torch.tensor(sampled_ids)\n",
    "def gumbel_softmax_sample(logits, tau=1.0):\n",
    "    \"\"\"\n",
    "    Gumbel softmax 采样：从 logits 中加噪声并argmax。\n",
    "    logits: [batch, seq_len, vocab]\n",
    "    返回: Tensor [batch, seq_len]\n",
    "    \"\"\"\n",
    "    noise = -torch.log(-torch.log(torch.rand_like(logits)))\n",
    "    y = (logits + noise) / tau\n",
    "    return torch.argmax(y, dim=-1)\n",
    "def mut_to_seq_aggressive(sampled_ids, residue_ids, wt_seq, id_to_aa, max_mut=6, allowed_positions=None):\n",
    "    results = []\n",
    "\n",
    "    # 构造突变位点列表\n",
    "    if allowed_positions:\n",
    "        target_positions = sorted(set(residue_ids) | set(allowed_positions))  # 并集\n",
    "    else:\n",
    "        target_positions = residue_ids\n",
    "\n",
    "    for row in sampled_ids:\n",
    "        seq = list(wt_seq)\n",
    "        tags = []\n",
    "        mut_count = 0\n",
    "\n",
    "        for aa_id, pos in zip(row, residue_ids):  # 注意这里的 row 是对 residue_ids 解码的结果\n",
    "            if pos not in target_positions:\n",
    "                continue  # 非目标位点跳过（更严谨）\n",
    "            \n",
    "            pred_aa = id_to_aa.get(int(aa_id), '-')\n",
    "            wt_aa = wt_seq[pos - 1]\n",
    "\n",
    "            if pred_aa != wt_aa and pred_aa != '-' and mut_count < max_mut:\n",
    "                seq[pos - 1] = pred_aa\n",
    "                tags.append(f\"{wt_aa}{pos}{pred_aa}\")\n",
    "                mut_count += 1\n",
    "            else:\n",
    "                seq[pos - 1] = wt_aa\n",
    "\n",
    "        results.append((\"\".join(seq), \":\".join(tags)))\n",
    "    return results\n",
    "def mut_to_seq_conservative(sampled_ids, residue_ids, wt_seq, id_to_aa, max_mut=2, mut_rate=0.3, allowed_positions=None):\n",
    "    results = []\n",
    "\n",
    "    # === 构造突变允许位置的并集 ===\n",
    "    if allowed_positions:\n",
    "        allowed_positions_set = set(allowed_positions).union(set(residue_ids))\n",
    "    else:\n",
    "        allowed_positions_set = set(residue_ids)\n",
    "\n",
    "    for row in sampled_ids:\n",
    "        seq = list(wt_seq)\n",
    "        tags = []\n",
    "        mut_count = 0\n",
    "        for aa_id, pos in zip(row, residue_ids):  # residue_ids 控制采样位置，但突变位置从 allowed ∪ residue_ids 中决定\n",
    "            pred_aa = id_to_aa.get(int(aa_id), '-')\n",
    "            wt_aa = wt_seq[pos - 1]\n",
    "            if pos not in allowed_positions_set:\n",
    "                seq[pos - 1] = wt_aa  # 不允许突变的保留 WT\n",
    "                continue\n",
    "            do_mutate = random.random() < mut_rate\n",
    "            if do_mutate and pred_aa != wt_aa and pred_aa != '-' and mut_count < max_mut:\n",
    "                seq[pos - 1] = pred_aa\n",
    "                tags.append(f\"{wt_aa}{pos}{pred_aa}\")\n",
    "                mut_count += 1\n",
    "            else:\n",
    "                seq[pos - 1] = wt_aa\n",
    "        results.append((\"\".join(seq), \":\".join(tags)))\n",
    "    return results\n",
    "\n",
    "def get_plm_guided_allowed_positions(sequence, model, alphabet, top_n=20):\n",
    "    \"\"\"\n",
    "    使用 Tranception 模型对给定序列进行评分，返回 Top-N 残基位点（1-based）\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from tranception.inference_utils import score_sequence\n",
    "\n",
    "    # 分别屏蔽一个残基，获取每个残基的重要性（掩码法）\n",
    "    log_probs_per_residue = []\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        mutated_seq = sequence[:i] + 'X' + sequence[i+1:]  # 用 'X' 掩码该位点\n",
    "        try:\n",
    "            log_prob = score_sequence(seq=mutated_seq, model=model, alphabet=alphabet, return_log_likelihood=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at position {i+1}: {e}\")\n",
    "            log_prob = -1e9\n",
    "        log_probs_per_residue.append(log_prob)\n",
    "\n",
    "    # 获取 Top-N residue（概率降幅越大越重要，故排序用值越小越前）\n",
    "    top_indices = np.argsort(log_probs_per_residue)[:top_n]  # 取下降最严重的位点\n",
    "    allowed_positions = (top_indices + 1).tolist()  # 转为 1-based 位点号\n",
    "\n",
    "    return allowed_positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6483e7e9ee7da5",
   "metadata": {},
   "source": [
    "###  位点选择策略（allowed_positions）\n",
    "\n",
    "你实现了两类位点来源整合：\n",
    "\n",
    "- **PLM 引导选择**：通过 `get_plm_guided_allowed_positions()`，基于 Tranception 模型掩码实验获取 top-n 重要位点；\n",
    "- **手动筛选位点**：如 `Top20_Mutation_Sites.csv` 中来自结构分析/文献的位点；\n",
    "- **最终允许突变集合**：取二者并集 `combined_positions`，用于生成时限制突变发生的 residue_id。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "127cced2-63ea-47de-bff6-520294240947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tranception model from Tranception_Medium ...\n",
      "PLM-guided allowed positions: [1, 2, 5, 6, 7, 9, 89, 13, 37, 65, 80, 211, 58, 86, 94, 187, 34, 75, 56, 54]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tranception.inference_utils import score_sequence,load_model_and_alphabet\n",
    "\n",
    "# === 配置路径 ===\n",
    "MODEL_DIR = \"Tranception_Medium\"\n",
    "# === 加载模型 ===\n",
    "print(f\"Loading Tranception model from {MODEL_DIR} ...\")\n",
    "PLM_model, alphabet = load_model_and_alphabet(MODEL_DIR,\n",
    "                                          model_name=\"Tranception_Medium\",\n",
    "                                          use_gpu=torch.cuda.is_available())\n",
    "plm_positions =get_plm_guided_allowed_positions( sequence=wt_seq,\n",
    "    model=PLM_model,\n",
    "    alphabet=alphabet,\n",
    "    top_n=20)\n",
    "print(\"PLM-guided allowed positions:\",plm_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29a5ffcc-f530-4071-8071-c6dbe10b15d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manually chosen positions: [149, 153, 214, 212, 68, 64, 222, 30, 145, 206, 203, 163, 130, 128, 136, 131, 135, 121, 140, 126]\n",
      "combined positions: [1, 2, 5, 6, 7, 9, 13, 30, 34, 37, 54, 56, 58, 64, 65, 68, 75, 80, 86, 89, 94, 121, 126, 128, 130, 131, 135, 136, 140, 145, 149, 153, 163, 187, 203, 206, 211, 212, 214, 222]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "manual_df = pd.read_csv(\"Top20_Mutation_Sites.csv\")\n",
    "manual_positions = manual_df[\"Residue_ID\"].dropna().astype(int).tolist()\n",
    "manual_positions\n",
    "combined_positions = sorted(set(manual_positions) | set(plm_positions))\n",
    "print(\"manually chosen positions:\",manual_positions)\n",
    "print(\"combined positions:\",combined_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5d76e025c4761",
   "metadata": {},
   "source": [
    "## 阶段 3.3.3：多策略采样并生成 Mutation Library\n",
    "\n",
    "### 本阶段目标\n",
    "\n",
    "- 构建包含保守、激进、PLM 引导等不同突变策略的多元样本集合；\n",
    "- 控制突变数量（`max_mut`）、突变位点（`allowed_positions`）、突变残基（前序控制）；\n",
    "- 保证生成序列的多样性、合法性；\n",
    "- 输出统一格式 CSV 文件，便于后续批量评估与分析。\n",
    "\n",
    "------\n",
    "\n",
    "## 关键生成配置说明\n",
    "\n",
    "| 序列类型              | 数量   | 策略           | 突变位点控制                          | 最大突变数 | 来源标记             |\n",
    "| --------------------- | ------ | -------------- | ------------------------------------- | ---------- | -------------------- |\n",
    "| `df_aggressive`       | 50,000 | 激进（全自由） | 无限制                                | 6          | `\"aggressive\"`       |\n",
    "| `df_conservative`     | 30,000 | 保守（全自由） | 无限制                                | 3          | `\"conservative\"`     |\n",
    "| `df_plm_aggressive`   | 70,000 | 激进           | 仅限 `combined_positions`（PLM+人工） | 6          | `\"plm_aggressive\"`   |\n",
    "| `df_plm_conservative` | 50,000 | 保守           | 仅限 `combined_positions`             | 3          | `\"plm_conservative\"` |\n",
    "\n",
    "\n",
    "\n",
    "合计生成 **200,000 条突变序列**，每条包括：\n",
    "\n",
    "- `mutated_sequence`：完整突变后序列（与 WT 序列长度一致）；\n",
    "- `mutation_tag`：标准突变标签（如 F64Y:T203S）；\n",
    "- `mutation_count`：突变数统计；\n",
    "- `source`：突变策略来源标识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52f202b6-00fa-4081-b5c1-70d12df57fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggressive = generate_sequences_from_vae(\n",
    "    model=model,\n",
    "    wt_seq=wt_seq,\n",
    "    id_to_aa=id_to_aa,\n",
    "    residue_ids=residue_ids,\n",
    "    total=50000,\n",
    "    max_mut=6,\n",
    "    mutation_mode=\"aggressive\",\n",
    "    allowed_positions=None,\n",
    "    seed=42,\n",
    "    save_path=\"aggressive_candidates.csv\"\n",
    ")\n",
    "df_aggressive[\"source\"] = \"aggressive\"\n",
    "df_aggressive.to_csv(\"vae_for_1\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539a9fcb-c4d4-4970-af6b-c5924028e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conservative = generate_sequences_from_vae(\n",
    "    model=model,\n",
    "    wt_seq=wt_seq,\n",
    "    id_to_aa=id_to_aa,\n",
    "    residue_ids=residue_ids,\n",
    "    total=30000,\n",
    "    max_mut=3,\n",
    "    mut_rate=0.5,\n",
    "    mutation_mode=\"conservative\",\n",
    "    allowed_positions=None,\n",
    "    seed=42,\n",
    "    save_path=\"conservative_candidates.csv\"\n",
    ")\n",
    "df_conservative[\"source\"] = \"conservative\"\n",
    "df_conservative.to_csv(\"vae_for_2\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ad83dd-4023-4e6c-9db8-0ed62a663667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plm_aggressive = generate_sequences_from_vae(\n",
    "    model=model,\n",
    "    wt_seq=wt_seq,\n",
    "    id_to_aa=id_to_aa,\n",
    "    residue_ids=residue_ids,\n",
    "    total=70000,\n",
    "    max_mut=6,\n",
    "    mutation_mode=\"aggressive\",\n",
    "    allowed_positions=combined_positions,\n",
    "    seed=42,\n",
    "    save_path=\"plm_aggressive_candidates.csv\"\n",
    ")\n",
    "df_plm_aggressive[\"source\"] = \"plm_aggressive\"\n",
    "df_plm_aggressive.to_csv(\"vae_for_3\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ca0f381-91a6-471a-b4f0-9660a40a655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plm_conservative = generate_sequences_from_vae(\n",
    "    model=model,\n",
    "    wt_seq=wt_seq,\n",
    "    id_to_aa=id_to_aa,\n",
    "    residue_ids=residue_ids,\n",
    "    total=50000,\n",
    "    max_mut=3,\n",
    "    mut_rate=0.5,\n",
    "    mutation_mode=\"conservative\",\n",
    "    allowed_positions=combined_positions,\n",
    "    seed=42,\n",
    "    save_path=\"plm_conservative_candidates.csv\"\n",
    ")\n",
    "df_plm_conservative[\"source\"] = \"plm_conservative\"\n",
    "df_plm_conservative.to_csv(\"vae_for_4\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d4fdc-baa5-40a7-89e1-d6c33361362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并四类结果并保存\n",
    "import pandas as pd\n",
    "df_all = pd.concat([\n",
    "    df_aggressive,\n",
    "    df_conservative,\n",
    "    df_plm_aggressive,\n",
    "    df_plm_conservative\n",
    "], ignore_index=True)\n",
    "\n",
    "df_all.to_csv(\"vae_200k_generated.csv\", index=False)\n",
    "print(\"All 4 mutation sets generated and saved to vae_200k_generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ddcffe-d9f3-48ea-b617-aec63857acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06166a-0e02-4773-a817-d7ad187feb52",
   "metadata": {},
   "source": [
    "## **第四阶段：亮度预测器构建与评估**\n",
    "\n",
    "为了对生成的大规模突变蛋白序列进行有效筛选，本阶段构建了一个基于蛋白语言模型（Protein Language Model, PLM）表示的**亮度预测模型**，用于在无需实验的前提下，对每条突变序列进行亮度性能的前期评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff2c12-b303-4ca3-bd41-9f782440091d",
   "metadata": {},
   "source": [
    "### **4.1.1常量定义与环境初始化**\n",
    "\n",
    "本阶段开始于一系列**环境适配与超参数定义**，用于确保模型在不同计算资源下均可稳定运行：\n",
    "\n",
    "- **模型选择策略**\n",
    "   根据计算资源是否具备 CUDA 加速自动选择合适的预训练蛋白语言模型：\n",
    "  - 若存在 GPU（如 Tesla V100），加载高容量模型 `esm2_t33_650M_UR50D`；\n",
    "  - 若无 GPU，则默认回退至轻量模型 `esm2_t6_8M_UR50D`。\n",
    "- **批次大小配置**\n",
    "   考虑显存利用效率与推理速度，GPU 环境下批次大小设置为 16，CPU 下为 8，并建议在大显存环境中进一步提高（如 32~64）。\n",
    "- **样本上限控制**\n",
    "   为避免嵌入阶段计算负担过重，设置了最大嵌入样本数 `MAX_TRAIN_SAMPLES_FOR_EMBEDDING = 58000`。如真实训练集规模大于该值，则将根据随机种子 `SEED=42` 抽样子集参与特征提取过程。\n",
    "- **警告控制与时间记录**\n",
    "   为提高输出信息清晰度，屏蔽不影响结果的警告，并引入 `time` 模块记录嵌入提取与模型训练所耗时间，方便性能分析与优化。\n",
    "\n",
    "通过该初始化步骤，系统能够根据运行环境自动适配计算图、选择合适模型、合理配置推理任务，最大化利用资源，为后续大规模序列嵌入与预测提供了高效稳定的运行基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce82b124-d6ff-4e91-9dd4-ceda458ce3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到CUDA GPU，将使用GPU进行计算。\n",
      "正在使用设备: cuda\n",
      "将加载的ESM模型: esm2_t33_650M_UR50D\n",
      "将使用的批次大小: 16\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import esm  # fair-esm库\n",
    "import os\n",
    "import time  # 用于计时\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略一些不影响结果的警告\n",
    "\n",
    "# --- 优化常量 ---\n",
    "# 根据设备选择合适的ESM模型\n",
    "ESM_MODEL_NAME_CPU = \"esm2_t6_8M_UR50D\"\n",
    "ESM_MODEL_NAME_GPU = \"esm2_t33_650M_UR50D\" #v100调试训练时可esm2_t33_650M_UR50D替换用esm2_t36_3B_UR50D\n",
    "\n",
    "# 根据设备选择合适的批次大小\n",
    "CPU_BATCH_SIZE = 8\n",
    "GPU_BATCH_SIZE = 16 # GPU通常可以处理更大的批次，可以根据GPU内存调整，v100时可采用32-64\n",
    "\n",
    "# 用于嵌入的最大训练样本数（可按需调整）\n",
    "# 如果数据集小于此值，则使用所有样本\n",
    "MAX_TRAIN_SAMPLES_FOR_EMBEDDING = 58000\n",
    "SEED = 42 # 确保采样可复现\n",
    "\n",
    "# --- 1. 设备检测 ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    ESM_MODEL_NAME = ESM_MODEL_NAME_GPU\n",
    "    BATCH_SIZE = GPU_BATCH_SIZE\n",
    "    print(\"检测到CUDA GPU，将使用GPU进行计算。\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    ESM_MODEL_NAME = ESM_MODEL_NAME_CPU\n",
    "    BATCH_SIZE = CPU_BATCH_SIZE\n",
    "    print(\"未检测到CUDA GPU或CUDA不可用，将使用CPU进行计算。\")\n",
    "\n",
    "print(f\"正在使用设备: {DEVICE}\")\n",
    "print(f\"将加载的ESM模型: {ESM_MODEL_NAME}\")\n",
    "print(f\"将使用的批次大小: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb3130-7a27-4742-b221-cbeaf53b13a1",
   "metadata": {},
   "source": [
    "## （优化）必要时对训练数据进行采样（我们不需要） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c2e914a-980b-4adc-9dec-1428d51cad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在检查训练数据大小以进行采样...\n",
      "训练数据大小 (51715) 在限制范围内。使用所有序列。\n",
      "\n",
      "正在加载ESM模型: esm2_t12_35M_UR50D 到设备 cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t12_35M_UR50D.pt\" to /home/ma-user/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t12_35M_UR50D-contact-regression.pt\" to /home/ma-user/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D-contact-regression.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM模型在 5.26 秒内成功加载。\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n正在检查训练数据大小以进行采样...\")\n",
    "# 直接使用已加载的 avGFP_train_df\n",
    "if len(avGFP_train_df) > MAX_TRAIN_SAMPLES_FOR_EMBEDDING:\n",
    "    print(f\"训练数据大小 ({len(avGFP_train_df)}) 超过限制 ({MAX_TRAIN_SAMPLES_FOR_EMBEDDING})。正在采样...\")\n",
    "    # 对DataFrame进行采样以减少用于嵌入的序列数量\n",
    "    sampled_train_df = avGFP_train_df.sample(n=MAX_TRAIN_SAMPLES_FOR_EMBEDDING, random_state=SEED)\n",
    "    print(f\"使用 {len(sampled_train_df)} 个采样序列进行嵌入。\")\n",
    "else:\n",
    "    print(f\"训练数据大小 ({len(avGFP_train_df)}) 在限制范围内。使用所有序列。\")\n",
    "    # 使用完整的DataFrame\n",
    "    sampled_train_df = avGFP_train_df.copy()  # 使用副本以避免修改原始数据\n",
    "\n",
    "# --- 3.1 加载适合选定设备的ESM模型 ---\n",
    "print(f\"\\n正在加载ESM模型: {ESM_MODEL_NAME} 到设备 {DEVICE}...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # 加载模型和字母表\n",
    "    esm_model, alphabet = esm.pretrained.load_model_and_alphabet(ESM_MODEL_NAME)\n",
    "    esm_model.eval()  # 设置为评估模式\n",
    "    esm_model = esm_model.to(DEVICE)  # 将模型移动到选定的设备 (GPU or CPU)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    print(f\"ESM模型在 {time.time() - start_time:.2f} 秒内成功加载。\")\n",
    "except Exception as e:\n",
    "    print(f\"加载ESM模型时出错: {e}\")\n",
    "    print(\"请确保已安装 'fair-esm' 库 (pip install fair-esm) 且模型名称正确。\")\n",
    "    print(\"对于GPU使用，请确保已安装与CUDA兼容的PyTorch版本。\")\n",
    "    exit()  # 无法加载模型则退出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54891580-1c95-4f12-88cb-029159784655",
   "metadata": {},
   "source": [
    "## 4.1.2 高亮度子集的 ESM 嵌入生成\n",
    "在本节中，你针对亮度预测任务中的性能瓶颈问题，采取了聚焦于高亮度子集样本的策略，以提升模型对高亮度蛋白的判别能力。主要包括以下关键操作："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf70a1476845b63",
   "metadata": {},
   "source": [
    "###  **嵌入提取函数构建 (`get_esm_embeddings`)**\n",
    "\n",
    "- 利用 `ESM-2` 系列蛋白语言模型（根据设备自动选择），对输入序列进行特征提取。\n",
    "- 嵌入提取采用**平均池化策略**：从模型最后一层输出中去除特殊标记 `<cls>`、`<eos>`、`<pad>` 后，计算每条序列的平均表示，形成稳定且维度统一的 embedding。\n",
    "\n",
    "> 特别设计的功能包括：\n",
    ">\n",
    "> - 自动跳过或降维 `NaN` 批次，保证整体嵌入维度可连接；\n",
    "> - 在出现 CUDA OOM 错误时能自动 fallback，避免任务中断；\n",
    "> - 批次进度条输出和错误追踪打印，利于大规模处理时的调试与监控。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3240c6-4939-4dbd-a6cd-54ae73fb0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esm_embeddings(sequences, model, alphabet, batch_converter, device, batch_size):\n",
    "    \"\"\"\n",
    "    在指定设备(CPU或GPU)上生成ESM嵌入（平均池化）。\n",
    "    将输入数据和模型都移动到 `device`。\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    num_sequences = len(sequences)\n",
    "    num_batches = (num_sequences + batch_size - 1) // batch_size\n",
    "    model.eval() # 确保模型在评估模式\n",
    "    #model = torch.compile(model)\n",
    "    model = model.to(device) # 确保模型在目标设备\n",
    "\n",
    "    print(f\"正在为 {num_sequences} 个序列生成嵌入，共 {num_batches} 个批次（批次大小: {batch_size}，设备: {device}）...\")\n",
    "    start_time_embed = time.time()\n",
    "\n",
    "    with torch.no_grad():  # 对推理速度和内存至关重要\n",
    "        with torch.cuda.amp.autocast():\n",
    "            for i in range(0, num_sequences, batch_size):\n",
    "                batch_seqs = sequences[i:i + batch_size]\n",
    "                batch_labels = [f\"seq_{j + i}\" for j in range(len(batch_seqs))]  # 每个批次项的唯一标签\n",
    "                data = list(zip(batch_labels, batch_seqs))\n",
    "\n",
    "                try:\n",
    "                    # 1. 准备批次\n",
    "                    _, _, batch_tokens = batch_converter(data)\n",
    "                    # 将令牌移动到目标设备 (GPU or CPU)\n",
    "                    batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "                    # 2. 获取表示（只需要最后一层）\n",
    "                    # results = model(batch_tokens, repr_layers=[model.num_layers], return_contacts=False) # 旧版写法\n",
    "                    # 适配新版 fair-esm (>=2.0.0)\n",
    "                    results = model(batch_tokens, repr_layers=[model.num_layers])\n",
    "                    token_representations = results[\"representations\"][model.num_layers]\n",
    "\n",
    "                    # 3. 对序列长度进行平均池化（忽略CLS/EOS/PAD令牌）\n",
    "                    # token_representations形状: [batch_size, seq_len+2, embed_dim]\n",
    "                    seq_repr_list = []\n",
    "                    for j, seq in enumerate(batch_seqs):\n",
    "                        # +1 是因为 batch_converter 添加了 <cls> token\n",
    "                        actual_len = len(seq)\n",
    "                        # 从索引 1 开始到 actual_len 结束 (不包括 <eos> 和 padding)\n",
    "                        # 注意：需要确保batch_converter没有添加其他的特殊token影响长度计算\n",
    "                        # 对于标准esm模型，通常是<cls>...seq...<eos><pad>...\n",
    "                        # 所以 token_representations[j, 1 : actual_len + 1, :] 是正确的\n",
    "                        seq_tokens_repr = token_representations[j, 1 : actual_len + 1, :]\n",
    "                        seq_repr = seq_tokens_repr.mean(dim=0) # 对实际序列长度的表示进行平均\n",
    "                        seq_repr_list.append(seq_repr)\n",
    "\n",
    "                    batch_seq_repr = torch.stack(seq_repr_list, dim=0) # [batch_size, embed_dim]\n",
    "\n",
    "                    # 4. 存储结果 (移回CPU以聚合和后续处理，如转Numpy)\n",
    "                    embeddings.append(batch_seq_repr.cpu())\n",
    "\n",
    "                    if (i // batch_size + 1) % 10 == 0 or (i // batch_size + 1) == num_batches:  # 每10个批次或最后一个批次打印进度\n",
    "                        elapsed_time = time.time() - start_time_embed\n",
    "                        print(f\"  已处理批次 {i // batch_size + 1}/{num_batches}... (耗时: {elapsed_time:.2f} 秒)\")\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    if \"CUDA out of memory\" in str(e) and device == torch.device(\"cuda\"):\n",
    "                        print(f\"\\n处理批次 {i // batch_size + 1} 时发生CUDA内存不足错误: {e}\")\n",
    "                        print(f\"当前批次大小: {batch_size}。尝试减小批次大小或使用更小的模型。\")\n",
    "                        # 记录错误并跳过，但会导致数据丢失\n",
    "                        embed_dim = model.embed_dim\n",
    "                        error_placeholder = torch.full((len(batch_seqs), embed_dim), float('nan'), device='cpu') # 存放在CPU\n",
    "                        embeddings.append(error_placeholder)\n",
    "                        torch.cuda.empty_cache() # 尝试释放一些内存\n",
    "                    else:\n",
    "                        print(f\"处理批次 {i // batch_size + 1} 时发生运行时错误: {e}\")\n",
    "                        embed_dim = model.embed_dim\n",
    "                        error_placeholder = torch.full((len(batch_seqs), embed_dim), float('nan'), device='cpu')\n",
    "                        embeddings.append(error_placeholder)\n",
    "                except Exception as e:\n",
    "                    print(f\"处理批次 {i // batch_size + 1} 时发生未知错误: {e}\")\n",
    "                    # 处理错误，例如跳过批次或用NaN填充\n",
    "                    embed_dim = model.embed_dim  # 获取预期维度\n",
    "                    error_placeholder = torch.full((len(batch_seqs), embed_dim), float('nan'), device='cpu')\n",
    "                    embeddings.append(error_placeholder)\n",
    "\n",
    "\n",
    "    total_embed_time = time.time() - start_time_embed\n",
    "    print(f\"嵌入生成在 {total_embed_time:.2f} 秒内完成。\")\n",
    "    if num_sequences > 0:\n",
    "      print(f\"平均每个序列耗时: {total_embed_time / num_sequences:.4f} 秒\")\n",
    "\n",
    "\n",
    "    if not embeddings:\n",
    "        return torch.tensor([])  # 返回空张量\n",
    "\n",
    "    # 连接所有批次的结果\n",
    "    try:\n",
    "        full_embeddings = torch.cat(embeddings, dim=0)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"连接嵌入批次时出错: {e}\")\n",
    "        print(\"这可能发生在批次处理出错导致维度不匹配时。请检查之前的错误信息。\")\n",
    "        # 尝试找出有效批次并连接，或者返回错误\n",
    "        try:\n",
    "            embed_dim = model.embed_dim # 获取模型维度\n",
    "            valid_embeddings = [emb for emb in embeddings if isinstance(emb, torch.Tensor) and emb.ndim == 2 and emb.shape[1] == embed_dim and not torch.isnan(emb).all()]\n",
    "            if valid_embeddings:\n",
    "                print(\"尝试仅连接有效的嵌入批次...\")\n",
    "                full_embeddings = torch.cat(valid_embeddings, dim=0)\n",
    "            else:\n",
    "                print(\"没有有效的嵌入批次可以连接。\")\n",
    "                return torch.tensor([]) # 返回空张量\n",
    "        except Exception as concat_err:\n",
    "             print(f\"尝试连接有效嵌入时再次出错: {concat_err}\")\n",
    "             return torch.tensor([]) # 返回空张量\n",
    "\n",
    "\n",
    "    return full_embeddings  # 作为单个张量返回 (在 CPU 上)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337b9fb-a174-4cd3-aafd-e7a0e902b14a",
   "metadata": {},
   "source": [
    "## 为（可能采样的）训练数据生成嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86d791-a56c-478a-98b7-1f3a86762541",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_to_embed = sampled_train_df['full_sequence'].tolist()\n",
    "\n",
    "X = None # 初始化 X\n",
    "y = None # 初始化 y\n",
    "\n",
    "if train_sequences_to_embed:\n",
    "    # 获取嵌入作为PyTorch张量\n",
    "    train_embeddings_tensor = get_esm_embeddings(\n",
    "        train_sequences_to_embed,\n",
    "        esm_model,\n",
    "        alphabet,\n",
    "        batch_converter,\n",
    "        DEVICE,            # 传递检测到的设备\n",
    "        batch_size=BATCH_SIZE # 传递适合设备的批次大小\n",
    "    )\n",
    "\n",
    "    if train_embeddings_tensor.numel() > 0: # 检查张量是否为空\n",
    "        print(f\"生成的嵌入张量形状: {train_embeddings_tensor.shape}\")\n",
    "\n",
    "        # 将张量转换为numpy数组以与scikit-learn兼容\n",
    "        # 因为上面 append 时已经 .cpu()，所以这里 tensor 已经在 CPU 上\n",
    "        train_embeddings_np = train_embeddings_tensor.numpy()\n",
    "\n",
    "        # --- 处理嵌入过程中可能出现的NaN值 ---\n",
    "        nan_rows_mask = np.isnan(train_embeddings_np).any(axis=1)\n",
    "        if np.any(nan_rows_mask):\n",
    "            num_nan_rows = nan_rows_mask.sum()\n",
    "            print(f\"警告: 在嵌入中发现 {num_nan_rows} 行NaN值（可能是由于错误）。正在移除这些行及其对应的原始数据。\")\n",
    "\n",
    "            # 过滤嵌入数组和相应的DataFrame行\n",
    "            valid_indices_bool = ~nan_rows_mask\n",
    "            # 获取原始sampled_train_df中对应valid_indices_bool为True的索引\n",
    "            original_indices = sampled_train_df.index[valid_indices_bool]\n",
    "\n",
    "            train_embeddings_np = train_embeddings_np[valid_indices_bool]\n",
    "            # 使用 .loc 和原始索引进行过滤，确保正确对齐\n",
    "            sampled_train_df_filtered = sampled_train_df.loc[original_indices]\n",
    "\n",
    "            print(f\"过滤后的嵌入数据大小: {train_embeddings_np.shape[0]}\")\n",
    "            print(f\"过滤后的DataFrame大小: {len(sampled_train_df_filtered)}\")\n",
    "\n",
    "\n",
    "            # 检查过滤后是否还有数据\n",
    "            if train_embeddings_np.shape[0] == 0:\n",
    "                 print(\"\\n错误: 移除NaN值后没有剩余数据。无法继续。\")\n",
    "                 X, y = None, None\n",
    "            else:\n",
    "                 # --- 为模型训练准备最终的X和y ---\n",
    "                 if train_embeddings_np.shape[0] == len(sampled_train_df_filtered):\n",
    "                     X = train_embeddings_np\n",
    "                     y = sampled_train_df_filtered['Brightness'].values\n",
    "                     print(f\"\\n准备好的X（嵌入）形状: {X.shape}, y（亮度）形状: {y.shape}\")\n",
    "                     print(\"步骤3（嵌入生成）完成。\")\n",
    "                 else:\n",
    "                     # 这个情况理论上不应发生，因为我们同时过滤了两者\n",
    "                     print(f\"\\n错误: NaN过滤后最终嵌入 ({train_embeddings_np.shape[0]}) 和DataFrame行 ({len(sampled_train_df_filtered)}) 数量不匹配。\")\n",
    "                     print(\"无法进行训练。请检查过滤逻辑。\")\n",
    "                     X, y = None, None\n",
    "\n",
    "        else:\n",
    "             # 没有NaN值，直接使用\n",
    "             print(\"嵌入中未检测到NaN值。\")\n",
    "             X = train_embeddings_np\n",
    "             # 直接从 sampled_train_df 获取 y，因为没有过滤\n",
    "             y = sampled_train_df['Brightness'].values\n",
    "             print(f\"\\n准备好的X（嵌入）形状: {X.shape}, y（亮度）形状: {y.shape}\")\n",
    "             print(\"步骤3（嵌入生成）完成。\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n嵌入过程未能生成有效的嵌入张量（可能所有批次都出错或输入为空）。\")\n",
    "        X, y = None, None\n",
    "\n",
    "else:\n",
    "    print(\"\\n采样/预处理后没有可用的序列进行嵌入。\")\n",
    "    X, y = None, None\n",
    "\n",
    "# --- 清理（可选，有助于释放内存，特别是GPU内存） ---\n",
    "print(\"\\n正在清理内存...\")\n",
    "del esm_model, alphabet, batch_converter\n",
    "# 删除可能已创建的张量和numpy数组\n",
    "if 'train_embeddings_tensor' in locals() and isinstance(train_embeddings_tensor, torch.Tensor):\n",
    "    del train_embeddings_tensor\n",
    "if 'train_embeddings_np' in locals():\n",
    "    del train_embeddings_np\n",
    "# 删除采样或过滤后的DataFrame副本\n",
    "if 'sampled_train_df' in locals():\n",
    "    del sampled_train_df\n",
    "if 'sampled_train_df_filtered' in locals():\n",
    "     del sampled_train_df_filtered\n",
    "\n",
    "# 如果使用了GPU，显式清空缓存\n",
    "if DEVICE == torch.device(\"cuda\"):\n",
    "    print(\"清空CUDA缓存...\")\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"清理完成。\")\n",
    "\n",
    "# --- 现在X和y（如果成功创建）已准备好用于步骤4（模型训练） ---\n",
    "if X is not None and y is not None:\n",
    "     print(f\"\\n数据准备就绪，可以进行模型训练。X shape: {X.shape}, y shape: {y.shape}\")\n",
    "     # 这里可以接上你的模型训练代码，例如：\n",
    "     # from sklearn.model_selection import train_test_split\n",
    "     # from sklearn.ensemble import RandomForestRegressor\n",
    "     # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "     # model = RandomForestRegressor(n_estimators=100, random_state=SEED)\n",
    "     # model.fit(X_train, y_train)\n",
    "     # print(\"模型训练完成。\")\n",
    "     # score = model.score(X_test, y_test)\n",
    "     # print(f\"模型在测试集上的 R^2 分数: {score:.4f}\")\n",
    "else:\n",
    "     print(\"\\n数据准备失败，无法进行模型训练。请检查之前的日志输出。\")\n",
    "\n",
    "# 示例: 如果X不为None，则打印(X.shape, y.shape)，否则打印(\"X is None\")\n",
    "# print(\"\\n--- 最终检查 ---\")\n",
    "# print((X.shape, y.shape) if X is not None else \"X is None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8860441-8bba-4fb9-82d9-e24e88034ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"X_esm_embedding.npy\", X)\n",
    "np.save(\"y_brightness.npy\", y)\n",
    "X = np.load(\"X_esm_embedding.npy\")\n",
    "y = np.load(\"y_brightness.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f47734ceea02b",
   "metadata": {},
   "source": [
    "### **高亮度样本筛选**\n",
    "------\n",
    "- 从训练集中筛选亮度值大于 3.5 的序列，构建亮度子集 `bright_df`，以避免低亮度样本对回归模型训练产生干扰。\n",
    "- 此步骤确保模型更加专注于实际应用更具价值的“高亮”突变体的学习与拟合。\n",
    "### **异常值处理与清洗**\n",
    "\n",
    "- 检查所有生成的嵌入是否包含 `NaN` 行，如发现则剔除对应样本并重新对齐标签 `y`。\n",
    "- 这样可确保训练模型的数据矩阵 `X` 完整且数值有效。\n",
    "\n",
    "------\n",
    "\n",
    "###  **保存处理结果**\n",
    "\n",
    "- 最终提取出的 `X`（embedding 特征）和 `y`（亮度标签）分别以 `.npy` 格式持久化，便于后续的多轮模型调试与超参数搜索。\n",
    "- 同时输出了完整的亮度子集元数据表 `bright_subset_meta.csv`，包括序列及其亮度标签，便于回溯与可视化分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b13c89-ec79-4a1e-a54e-64585bd68461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 构造亮度 > 3.5 的训练子集 ---\n",
    "bright_df = avGFP_train_df.copy()\n",
    "bright_df = bright_df[bright_df[\"Brightness\"] > 3.5].dropna(subset=[\"full_sequence\", \"Brightness\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"亮度 > 3.5 的样本数量: {len(bright_df)}\")\n",
    "print(f\"亮度范围: {bright_df['Brightness'].min():.2f} - {bright_df['Brightness'].max():.2f}\")\n",
    "\n",
    "# --- 获取 ESM 嵌入 ---\n",
    "bright_sequences = bright_df[\"full_sequence\"].astype(str).tolist()\n",
    "train_embeddings_tensor = get_esm_embeddings(\n",
    "    bright_sequences, esm_model, alphabet, batch_converter, DEVICE, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "if train_embeddings_tensor.numel() == 0:\n",
    "    print(\"嵌入失败，没有生成有效向量。\")\n",
    "    X, y = None, None\n",
    "else:\n",
    "    print(f\"生成嵌入张量形状: {train_embeddings_tensor.shape}\")\n",
    "    train_embeddings_np = train_embeddings_tensor.cpu().numpy()\n",
    "\n",
    "    # 处理 NaN\n",
    "    nan_mask = np.isnan(train_embeddings_np).any(axis=1)\n",
    "    if nan_mask.any():\n",
    "        print(f\"发现 NaN 行: {nan_mask.sum()}，正在剔除...\")\n",
    "        X = train_embeddings_np[~nan_mask]\n",
    "        bright_df = bright_df.loc[~nan_mask].reset_index(drop=True)\n",
    "        y = bright_df[\"Brightness\"].values\n",
    "    else:\n",
    "        X = train_embeddings_np\n",
    "        y = bright_df[\"Brightness\"].values\n",
    "\n",
    "    print(f\"最终用于训练的数据: X.shape = {X.shape}, y.shape = {y.shape}\")\n",
    "\n",
    "# --- 保存 ---\n",
    "np.save(\"bright_subset_embeddings.npy\", X)\n",
    "np.save(\"bright_subset_labels.npy\", y)\n",
    "bright_df.to_csv(\"bright_subset_meta.csv\", index=False)\n",
    "print(\"已保存亮度子集的嵌入与标签。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083f863-80c4-40a2-9a17-1f4c56bbf545",
   "metadata": {},
   "source": [
    "## **4.2 构建与优化亮度预测器**\n",
    "\n",
    "为了对生成的蛋白突变序列进行可信亮度打分，我们训练了一个基于 ESM 嵌入的回归模型来预测荧光强度（亮度），并结合高亮度子集进一步提升对关键候选的识别能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175fc7ae25a11cc",
   "metadata": {},
   "source": [
    "### **4.2.1 基础亮度预测器训练（ResMLP）**\n",
    "\n",
    "#### 模型目标\n",
    "\n",
    "建立一个可泛化、鲁棒的亮度回归器，基于大规模训练集的蛋白 ESM 嵌入向量，拟合其对应的实验亮度值。\n",
    "\n",
    "#### 核心步骤\n",
    "\n",
    "- **数据预处理**\n",
    "  - 读取 `X_esm_embedding.npy` 和 `y_brightness.npy`，并使用 `StandardScaler` 标准化嵌入特征。\n",
    "  - 使用 `train_test_split` 将数据划分为训练集和验证集，构造 PyTorch `DataLoader`。\n",
    "  - 将 scaler 对象持久化保存，便于后续新数据推理标准化对齐。\n",
    "- **模型结构**\n",
    "  - 自定义 `BrightnessRegressor` 模块，采用 3 层全连接网络 + BatchNorm + ReLU + Dropout 的多层感知机（MLP）架构。\n",
    "  - 最后一层输出单个标量亮度预测值。\n",
    "- **训练策略**\n",
    "  - 使用 MSELoss 和 Adam 优化器，设定 early stopping（5轮不提升则提前终止）监控验证集 R² 分数。\n",
    "  - 在每一轮中记录最佳模型并保存为 `best_brightness_model.pt`。\n",
    "\n",
    "####  训练表现与优势\n",
    "\n",
    "- 模型在训练集与验证集上均表现出色，尤其适合对整个亮度分布的整体学习；\n",
    "- 为后续“高亮度特征学习”提供了良好的初始化权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fdfbcab-edb6-477b-b7ff-cafe2b4676ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into training (41372) and validation (10343) sets.\n",
      "\n",
      "Training ResMLP Regressor (PyTorch)...\n",
      "Epoch 1/30 - Train Loss: 6.7494 | Val R²: -2.8156\n",
      "R² improved to -2.8156 — saving model.\n",
      "Epoch 2/30 - Train Loss: 2.9572 | Val R²: -0.9135\n",
      "R² improved to -0.9135 — saving model.\n",
      "Epoch 3/30 - Train Loss: 1.4630 | Val R²: 0.0485\n",
      "R² improved to 0.0485 — saving model.\n",
      "Epoch 4/30 - Train Loss: 0.7971 | Val R²: 0.4041\n",
      "R² improved to 0.4041 — saving model.\n",
      "Epoch 5/30 - Train Loss: 0.5334 | Val R²: 0.5179\n",
      "R² improved to 0.5179 — saving model.\n",
      "Epoch 6/30 - Train Loss: 0.4310 | Val R²: 0.6545\n",
      "R² improved to 0.6545 — saving model.\n",
      "Epoch 7/30 - Train Loss: 0.3887 | Val R²: 0.6875\n",
      "R² improved to 0.6875 — saving model.\n",
      "Epoch 8/30 - Train Loss: 0.3559 | Val R²: 0.7036\n",
      "R² improved to 0.7036 — saving model.\n",
      "Epoch 9/30 - Train Loss: 0.3402 | Val R²: 0.6859\n",
      "No R² improvement for 1 epoch(s).\n",
      "Epoch 10/30 - Train Loss: 0.3222 | Val R²: 0.7362\n",
      "R² improved to 0.7362 — saving model.\n",
      "Epoch 11/30 - Train Loss: 0.3051 | Val R²: 0.7355\n",
      "No R² improvement for 1 epoch(s).\n",
      "Epoch 12/30 - Train Loss: 0.2838 | Val R²: 0.7453\n",
      "R² improved to 0.7453 — saving model.\n",
      "Epoch 13/30 - Train Loss: 0.2804 | Val R²: 0.7554\n",
      "R² improved to 0.7554 — saving model.\n",
      "Epoch 14/30 - Train Loss: 0.2685 | Val R²: 0.7445\n",
      "No R² improvement for 1 epoch(s).\n",
      "Epoch 15/30 - Train Loss: 0.2552 | Val R²: 0.7561\n",
      "R² improved to 0.7561 — saving model.\n",
      "Epoch 16/30 - Train Loss: 0.2564 | Val R²: 0.7673\n",
      "R² improved to 0.7673 — saving model.\n",
      "Epoch 17/30 - Train Loss: 0.2439 | Val R²: 0.7741\n",
      "R² improved to 0.7741 — saving model.\n",
      "Epoch 18/30 - Train Loss: 0.2330 | Val R²: 0.7786\n",
      "R² improved to 0.7786 — saving model.\n",
      "Epoch 19/30 - Train Loss: 0.2307 | Val R²: 0.7781\n",
      "No R² improvement for 1 epoch(s).\n",
      "Epoch 20/30 - Train Loss: 0.2235 | Val R²: 0.7848\n",
      "R² improved to 0.7848 — saving model.\n",
      "Epoch 21/30 - Train Loss: 0.2131 | Val R²: 0.7889\n",
      "R² improved to 0.7889 — saving model.\n",
      "Epoch 22/30 - Train Loss: 0.2104 | Val R²: 0.7628\n",
      "No R² improvement for 1 epoch(s).\n",
      "Epoch 23/30 - Train Loss: 0.2033 | Val R²: 0.7975\n",
      "R² improved to 0.7975 — saving model.\n",
      "Epoch 24/30 - Train Loss: 0.1963 | Val R²: 0.7824\n",
      "No R² improvement for 1 epoch(s).\n",
      "Epoch 25/30 - Train Loss: 0.2002 | Val R²: 0.7952\n",
      "No R² improvement for 2 epoch(s).\n",
      "Epoch 26/30 - Train Loss: 0.1903 | Val R²: 0.7961\n",
      "No R² improvement for 3 epoch(s).\n",
      "Epoch 27/30 - Train Loss: 0.1877 | Val R²: 0.7898\n",
      "No R² improvement for 4 epoch(s).\n",
      "Epoch 28/30 - Train Loss: 0.1841 | Val R²: 0.7939\n",
      "No R² improvement for 5 epoch(s).\n",
      "\n",
      " Early stopping triggered: no improvement in 5 epochs.\n",
      "ResMLP training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 标准化\n",
    "X = np.load(\"X_esm_embedding.npy\")  \n",
    "y = np.load(\"y_brightness.npy\") \n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "# 载入 scaler 并用于新数据\n",
    "#scaler = joblib.load('scaler.pkl')\n",
    "#X_new = scaler.transform(X_new)  # 然后送入 PyTorch 模型前转换为 tensor\n",
    "\n",
    "# --- 4.1 划分训练集和验证集 ---\n",
    "if len(X) > 10:\n",
    "    X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "    print(f\"Split data into training ({len(X_train_np)}) and validation ({len(X_val_np)}) sets.\")\n",
    "else:\n",
    "    print(\"Dataset too small for validation split, using all data for training.\")\n",
    "    X_train_np, y_train_np = X, y\n",
    "    X_val_np, y_val_np = None, None\n",
    "\n",
    "# --- 转换为 PyTorch Tensor ---\n",
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "if X_val_np is not None:\n",
    "    X_val_tensor = torch.tensor(X_val_np, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val_np, dtype=torch.float32)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "else:\n",
    "    val_loader = None\n",
    "\n",
    "# --- ResMLP 模型定义 ---\n",
    "class BrightnessRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=1280):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)  # 输出为 shape: [B]\n",
    "\n",
    "# --- 训练配置 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BrightnessRegressor(input_dim=X.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "epochs = 30\n",
    "early_stop_patience = 5\n",
    "best_r2 = -np.inf\n",
    "no_improve_count = 0\n",
    "\n",
    "print(\"\\nTraining ResMLP Regressor (PyTorch)...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataset)\n",
    "\n",
    "    if val_loader:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        preds_all = []\n",
    "        targets_all = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_loss += loss_fn(preds, yb).item() * xb.size(0)\n",
    "                preds_all.append(preds.cpu().numpy())\n",
    "                targets_all.append(yb.cpu().numpy())\n",
    "\n",
    "        y_pred_val = np.concatenate(preds_all)\n",
    "        y_val = np.concatenate(targets_all)\n",
    "        r2 = r2_score(y_val, y_pred_val)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} | Val R²: {r2:.4f}\")\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            no_improve_count = 0\n",
    "            torch.save(model.state_dict(), \"best_brightness_model.pt\")\n",
    "            print(f\"R² improved to {r2:.4f} — saving model.\")\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            print(f\"No R² improvement for {no_improve_count} epoch(s).\")\n",
    "            if no_improve_count >= early_stop_patience:\n",
    "                print(f\"\\n Early stopping triggered: no improvement in {early_stop_patience} epochs.\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "print(\"ResMLP training complete.\")\n",
    "\n",
    "# 模型结构应与训练时完全一致\n",
    "# model = BrightnessRegressor(input_dim=1280).to(device)\n",
    "\n",
    "# 加载保存的权重参数\n",
    "# model.load_state_dict(torch.load(\"best_brightness_model.pt\"))\n",
    "\n",
    "# 设置为推理模式\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9426c833d9910e27",
   "metadata": {},
   "source": [
    "### **4.2.2 微调模型：高亮度子集专属优化**\n",
    "\n",
    "#### 微调目标\n",
    "\n",
    "通过在亮度 > 3.5 的蛋白质序列上微调已有模型参数，使其更加专注于高亮突变体的亮度预测能力，从而提升后续筛选可信度。\n",
    "\n",
    "#### 核心操作\n",
    "\n",
    "- **载入高亮度样本嵌入与标签**\n",
    "  - 数据来源：`bright_subset_embeddings.npy` 与 `bright_subset_labels.npy`；\n",
    "  - 直接构建 PyTorch 数据集与 `DataLoader`。\n",
    "- **模型微调**\n",
    "  - 使用先前训练好的 `best_brightness_model.pt` 权重初始化模型；\n",
    "  - 设置较小学习率（`1e-5`），进行轻量级训练 10 轮；\n",
    "  - 优化器与损失函数同主训练阶段一致（Adam + MSELoss）；\n",
    "  - 训练完成后保存为 `brightness_model_finetuned_on_high.pt`。\n",
    "\n",
    "####  优势与意义\n",
    "\n",
    "- 该策略可视为领域自适应（domain adaptation）的一种形式；\n",
    "- 可显著提升模型在高亮度序列附近的局部拟合能力，使其更有效识别亮度增强突变体；\n",
    "- 微调后的模型可作为候选序列筛选阶段的核心打分器。\n",
    "\n",
    "------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c863a5e0-696d-42b2-bda2-3850146248b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (20219, 1280), y shape: (20219,)\n",
      "\n",
      "Start fine-tuning on high-brightness subset...\n",
      "Epoch 1/10 - Fine-tune Loss: 1.4054\n",
      "Epoch 2/10 - Fine-tune Loss: 0.9663\n",
      "Epoch 3/10 - Fine-tune Loss: 0.7243\n",
      "Epoch 4/10 - Fine-tune Loss: 0.5698\n",
      "Epoch 5/10 - Fine-tune Loss: 0.4482\n",
      "Epoch 6/10 - Fine-tune Loss: 0.3520\n",
      "Epoch 7/10 - Fine-tune Loss: 0.2852\n",
      "Epoch 8/10 - Fine-tune Loss: 0.2290\n",
      "Epoch 9/10 - Fine-tune Loss: 0.1858\n",
      "Epoch 10/10 - Fine-tune Loss: 0.1588\n",
      "\n",
      "Fine-tuning complete. Model saved as 'brightness_model_finetuned_on_high.pt'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# --- 加载亮度子集的嵌入和标签 ---\n",
    "X = np.load(\"bright_subset_embeddings.npy\")  # shape: [N, 1280]\n",
    "y = np.load(\"bright_subset_labels.npy\")      # shape: [N,]\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "# --- 转换为 PyTorch Tensor 并构建 DataLoader ---\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "finetune_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "finetune_loader = DataLoader(finetune_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# --- 定义模型（结构应与之前完全一致） ---\n",
    "\n",
    "# --- 初始化模型并加载之前的预训练权重 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BrightnessRegressor(input_dim=X.shape[1]).to(device)\n",
    "\n",
    "# 加载之前训练好的模型参数作为起点\n",
    "model.load_state_dict(torch.load(\"best_brightness_model.pt\"))\n",
    "\n",
    "# --- Fine-tuning 设置 ---\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # 微调用较小学习率\n",
    "loss_fn = nn.MSELoss()\n",
    "epochs = 10  # 微调通常不需要太多 epoch\n",
    "print(\"\\nStart fine-tuning on high-brightness subset...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xb, yb in finetune_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(finetune_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Fine-tune Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# --- 保存微调后的模型 ---\n",
    "torch.save(model.state_dict(), \"brightness_model_finetuned_on_high.pt\")\n",
    "print(\"\\nFine-tuning complete. Model saved as 'brightness_model_finetuned_on_high.pt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfcd0e-dd3a-4b0a-90a7-388c0c21d700",
   "metadata": {},
   "source": [
    "### **第五阶段：突变候选的多指标初筛（从200k → 5k）**\n",
    "\n",
    "本阶段的目标是从 VAE 生成的 20 万条突变候选序列中，基于多维指标进行排序打分，最终筛选出综合表现最优的 Top5000 条进入下一阶段的结构预测与亮度验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29cb179c44d055",
   "metadata": {},
   "source": [
    "#### **5.1 数据预处理与合法性筛选**\n",
    "\n",
    "首先加载 VAE 生成的突变候选序列（`vae_200k_generated.csv`），并执行以下清洗步骤：\n",
    "\n",
    "- **Exclusion List 过滤**：排除此前结构失败或生物实验明确无效的序列；\n",
    "- **序列去重**：防止模型重复生成同一突变序列；\n",
    "- **合法性判断**：使用 amino acid 字符集（20种天然氨基酸）和长度要求，确保突变序列在技术上可预测、可表达。\n",
    "\n",
    "结果从 200,000 条中保留约 100,000 条“合法 + 唯一”的候选序列，存入 `vae_100k_after_censored.csv`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b03df16-29c7-431b-b438-d12b1fcec928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "根据排除列表 (739 个序列) 进行过滤...\n",
      "移除了 17 个在排除列表中的序列。\n",
      "进一步去重，移除重复序列 96430 条。\n",
      "saving done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载生成序列表\n",
    "vae_df = pd.read_csv(\"vae_200k_generated.csv\")\n",
    "\n",
    "# 记录初始数量\n",
    "initial_count = len(vae_df)\n",
    "\n",
    "# 过滤排除列表\n",
    "print(f\"\\n根据排除列表 ({len(exclusion_sequences)} 个序列) 进行过滤...\")\n",
    "vae_df = vae_df[~vae_df[\"mutated_sequence\"].astype(str).isin(exclusion_sequences)].reset_index(drop=True)\n",
    "after_exclusion_count = len(vae_df)\n",
    "remove_count = initial_count - after_exclusion_count\n",
    "\n",
    "if remove_count > 0:\n",
    "    print(f\"移除了 {remove_count} 个在排除列表中的序列。\")\n",
    "else:\n",
    "    print(\"候选列表中的序列均不在排除列表中。\")\n",
    "\n",
    "# 去重操作\n",
    "before_dedup = len(vae_df)\n",
    "vae_df = vae_df.drop_duplicates(subset=[\"mutated_sequence\"]).reset_index(drop=True)\n",
    "dedup_count = before_dedup - len(vae_df)\n",
    "if dedup_count > 0:\n",
    "    print(f\"进一步去重，移除重复序列 {dedup_count} 条。\")\n",
    "else:\n",
    "    print(\"无重复序列，无需去重。\")\n",
    "\n",
    "# 保存最终结果\n",
    "vae_df.to_csv(\"vae_100k_after_censored.csv\", index=False)\n",
    "print(\"saving done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46e23ee7-7624-4380-a9ac-fc30f8807836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已从 VAE 库中排除 17 条 Exclusion List 中的序列。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(103553, 103553)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"已从 VAE 库中排除 {initial_count - after_exclusion_count} 条 Exclusion List 中的序列。\")\n",
    "# 重新加载 CSV 和 Excel 文件\n",
    "vae_df = pd.read_csv(\"vae_100k_after_censored.csv\")\n",
    "top10_df = pd.read_excel(\"Last year Top10 Amino Acid sequence.xlsx\",engine='openpyxl')\n",
    "\n",
    "# 合法性检查函数\n",
    "valid_amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "def check_sequence_validity(seq: str, expected_length: int = None) -> bool:\n",
    "    if expected_length and len(seq) != expected_length:\n",
    "        return False\n",
    "    return all(aa in valid_amino_acids for aa in seq)\n",
    "\n",
    "# 获取参考序列长度\n",
    "reference_length = len(top10_df.iloc[0, 0])\n",
    "\n",
    "# 应用合法性检查\n",
    "vae_df[\"is_valid\"] = vae_df[\"mutated_sequence\"].apply(lambda s: check_sequence_validity(s, expected_length=reference_length))\n",
    "\n",
    "# 过滤出合法序列\n",
    "valid_vae_df = vae_df[vae_df[\"is_valid\"]].reset_index(drop=True)\n",
    "\n",
    "# 返回合法序列数量\n",
    "valid_count = len(valid_vae_df)\n",
    "total_count = len(vae_df)\n",
    "valid_count, total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f2c357e533049",
   "metadata": {},
   "source": [
    "#### **5.2 评估指标计算与归一化**\n",
    "\n",
    "对剩余候选序列从以下三个维度进行生物意义上的打分：\n",
    "\n",
    "1. **BLOSUM62 相似性得分（blosum_score）**\n",
    "   - 衡量突变残基的保守性，越保守打分越高；\n",
    "   - 生物学上更易表达或更接近天然蛋白的变体更受偏好；\n",
    "   - 低分突变代表“激进”的氨基酸替换。\n",
    "2. **Levenshtein 距离（levenshtein_avg）**\n",
    "   - 计算每条候选序列与去年 Top10 最优蛋白序列的平均编辑距离；\n",
    "   - 距离越大代表突变越多、结构越新颖，体现 **多样性与跳跃性**；\n",
    "   - 鼓励模型探索更广泛序列空间，跳出局部最优。\n",
    "3. **突变频率惩罚项（freq_penalty）**\n",
    "   - 基于训练集中统计出的突变频率，使用 `-log(frequency)` 评估每个突变的“新颖性”；\n",
    "   - 越常见的突变惩罚越小，越罕见的突变惩罚越高；\n",
    "   - 防止模型只生成“习惯性安全突变”，增强创新性探索能力。\n",
    "\n",
    "上述三项指标通过 MinMaxScaler 归一化为 `[0, 1]` 区间，以便统一尺度进行加权融合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "626bc1f6-cfff-4d29-99d1-c0c497acf9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: biopython in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (1.85)\n",
      "Requirement already satisfied: numpy in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from biopython) (1.24.2)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutation_tag</th>\n",
       "      <th>blosum_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N135H</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F130L:I128K</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M153D:T203F</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T203I:K126E</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E222S</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mutation_tag  blosum_score\n",
       "0        N135H           1.0\n",
       "1  F130L:I128K          -3.0\n",
       "2  M153D:T203F          -5.0\n",
       "3  T203I:K126E           0.0\n",
       "4        E222S           0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install biopython\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "# 加载 BLOSUM62 矩阵\n",
    "blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
    "\n",
    "# 定义突变得分函数\n",
    "def compute_blosum_score(mutation_tag: str) -> float:\n",
    "    if not isinstance(mutation_tag, str) or not mutation_tag:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0\n",
    "    for mut in mutation_tag.split(\":\"):\n",
    "        if len(mut) < 3:\n",
    "            continue\n",
    "        wt, mut_aa = mut[0], mut[-1]\n",
    "        try:\n",
    "            score += blosum62[(wt, mut_aa)]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                score += blosum62[(mut_aa, wt)]\n",
    "            except KeyError:\n",
    "                score += -5  # 惩罚性得分\n",
    "    return score\n",
    "\n",
    "# 应用到数据集\n",
    "vae_df[\"blosum_score\"] = vae_df[\"mutation_tag\"].apply(compute_blosum_score)\n",
    "vae_df[[\"mutation_tag\", \"blosum_score\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2fe6771c-2dc7-4382-a49c-4979d62bca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: Levenshtein in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from Levenshtein) (3.13.0)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutated_sequence</th>\n",
       "      <th>levenshtein_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    mutated_sequence  levenshtein_avg\n",
       "0  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...              5.2\n",
       "1  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...              6.2\n",
       "2  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...              5.8\n",
       "3  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...              5.6\n",
       "4  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...              5.2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install Levenshtein\n",
    "import numpy as np\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# 提取 Top10 参考序列列表（已 strip 处理）\n",
    "top10_seqs = top10_df.iloc[:, 0].apply(lambda s: str(s).strip()).tolist()\n",
    "\n",
    "# 定义 Levenshtein 多样性得分函数\n",
    "def compute_avg_levenshtein(seq: str) -> float:\n",
    "    \"\"\"\n",
    "    计算一个突变序列与 Top10 序列的平均 Levenshtein 距离\n",
    "    \"\"\"\n",
    "    distances = [levenshtein_distance(seq, ref) for ref in top10_seqs]\n",
    "    return float(np.mean(distances))\n",
    "\n",
    "# 应用到全部候选序列\n",
    "vae_df[\"levenshtein_avg\"] = vae_df[\"mutated_sequence\"].apply(compute_avg_levenshtein)\n",
    "vae_df[[\"mutated_sequence\", \"levenshtein_avg\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd256a4a-53fd-486d-a8b9-21e173cb3049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('N120S', 0.004742030546455445),\n",
       " ('K213E', 0.004183557968954909),\n",
       " ('K112R', 0.004138680708262901),\n",
       " ('N163S', 0.004138680708262901),\n",
       " ('F7L', 0.004128707983664677),\n",
       " ('K78R', 0.003964158027793984),\n",
       " ('F129L', 0.003869417144110857),\n",
       " ('N143D', 0.0038145671588206255),\n",
       " ('V67A', 0.00377467626042773),\n",
       " ('K157R', 0.003769689898128618)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 载入训练集，统计突变频率\n",
    "train_df = pd.read_excel(\"processed_training_data.xlsx\",engine='openpyxl')\n",
    "\n",
    "# 提取所有突变记录（mutation_tag 列中的 L64E:S30F:... 拆分）\n",
    "from collections import Counter\n",
    "\n",
    "mutation_counter = Counter()\n",
    "\n",
    "for tag in train_df[\"aaMutations\"].dropna():\n",
    "    for mut in str(tag).split(\":\"):\n",
    "        if len(mut) >= 3:\n",
    "            mutation_counter[mut] += 1\n",
    "\n",
    "# 计算总突变次数\n",
    "total_mutations = sum(mutation_counter.values())\n",
    "\n",
    "# 构建频率表（突变 → 出现频率）\n",
    "mutation_freq = {mut: count / total_mutations for mut, count in mutation_counter.items()}\n",
    "\n",
    "# 展示部分频率统计结果\n",
    "sorted_freq = sorted(mutation_freq.items(), key=lambda x: -x[1])\n",
    "sorted_freq[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "defff73a-09e2-4a34-80a8-c02b99dd5997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutation_tag</th>\n",
       "      <th>freq_penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N135H</td>\n",
       "      <td>13.815511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F130L:I128K</td>\n",
       "      <td>13.815511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M153D:T203F</td>\n",
       "      <td>13.815511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T203I:K126E</td>\n",
       "      <td>13.815511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E222S</td>\n",
       "      <td>13.815511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mutation_tag  freq_penalty\n",
       "0        N135H     13.815511\n",
       "1  F130L:I128K     13.815511\n",
       "2  M153D:T203F     13.815511\n",
       "3  T203I:K126E     13.815511\n",
       "4        E222S     13.815511"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 计算突变频率惩罚分数：越罕见分数越低，常见突变得分更高（可惩罚）\n",
    "def compute_mutation_penalty(tag: str) -> float:\n",
    "    if not isinstance(tag, str) or not tag:\n",
    "        return 0.0\n",
    "\n",
    "    penalties = []\n",
    "    for mut in tag.split(\":\"):\n",
    "        freq = mutation_freq.get(mut, 1e-6)  # 如果未出现，设为极小频率\n",
    "        penalty = -np.log(freq)\n",
    "        penalties.append(penalty)\n",
    "\n",
    "    return float(np.mean(penalties)) if penalties else 0.0\n",
    "\n",
    "# 应用于数据集\n",
    "vae_df[\"freq_penalty\"] = vae_df[\"mutation_tag\"].apply(compute_mutation_penalty)\n",
    "vae_df[[\"mutation_tag\", \"freq_penalty\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefdf19360ad7b29",
   "metadata": {},
   "source": [
    "#### **5.3 综合得分计算与 Top5000 筛选**\n",
    "\n",
    "定义综合打分函数（Combined Score）：\n",
    "\n",
    "$\\text{Score}_{\\text{combined}} = 0.4 \\cdot \\text{blosum}_{\\text{norm}} + 0.4 \\cdot \\text{levenshtein}_{\\text{norm}} + 0.2 \\cdot (1 - \\text{freq\\_penalty}_{\\text{norm}})$\n",
    "\n",
    "解释如下：\n",
    "\n",
    "- `blosum_score_norm`：代表稳定性与可表达性；\n",
    "- `levenshtein_avg_norm`：代表多样性与创新性；\n",
    "- `1 - freq_penalty_norm`：代表偏好“自然偏好”的突变。\n",
    "\n",
    "加权比例如上（可调），代表对“保守性”和“多样性”两个维度的均衡考量。\n",
    "\n",
    "最终，按照 `combined_score` 从高到低排序，选出 Top5000 条高质量序列，进入下一阶段的结构预测与亮度打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "501e6416-691e-4dfd-a62f-e8c37bc5e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: seaborn in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from seaborn) (3.9.4)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from seaborn) (1.24.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from seaborn) (1.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.41.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 选取要归一化的列\n",
    "score_columns = [\"blosum_score\", \"levenshtein_avg\", \"freq_penalty\"]\n",
    "\n",
    "# 初始化并拟合 Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "vae_df[[col + \"_norm\" for col in score_columns]] = scaler.fit_transform(vae_df[score_columns])\n",
    "\n",
    "# 加权组合打分公式（可自定义权重）\n",
    "# 越高越好：blosum_score_norm、levenshtein_avg_norm\n",
    "# 越低越好：freq_penalty_norm → 取 1 - norm\n",
    "vae_df[\"combined_score\"] = (\n",
    "    0.4 * vae_df[\"blosum_score_norm\"] +\n",
    "    0.4 * vae_df[\"levenshtein_avg_norm\"] +\n",
    "    0.2 * (1 - vae_df[\"freq_penalty_norm\"])\n",
    ")\n",
    "\n",
    "# 按照综合得分排序并选出 Top 5000\n",
    "top_candidates_df = vae_df.sort_values(\"combined_score\", ascending=False).head(5000).reset_index(drop=True)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 查看综合得分分布\n",
    "sns.histplot(vae_df[\"combined_score\"], bins=50, kde=True)\n",
    "plt.title(\"Combined Score Distribution (Weighted Normalized)\")\n",
    "plt.xlabel(\"Combined Score\")\n",
    "plt.ylabel(\"Sequence Count\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faa6b282-5308-4008-a61d-44390e6551f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    mutated_sequence  \\\n",
      "0  MSKGEELFTGVVPILVELDGDVNGHKFSVAGEGEGDATYGKLTLKF...   \n",
      "1  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
      "2  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
      "3  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
      "4  MSKGEELFTGVVPILVELDGDVNGHKFSVKGEGEGDATYGKLTLKF...   \n",
      "\n",
      "                          mutation_tag  combined_score  \n",
      "0    V68L:E222Q:S30A:Y145H:I136L:K140E        0.992857  \n",
      "1   N149H:M153L:K214R:V68M:I136L:N121S        0.992857  \n",
      "2  Y145H:A206S:I128L:N135D:N121H:K140Q        0.985185  \n",
      "3  M153L:A206S:I128L:I136M:K131E:N135S        0.985185  \n",
      "4   N149D:M153I:S30K:Y145F:I128L:K131E        0.985185  \n"
     ]
    }
   ],
   "source": [
    "# 展示前5条 Top 序列\n",
    "print(top_candidates_df[[\"mutated_sequence\", \"mutation_tag\", \"combined_score\"]].head())\n",
    "top_candidates_df.to_excel(\"初筛Top5000序列.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59af9040bb6079",
   "metadata": {},
   "source": [
    "## 第六阶段：亮度预测与精筛（从5000 → 2000）\n",
    "\n",
    "在第五阶段完成 Top5000 多指标候选筛选后，本阶段目标是进一步使用训练好的亮度预测器模型（ResMLP），基于 ESM 表征向量对每条候选序列进行预测排序，选出亮度最优的 Top2000 作为最终提交或结构预测对象。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f84a157-8de3-40de-b03b-e41abcfb3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5000_df = pd.read_excel(\"初筛Top5000序列.xlsx\",engine = 'openpyxl')\n",
    "candidate_list = top5000_df[\"mutated_sequence\"].astype(str).tolist()\n",
    "mutation_list = top5000_df[\"mutation_tag\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e366d37ab0ac59d",
   "metadata": {},
   "source": [
    "### 6.1 候选序列 ESM 表征向量生成\n",
    "\n",
    "对 Top5000 序列执行批量推理，获得其 ESM 表征嵌入（N×1280）：\n",
    "\n",
    "```python\n",
    "candidate_embeddings_tensor = get_esm_embeddings(\n",
    "    candidate_list,\n",
    "    esm_model_pred,\n",
    "    alphabet_pred,\n",
    "    batch_converter_pred,\n",
    "    DEVICE_pred,\n",
    "    batch_size=prediction_batch_size\n",
    ")\n",
    "```\n",
    "\n",
    "注意：\n",
    "- 检查 `NaN` 情况，避免模型不收敛或出现非法向量；\n",
    "- 保存嵌入结果为 `top5000_candidate_embeddings.npy` 便于复用。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166376af-334f-4ecd-9ce2-a00f12af3025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尝试加载用于预测的 ESM 模型: esm2_t33_650M_UR50D\n",
      "用于预测的 ESM 模型 'esm2_t33_650M_UR50D' 已加载到 cuda，耗时 9.58 秒。\n",
      "\n",
      "使用预测模型为候选序列生成 ESM 嵌入...\n",
      "检测到 GPU，使用 GPU 批次大小: 16\n",
      "正在为 5000 个序列生成嵌入，共 313 个批次（批次大小: 16，设备: cuda）...\n",
      "  已处理批次 10/313... (耗时: 9.21 秒)\n",
      "  已处理批次 20/313... (耗时: 18.08 秒)\n",
      "  已处理批次 30/313... (耗时: 26.94 秒)\n",
      "  已处理批次 40/313... (耗时: 35.82 秒)\n",
      "  已处理批次 50/313... (耗时: 44.69 秒)\n",
      "  已处理批次 60/313... (耗时: 53.55 秒)\n",
      "  已处理批次 70/313... (耗时: 62.42 秒)\n",
      "  已处理批次 80/313... (耗时: 71.27 秒)\n",
      "  已处理批次 90/313... (耗时: 80.15 秒)\n",
      "  已处理批次 100/313... (耗时: 89.02 秒)\n",
      "  已处理批次 110/313... (耗时: 97.90 秒)\n",
      "  已处理批次 120/313... (耗时: 106.77 秒)\n",
      "  已处理批次 130/313... (耗时: 115.64 秒)\n",
      "  已处理批次 140/313... (耗时: 124.50 秒)\n",
      "  已处理批次 150/313... (耗时: 133.37 秒)\n",
      "  已处理批次 160/313... (耗时: 142.26 秒)\n",
      "  已处理批次 170/313... (耗时: 151.14 秒)\n",
      "  已处理批次 180/313... (耗时: 160.02 秒)\n",
      "  已处理批次 190/313... (耗时: 168.89 秒)\n",
      "  已处理批次 200/313... (耗时: 177.75 秒)\n",
      "  已处理批次 210/313... (耗时: 186.62 秒)\n",
      "  已处理批次 220/313... (耗时: 195.49 秒)\n",
      "  已处理批次 230/313... (耗时: 204.38 秒)\n",
      "  已处理批次 240/313... (耗时: 213.25 秒)\n",
      "  已处理批次 250/313... (耗时: 222.12 秒)\n",
      "  已处理批次 260/313... (耗时: 230.98 秒)\n",
      "  已处理批次 270/313... (耗时: 239.86 秒)\n",
      "  已处理批次 280/313... (耗时: 248.73 秒)\n",
      "  已处理批次 290/313... (耗时: 257.59 秒)\n",
      "  已处理批次 300/313... (耗时: 266.46 秒)\n",
      "  已处理批次 310/313... (耗时: 275.34 秒)\n",
      "  已处理批次 313/313... (耗时: 277.57 秒)\n",
      "嵌入生成在 277.57 秒内完成。\n",
      "平均每个序列耗时: 0.0555 秒\n",
      "候选序列嵌入的形状: (5000, 1280)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import esm # 假设 esm 库已导入\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 假设 model, candidate_list, mutation_list, exclusion_sequences,\n",
    "# TOP_N_SELECT, BATCH_SIZE, get_esm_embeddings (来自上一步) 已经定义好\n",
    "# 并且 avGFP_WT_sequence, candidate_position_pool_0based, MAX_MUTATIONS, N_CANDIDATES_TO_GENERATE 也已定义\n",
    "\n",
    "# --- 修正：定义用于预测的 ESM 模型名称 ---\n",
    "# !!! 关键：这里必须使用与训练 rf_model 时相同的 ESM 模型 !!!\n",
    "# 根据之前的日志，rf_model 是用 640 维嵌入训练的 (来自 esm2_t30_150M_UR50D)\n",
    "PREDICTION_ESM_MODEL_NAME = \"esm2_t33_650M_UR50D\" # <--- 确认这个模型与训练时一致\n",
    "\n",
    "print(f\"尝试加载用于预测的 ESM 模型: {PREDICTION_ESM_MODEL_NAME}\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # 加载指定用于预测的 ESM 模型和字母表\n",
    "    esm_model_pred, alphabet_pred = esm.pretrained.load_model_and_alphabet(PREDICTION_ESM_MODEL_NAME)\n",
    "    batch_converter_pred = alphabet_pred.get_batch_converter()\n",
    "    # 重新确定设备 (优先使用 GPU)\n",
    "    DEVICE_pred = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    esm_model_pred.to(DEVICE_pred)\n",
    "    esm_model_pred.eval() # 设置模型为评估模式 (关闭 dropout 等)\n",
    "    print(f\"用于预测的 ESM 模型 '{PREDICTION_ESM_MODEL_NAME}' 已加载到 {DEVICE_pred}，耗时 {time.time() - start_time:.2f} 秒。\")\n",
    "except Exception as e:\n",
    "    print(f\"加载 ESM 模型 {PREDICTION_ESM_MODEL_NAME} 时出错: {e}\")\n",
    "    print(\"请确保 'fair-esm' 已安装，模型名称正确，并且有足够的内存。\")\n",
    "    # 根据需要处理错误，例如退出\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 6.1 为候选序列生成 ESM 嵌入 ---\n",
    "print(\"\\n使用预测模型为候选序列生成 ESM 嵌入...\")\n",
    "candidate_embeddings_np = np.array([]) # 初始化为空 numpy 数组\n",
    "\n",
    "if candidate_list:\n",
    "    # 确定批次大小 (如果使用GPU，可以尝试更大的批次)\n",
    "    prediction_batch_size = 8 # 默认使用 CPU 批次大小 (来自之前的定义)\n",
    "    if DEVICE_pred == torch.device(\"cuda\"):\n",
    "        # 如果是GPU，可以尝试更大的批次，例如 8, 16 或 32，取决于GPU内存和模型大小\n",
    "        # 对于较大的模型如 150M，可能需要较小的批次大小\n",
    "        prediction_batch_size = 16 # 示例值 (根据你的 GPU 内存调整，与训练时用的GPU_BATCH_SIZE可以不同)\n",
    "        print(f\"检测到 GPU，使用 GPU 批次大小: {prediction_batch_size}\")\n",
    "    else:\n",
    "        print(f\"使用 CPU 批次大小: {prediction_batch_size}\")\n",
    "\n",
    "    # *** 修复点：使用正确的函数名 'get_esm_embeddings' ***\n",
    "    # 传递用于预测的模型、字母表、转换器和设备\n",
    "    candidate_embeddings_tensor = get_esm_embeddings( # <-- 使用正确的函数名\n",
    "        candidate_list,\n",
    "        esm_model_pred,         # 使用预测模型\n",
    "        alphabet_pred,        # 使用预测模型的字母表\n",
    "        batch_converter_pred, # 使用预测模型的转换器\n",
    "        DEVICE_pred,          # 使用预测模型所在的设备 (可能是 cuda 或 cpu)\n",
    "        batch_size=prediction_batch_size # 使用调整后的批次大小\n",
    "    )\n",
    "\n",
    "    # 将嵌入结果转换为 NumPy 数组以用于 scikit-learn 模型\n",
    "    # .cpu() 确保数据在 CPU 上，然后转换为 numpy\n",
    "    candidate_embeddings_np = candidate_embeddings_tensor.cpu().numpy()\n",
    "\n",
    "    print(f\"候选序列嵌入的形状: {candidate_embeddings_np.shape}\") # 检查维度是否正确 (应为 N x 640)\n",
    "\n",
    "    # 检查是否有 NaN (如果嵌入过程中出错)\n",
    "    if np.isnan(candidate_embeddings_np).any():\n",
    "        print(\"警告：在候选序列嵌入中发现 NaN 值。将移除相应的候选序列。\")\n",
    "        nan_mask = np.isnan(candidate_embeddings_np).any(axis=1)\n",
    "        # 需要同时过滤 candidate_list, mutation_list 和 embeddings\n",
    "        # 使用列表推导式进行过滤\n",
    "        original_count = len(candidate_list)\n",
    "        candidate_list = [seq for i, seq in enumerate(candidate_list) if not nan_mask[i]]\n",
    "        mutation_list = [mut for i, mut in enumerate(mutation_list) if not nan_mask[i]]\n",
    "        candidate_embeddings_np = candidate_embeddings_np[~nan_mask]\n",
    "        print(f\"因 NaN 嵌入移除了 {original_count - len(candidate_list)} 个候选序列。\")\n",
    "        print(f\"剩余候选序列数量: {len(candidate_list)}\")\n",
    "        print(f\"过滤后的候选序列嵌入形状: {candidate_embeddings_np.shape}\")\n",
    "\n",
    "else:\n",
    "    # candidate_embeddings_np 已经初始化为空数组\n",
    "    print(\"上一步没有生成候选序列，跳过预测。\")\n",
    "np.save(\"top5000_candidate_embeddings.npy\", candidate_embeddings_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51b558622c9f11",
   "metadata": {},
   "source": [
    "### 6.2 使用 ResMLP 模型进行亮度预测\n",
    "\n",
    "加载已训练的亮度预测模型：\n",
    "\n",
    "- 输入维度由 `candidate_embeddings_np.shape[1]` 自动确认（应为 1280）；\n",
    "- 使用 `.eval()` 模式关闭 Dropout 以保证稳定性；\n",
    "- 使用 `.to(device)` 将模型迁移到相应计算设备。\n",
    "\n",
    "在无梯度模式下进行亮度预测：\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    predicted_brightness = model(tensor_input).cpu().numpy().tolist()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48cd8fab-fada-4c0c-89e4-6a5b4e33c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在为候选序列预测亮度（使用 ResMLP）...\n",
      "预测完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 6.2 预测亮度 ---\n",
    "candidate_embeddings_np = np.load(\"top5000_candidate_embeddings.npy\")\n",
    "\n",
    "# 加载模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BrightnessRegressor(input_dim=candidate_embeddings_np.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load(\"best_brightness_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "predicted_brightness = []\n",
    "if candidate_embeddings_np.shape[0] > 0 and len(candidate_list) == candidate_embeddings_np.shape[0]:\n",
    "    print(\"\\n正在为候选序列预测亮度（使用 ResMLP）...\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            tensor_input = torch.tensor(candidate_embeddings_np, dtype=torch.float32).to(device)\n",
    "            predicted_brightness = model(tensor_input).cpu().numpy().tolist()\n",
    "        print(\"预测完成。\")\n",
    "    except Exception as e:\n",
    "        print(f\"ResMLP 模型预测期间发生错误: {e}\")\n",
    "        predicted_brightness = []\n",
    "else:\n",
    "    print(\"候选列表或嵌入向量为空，或数量不匹配，无法进行预测。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02976c8d03ddd6",
   "metadata": {},
   "source": [
    "### 6.3 Top2000 精筛与保存\n",
    "\n",
    "根据 `PredictedBrightness` 对所有候选进行降序排序：\n",
    "\n",
    "- 过滤掉在 `exclusion_list` 中的非法或重复序列；\n",
    "- 插入 `Sequence ID` 标识；\n",
    "- 提取最终字段 `[\"Sequence ID\", \"Mutations\", \"Sequence\", \"PredictedBrightness\"]`；\n",
    "- 保存为标准提交格式 `Top2000_Brightness_Selected.csv`。\n",
    "\n",
    "```python\n",
    "final_candidates_formatted.to_csv(\"Top2000_Brightness_Selected.csv\", index=False)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a46a5e3f-0c27-4fc3-a300-ea04913ff86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "\n",
      "根据排除列表 (739 个序列) 进行过滤...\n",
      "候选列表中的序列均不在排除列表中。\n",
      "\n",
      "预测出的 Top 2000 个候选序列 (已排除):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>Mutations</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>PredictedBrightness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>M153I:A206W:T203I:I136L:K131E:N121Y</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.162509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>Candidate_2</td>\n",
       "      <td>N149S:S30A:Y145L:A206I:T203L:K140Q</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVAGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.148169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>Candidate_3</td>\n",
       "      <td>K214Q:V68L:V163Q:F130L:K131D:N121S</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.147022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>Candidate_4</td>\n",
       "      <td>N149D:M153S:T203L:K131E:N135E:N121A</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.139331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>Candidate_5</td>\n",
       "      <td>M153I:K214E:T203L:K131D:N121S:K140Y</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.136574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>Candidate_1996</td>\n",
       "      <td>N149H:N212S:A206M:T203K:I136L:N135S</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.092048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>Candidate_1997</td>\n",
       "      <td>K214E:N212D:A206M:K131Q:N135A</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.092048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Candidate_1998</td>\n",
       "      <td>N212D:V68S:I136T:N135H:N121S:K126Q</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.092044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Candidate_1999</td>\n",
       "      <td>V68L:S30N:I128L:K131Q:N135D</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVNGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.092038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>Candidate_2000</td>\n",
       "      <td>S30A:Y145F:A206E:N121H</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVAGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.092032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sequence ID                            Mutations  \\\n",
       "4293     Candidate_1  M153I:A206W:T203I:I136L:K131E:N121Y   \n",
       "1617     Candidate_2   N149S:S30A:Y145L:A206I:T203L:K140Q   \n",
       "1330     Candidate_3   K214Q:V68L:V163Q:F130L:K131D:N121S   \n",
       "3296     Candidate_4  N149D:M153S:T203L:K131E:N135E:N121A   \n",
       "2297     Candidate_5  M153I:K214E:T203L:K131D:N121S:K140Y   \n",
       "...              ...                                  ...   \n",
       "436   Candidate_1996  N149H:N212S:A206M:T203K:I136L:N135S   \n",
       "4169  Candidate_1997        K214E:N212D:A206M:K131Q:N135A   \n",
       "566   Candidate_1998   N212D:V68S:I136T:N135H:N121S:K126Q   \n",
       "535   Candidate_1999          V68L:S30N:I128L:K131Q:N135D   \n",
       "4855  Candidate_2000               S30A:Y145F:A206E:N121H   \n",
       "\n",
       "                                               Sequence  PredictedBrightness  \n",
       "4293  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.162509  \n",
       "1617  MSKGEELFTGVVPILVELDGDVNGHKFSVAGEGEGDATYGKLTLKF...             3.148169  \n",
       "1330  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.147022  \n",
       "3296  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.139331  \n",
       "2297  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.136574  \n",
       "...                                                 ...                  ...  \n",
       "436   MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.092048  \n",
       "4169  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.092048  \n",
       "566   MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.092044  \n",
       "535   MSKGEELFTGVVPILVELDGDVNGHKFSVNGEGEGDATYGKLTLKF...             3.092038  \n",
       "4855  MSKGEELFTGVVPILVELDGDVNGHKFSVAGEGEGDATYGKLTLKF...             3.092032  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 Top2000 候选序列到 Top2000_Brightness_Selected.csv\n",
      "\n",
      "清理预测模型内存...\n",
      "清空CUDA缓存...\n",
      "预测模型清理完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 6.3 组合结果并筛选 ---\n",
    "Top_N_SELECT=2000\n",
    "\n",
    "final_candidates_formatted = pd.DataFrame() # 初始化为空 DataFrame\n",
    "\n",
    "# 只有在成功生成预测并且数量与候选列表匹配时才继续\n",
    "if len(candidate_list) > 0 and len(predicted_brightness) > 0 and len(candidate_list) == len(predicted_brightness):\n",
    "    # 创建包含序列、突变和预测值的 DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Sequence': candidate_list,\n",
    "        'Mutations': mutation_list, # 确保 mutation_list 与 candidate_list 保持同步\n",
    "        'PredictedBrightness': predicted_brightness\n",
    "    })\n",
    "    print(len(results_df))\n",
    "    # 过滤掉排除列表中的序列\n",
    "    print(f\"\\n根据排除列表 ({len(exclusion_sequences)} 个序列) 进行过滤...\")\n",
    "    initial_candidate_count = len(results_df)\n",
    "    # 确保比较的是字符串类型\n",
    "    results_df = results_df[~results_df['Sequence'].astype(str).isin(exclusion_sequences)]\n",
    "    removed_count = initial_candidate_count - len(results_df)\n",
    "    if removed_count > 0:\n",
    "        print(f\"移除了 {removed_count} 个在排除列表中的序列。\")\n",
    "    else:\n",
    "        print(\"候选列表中的序列均不在排除列表中。\")\n",
    "\n",
    "    # 按预测亮度降序排序\n",
    "    results_df = results_df.sort_values(by='PredictedBrightness', ascending=False)\n",
    "\n",
    "    # 选择 Top N 个结果\n",
    "    final_candidates = results_df.head(2000).copy() # 使用 .copy() 避免 SettingWithCopyWarning\n",
    "\n",
    "    print(f\"\\n预测出的 Top {min(2000, len(final_candidates))} 个候选序列 (已排除):\") # 显示实际选出的数量\n",
    "\n",
    "    if not final_candidates.empty:\n",
    "        # 为了更清晰地展示，可以添加一个 ID 列\n",
    "        final_candidates.insert(0, 'Sequence ID', [f'Candidate_{i+1}' for i in range(len(final_candidates))])\n",
    "        # 调整列顺序以符合提交格式要求\n",
    "        final_candidates_formatted = final_candidates[['Sequence ID', 'Mutations', 'Sequence', 'PredictedBrightness']]\n",
    "        # 使用 display 或 print 显示 DataFrame\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(final_candidates_formatted) # 在 Jupyter 环境中友好显示\n",
    "        except ImportError:\n",
    "            print(final_candidates_formatted.to_string()) # 在非 IPython 环境中打印完整 DataFrame\n",
    "    else:\n",
    "        print(\"经过过滤和筛选后，没有剩余的候选序列。\")\n",
    "\n",
    "        if not candidate_list:\n",
    "            print(\"\\n没有生成候选序列或候选序列在预测前已被过滤掉。\")\n",
    "        elif not predicted_brightness:\n",
    "            print(\"\\n预测步骤失败或没有产生结果。请检查之前的错误信息。\")\n",
    "        else: # candidate_list 和 predicted_brightness 长度不匹配\n",
    "            print(\"\\n错误：最终候选序列数量与预测结果数量不匹配。无法继续处理。\")\n",
    "final_candidates_formatted.to_csv(\"Top2000_Brightness_Selected.csv\", index=False)\n",
    "print(\"已保存 Top2000 候选序列到 Top2000_Brightness_Selected.csv\")\n",
    "\n",
    "# --- 清理预测模型占用的内存 ---\n",
    "print(\"\\n清理预测模型内存...\")\n",
    "\n",
    "for var_name in [\"esm_model_pred\", \"alphabet_pred\", \"batch_converter_pred\"]:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "\n",
    "if 'candidate_embeddings_tensor' in locals():\n",
    "    del candidate_embeddings_tensor\n",
    "if 'candidate_embeddings_np' in locals():\n",
    "    del candidate_embeddings_np\n",
    "if 'DEVICE_pred' in globals() and DEVICE_pred == torch.device(\"cuda\"):\n",
    "    print(\"清空CUDA缓存...\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"预测模型清理完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd0341ad-bf47-4a87-993e-2c12eca6310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在为候选序列预测亮度（使用 ResMLP_Fine_Tuning）...\n",
      "预测完成。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "# --- 预测亮度 ---\n",
    "candidate_embeddings_np = np.load(\"top5000_candidate_embeddings.npy\")\n",
    "\n",
    "# 加载模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BrightnessRegressor(input_dim=candidate_embeddings_np.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load(\"brightness_model_finetuned_on_high.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "predicted_brightness = []\n",
    "if candidate_embeddings_np.shape[0] > 0 and len(candidate_list) == candidate_embeddings_np.shape[0]:\n",
    "    print(\"\\n正在为候选序列预测亮度（使用 ResMLP_Fine_Tuning）...\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            tensor_input = torch.tensor(candidate_embeddings_np, dtype=torch.float32).to(device)\n",
    "            predicted_brightness = model(tensor_input).cpu().numpy().tolist()\n",
    "        print(\"预测完成。\")\n",
    "    except Exception as e:\n",
    "        print(f\"ResMLP 模型预测期间发生错误: {e}\")\n",
    "        predicted_brightness = []\n",
    "else:\n",
    "    print(\"候选列表或嵌入向量为空，或数量不匹配，无法进行预测。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64132195-dfb4-4ee1-a39c-e40816e1106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "\n",
      "根据排除列表 (739 个序列) 进行过滤...\n",
      "候选列表中的序列均不在排除列表中。\n",
      "\n",
      "预测出的 Top 2000 个候选序列 (已排除):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>Mutations</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>PredictedBrightness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>N149H:E222K:A206N:F130T:K140I:K126E</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>4.663651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4702</th>\n",
       "      <td>Candidate_2</td>\n",
       "      <td>N149H:N212E:L64F:F130T:K131T:N135K</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>4.569467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3896</th>\n",
       "      <td>Candidate_3</td>\n",
       "      <td>N149H:M153I:E222K:T203G:K126Q</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>4.421463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>Candidate_4</td>\n",
       "      <td>N149D:M153V:K214Q:V68E:Y145F</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.907406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>Candidate_5</td>\n",
       "      <td>N149H:K214E:A206E:K131T:K126Q</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.802310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>Candidate_1996</td>\n",
       "      <td>M153I:A206S:F130W:I128T:N135S</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.328672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>Candidate_1997</td>\n",
       "      <td>N149S:M153V:V68E:L64K:S30K:A206E</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVKGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.328447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>Candidate_1998</td>\n",
       "      <td>L64E:S30N:A206E:I128T:K131E:K126Q</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVNGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.328374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4853</th>\n",
       "      <td>Candidate_1999</td>\n",
       "      <td>N149S:K214Q:F130W:K140Q</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.328326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>Candidate_2000</td>\n",
       "      <td>N149S:T203L:V163L:I128M:K126R</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.328313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sequence ID                            Mutations  \\\n",
       "3406     Candidate_1  N149H:E222K:A206N:F130T:K140I:K126E   \n",
       "4702     Candidate_2   N149H:N212E:L64F:F130T:K131T:N135K   \n",
       "3896     Candidate_3        N149H:M153I:E222K:T203G:K126Q   \n",
       "1289     Candidate_4         N149D:M153V:K214Q:V68E:Y145F   \n",
       "2970     Candidate_5        N149H:K214E:A206E:K131T:K126Q   \n",
       "...              ...                                  ...   \n",
       "1427  Candidate_1996        M153I:A206S:F130W:I128T:N135S   \n",
       "4724  Candidate_1997     N149S:M153V:V68E:L64K:S30K:A206E   \n",
       "3258  Candidate_1998    L64E:S30N:A206E:I128T:K131E:K126Q   \n",
       "4853  Candidate_1999              N149S:K214Q:F130W:K140Q   \n",
       "2751  Candidate_2000        N149S:T203L:V163L:I128M:K126R   \n",
       "\n",
       "                                               Sequence  PredictedBrightness  \n",
       "3406  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             4.663651  \n",
       "4702  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             4.569467  \n",
       "3896  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             4.421463  \n",
       "1289  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.907406  \n",
       "2970  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.802310  \n",
       "...                                                 ...                  ...  \n",
       "1427  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.328672  \n",
       "4724  MSKGEELFTGVVPILVELDGDVNGHKFSVKGEGEGDATYGKLTLKF...             3.328447  \n",
       "3258  MSKGEELFTGVVPILVELDGDVNGHKFSVNGEGEGDATYGKLTLKF...             3.328374  \n",
       "4853  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.328326  \n",
       "2751  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.328313  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 Top2000 候选序列到 Top2000_Brightness_Selected_FineTune.csv\n",
      "\n",
      "清理预测模型内存...\n",
      "清空CUDA缓存...\n",
      "预测模型清理完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 6.3 组合结果并筛选 ---\n",
    "Top_N_SELECT=2000\n",
    "\n",
    "final_candidates_formatted = pd.DataFrame() # 初始化为空 DataFrame\n",
    "\n",
    "# 只有在成功生成预测并且数量与候选列表匹配时才继续\n",
    "if len(candidate_list) > 0 and len(predicted_brightness) > 0 and len(candidate_list) == len(predicted_brightness):\n",
    "    # 创建包含序列、突变和预测值的 DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Sequence': candidate_list,\n",
    "        'Mutations': mutation_list, # 确保 mutation_list 与 candidate_list 保持同步\n",
    "        'PredictedBrightness': predicted_brightness\n",
    "    })\n",
    "    print(len(results_df))\n",
    "    # 过滤掉排除列表中的序列\n",
    "    print(f\"\\n根据排除列表 ({len(exclusion_sequences)} 个序列) 进行过滤...\")\n",
    "    initial_candidate_count = len(results_df)\n",
    "    # 确保比较的是字符串类型\n",
    "    results_df = results_df[~results_df['Sequence'].astype(str).isin(exclusion_sequences)]\n",
    "    removed_count = initial_candidate_count - len(results_df)\n",
    "    if removed_count > 0:\n",
    "        print(f\"移除了 {removed_count} 个在排除列表中的序列。\")\n",
    "    else:\n",
    "        print(\"候选列表中的序列均不在排除列表中。\")\n",
    "\n",
    "    # 按预测亮度降序排序\n",
    "    results_df = results_df.sort_values(by='PredictedBrightness', ascending=False)\n",
    "\n",
    "    # 选择 Top N 个结果\n",
    "    final_candidates = results_df.head(2000).copy() # 使用 .copy() 避免 SettingWithCopyWarning\n",
    "\n",
    "    print(f\"\\n预测出的 Top {min(2000, len(final_candidates))} 个候选序列 (已排除):\") # 显示实际选出的数量\n",
    "\n",
    "    if not final_candidates.empty:\n",
    "        # 为了更清晰地展示，可以添加一个 ID 列\n",
    "        final_candidates.insert(0, 'Sequence ID', [f'Candidate_{i+1}' for i in range(len(final_candidates))])\n",
    "        # 调整列顺序以符合提交格式要求\n",
    "        final_candidates_formatted = final_candidates[['Sequence ID', 'Mutations', 'Sequence', 'PredictedBrightness']]\n",
    "        # 使用 display 或 print 显示 DataFrame\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(final_candidates_formatted) # 在 Jupyter 环境中友好显示\n",
    "        except ImportError:\n",
    "            print(final_candidates_formatted.to_string()) # 在非 IPython 环境中打印完整 DataFrame\n",
    "    else:\n",
    "        print(\"经过过滤和筛选后，没有剩余的候选序列。\")\n",
    "\n",
    "        if not candidate_list:\n",
    "            print(\"\\n没有生成候选序列或候选序列在预测前已被过滤掉。\")\n",
    "        elif not predicted_brightness:\n",
    "            print(\"\\n预测步骤失败或没有产生结果。请检查之前的错误信息。\")\n",
    "        else: # candidate_list 和 predicted_brightness 长度不匹配\n",
    "            print(\"\\n错误：最终候选序列数量与预测结果数量不匹配。无法继续处理。\")\n",
    "final_candidates_formatted.to_csv(\"Top2000_Brightness_Selected_FineTune.csv\", index=False)\n",
    "print(\"已保存 Top2000 候选序列到 Top2000_Brightness_Selected_FineTune.csv\")\n",
    "\n",
    "# --- 清理预测模型占用的内存 ---\n",
    "print(\"\\n清理预测模型内存...\")\n",
    "\n",
    "for var_name in [\"esm_model_pred\", \"alphabet_pred\", \"batch_converter_pred\"]:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "\n",
    "if 'candidate_embeddings_tensor' in locals():\n",
    "    del candidate_embeddings_tensor\n",
    "if 'candidate_embeddings_np' in locals():\n",
    "    del candidate_embeddings_np\n",
    "if 'DEVICE_pred' in globals() and DEVICE_pred == torch.device(\"cuda\"):\n",
    "    print(\"清空CUDA缓存...\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"预测模型清理完成。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776a8f4-625c-4192-b42b-889c3dedff91",
   "metadata": {},
   "source": [
    "---\n",
    "## 第七阶段：PLM融合打分与结构一致性精筛（2000 → 300 ∪ 300）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc537fe37b3be667",
   "metadata": {},
   "source": [
    "### 7.1 蛋白语言模型融合打分（ESM-1b + Tranception）\n",
    "\n",
    "本子阶段通过两个训练于不同语料和策略的蛋白语言模型对候选序列进行打分：\n",
    "\n",
    "- **ESM-1b Flat**：基于 Facebook 的预训练语言模型，使用 `log-likelihood` 评估序列天然性；\n",
    "- **Tranception**：基于 Transformer + 自动回归建模策略的蛋白语言模型，适用于泛化打分。\n",
    "#### 流程概述：\n",
    "\n",
    "1. **加载 ESM-1b 模型**\n",
    "   - 使用 HuggingFace 接口加载 `esm1b_flat` 本地模型；\n",
    "   - 对 Top2000 每条序列计算其 token-level `log-likelihood` 和 `perplexity`。\n",
    "\n",
    "2. **加载 Tranception 模型**\n",
    "   - 使用 `Tranception_Medium` 版本；\n",
    "   - 对每条序列计算其 log-likelihood 和 perplexity，输出保存在 `tranception_scores.csv`。\n",
    "\n",
    "3. **三方打分融合**\n",
    "   - 将亮度预测结果（`PredictedBrightness`）与上述两种语言模型的打分合并；\n",
    "   - 使用 `StandardScaler` 对三项打分结果（亮度、ESM、Tranception）进行标准化；\n",
    "   - 构建融合打分：\n",
    "     ```math\n",
    "     Final\\_Score = mean(Brightness\\_z, ESM\\_LL\\_z, Tranception\\_LL\\_z)\n",
    "     ```\n",
    "     默认采用等权重。\n",
    "\n",
    "4. **选出前300条高分序列**\n",
    "   - 根据 `Final_Score` 降序排列，选择前300条序列作为语言模型层面可信候选；\n",
    "   - 保存为 `top300_fused_scored.csv`。\n",
    "\n",
    "#### 亮点：\n",
    "- 融合亮度预测 + 两种语言建模视角，确保筛选出的序列既具潜在功能又具自然合理性；\n",
    "- 利用 ESM 提供的 token 级别 likelihood 和 Tranception 提供的 full-sequence log-likelihood 互补增强判断；\n",
    "- 融合策略灵活，可根据评估权重自定义扩展。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b72c9-cf8a-497f-85bd-acaea5ab1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"esm1b.zip\"  # 如果你命名成别的，记得改这里\n",
    "extract_dir = \"esm1b\"\n",
    "\n",
    "# 解压模型压缩包\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\" 模型已解压至: {extract_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "389bd11e-da74-451e-aa39-6b57553657af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 模型加载成功\n"
     ]
    }
   ],
   "source": [
    "from transformers import EsmTokenizer, EsmForMaskedLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = \"esm1b_flat\"\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_path)\n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\" 模型加载成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f50be8-6b6e-4ff9-9e9e-1c96c14c692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring Sequences: 100%|██████████| 2000/2000 [02:21<00:00, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果已保存为 esm1b_scores.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import EsmForMaskedLM, EsmTokenizer\n",
    "\n",
    "# ----------------- 参数设置 -----------------\n",
    "CSV_PATH = \"Top2000_Brightness_Selected_FineTune.csv\"  # 输入CSV路径\n",
    "OUTPUT_PATH = \"esm1b_scores.csv\"                        # 输出打分文件\n",
    "BATCH_SIZE = 1                                          # HuggingFace不支持变长批次，建议设置为1\n",
    "\n",
    "# ----------------- 初始化模型与设备 -----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"esm1b_flat\"  # 改为你解压的真实路径\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_path)\n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ----------------- 读取序列 -----------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert \"Sequence\" in df.columns, \"缺少 'Sequence' 列\"\n",
    "sequences = df[\"Sequence\"].tolist()\n",
    "\n",
    "# ----------------- 打分函数 -----------------\n",
    "def compute_ll_ppl(sequence: str):\n",
    "    \"\"\"计算单个序列的 log-likelihood 和 perplexity\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            log_likelihood = log_probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            mask = attention_mask.bool()\n",
    "            ll_sum = log_likelihood[mask].sum().item()\n",
    "            token_count = mask.sum().item()\n",
    "            avg_log_likelihood = ll_sum / token_count\n",
    "            perplexity = torch.exp(torch.tensor(-avg_log_likelihood)).item()\n",
    "\n",
    "        return avg_log_likelihood, perplexity\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] 序列处理失败: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ----------------- 批量处理 -----------------\n",
    "results = []\n",
    "for seq in tqdm(sequences, desc=\"Scoring Sequences\"):\n",
    "    ll, ppl = compute_ll_ppl(seq)\n",
    "    results.append({\n",
    "        \"Sequence\": seq,\n",
    "        \"LogLikelihood\": ll,\n",
    "        \"Perplexity\": ppl\n",
    "    })\n",
    "\n",
    "# ----------------- 保存结果 -----------------\n",
    "out_df = pd.DataFrame(results)\n",
    "out_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"结果已保存为 {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb9ff3c-d5b3-434b-bd4d-c99322e55217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 解压路径\n",
    "base_dir = \"/home/ma-user/work\"\n",
    "\n",
    "# === 解压模型 checkpoint ===\n",
    "ckpt_zip_path = os.path.join(base_dir, \"Tranception_Medium_checkpoint.zip\")\n",
    "ckpt_extract_dir = os.path.join(base_dir, \"tranception_medium\")\n",
    "\n",
    "with zipfile.ZipFile(ckpt_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(ckpt_extract_dir)\n",
    "\n",
    "print(\"解压 Tranception_Medium_checkpoint.zip 完成\")\n",
    "\n",
    "# === 解压代码文件 ===\n",
    "code_zip_path = os.path.join(base_dir, \"Tranception-main.zip\")\n",
    "\n",
    "with zipfile.ZipFile(code_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(base_dir)\n",
    "\n",
    "# 移动子目录 tranception-main/tranception → ./tranception\n",
    "import shutil\n",
    "\n",
    "src = os.path.join(base_dir, \"Tranception-main\", \"tranception\")\n",
    "dst = os.path.join(base_dir, \"tranception\")\n",
    "\n",
    "if not os.path.exists(dst):\n",
    "    shutil.move(src, dst)\n",
    "    print(\"已将 tranception 移动到工作目录\")\n",
    "else:\n",
    "    print(\"tranception 目录已存在，跳过移动\")\n",
    "print(\"所有文件解压完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a99ccb-fff3-42ac-982a-5b8a7fb37380",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a432639-d0b7-4fdd-866c-ad1ae24b094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages/Bio/Application/__init__.py:39: BiopythonDeprecationWarning: The Bio.Application modules and modules relying on it have been deprecated.\n",
      "\n",
      "Due to the on going maintenance burden of keeping command line application\n",
      "wrappers up to date, we have decided to deprecate and eventually remove these\n",
      "modules.\n",
      "\n",
      "We instead now recommend building your command line and invoking it directly\n",
      "with the subprocess module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tranception model from Tranception_Medium ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tranception.inference_utils import load_model_and_alphabet, score_sequence\n",
    "\n",
    "# === 配置路径 ===\n",
    "MODEL_DIR = \"Tranception_Medium\"\n",
    "CSV_PATH = \"Top2000_Brightness_Selected_FineTune.csv\"\n",
    "OUTPUT_PATH = \"tranception_scores.csv\"\n",
    "\n",
    "# === 加载模型 ===\n",
    "print(f\"Loading Tranception model from {MODEL_DIR} ...\")\n",
    "model, alphabet = load_model_and_alphabet(MODEL_DIR,\n",
    "                                          model_name=\"Tranception_Medium\",\n",
    "                                          use_gpu=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ca780f-d8d1-43e5-b13a-fcd1ec607647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood (avg): -4.394589900970459\n",
      "Perplexity: 81.01140125246286\n"
     ]
    }
   ],
   "source": [
    "# === 加载数据 ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert \"Sequence\" in df.columns, \"CSV文件中缺少'Sequence'列\"\n",
    "sequences = df[\"Sequence\"].tolist()\n",
    "ids = df[\"Sequence ID\"].tolist() if \"Sequence ID\" in df.columns else [f\"Seq_{i}\" for i in range(len(sequences))]\n",
    "test_seq = df[\"Sequence\"].iloc[1]\n",
    "_ = score_sequence(test_seq, model, alphabet)\n",
    "test_seq = df[\"Sequence\"].iloc[1]\n",
    "log_likelihood = score_sequence(test_seq, model, alphabet)\n",
    "log_likelihood = score_sequence(test_seq, model, alphabet)\n",
    "log_avg_prob = log_likelihood[0]\n",
    "print(\"Log-likelihood (avg):\", log_avg_prob)\n",
    "print(\"Perplexity:\", math.exp(-log_avg_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b046c7-e507-4894-93bd-f70086b79705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring sequences using Tranception...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring: 100%|██████████| 2000/2000 [01:49<00:00, 18.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Results saved to tranception_scores.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === 打分并计算 perplexity ===\n",
    "results = []\n",
    "print(\"Scoring sequences using Tranception...\")\n",
    "for i, seq in tqdm(enumerate(sequences), total=len(sequences), desc=\"Scoring\"):\n",
    "    try:\n",
    "        log_prob, total_nll, length = score_sequence(seq=seq,\n",
    "                                                     model=model,\n",
    "                                                     alphabet=alphabet,\n",
    "                                                     scoring_matrices=None,\n",
    "                                                     temperature=1.0,\n",
    "                                                     return_log_likelihood=True)\n",
    "        perplexity = math.exp(total_nll / length) if log_prob is not None else None\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {ids[i]}: {e}\")\n",
    "        log_prob = None\n",
    "        perplexity = None\n",
    "\n",
    "    results.append({\n",
    "        \"Sequence ID\": ids[i],\n",
    "        \"Sequence\": seq,\n",
    "        \"Tranception_LogLikelihood\": log_prob,\n",
    "        \"Tranception_Perplexity\": perplexity\n",
    "    })\n",
    "\n",
    "# === 保存结果 ===\n",
    "out_df = pd.DataFrame(results)\n",
    "out_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Done. Results saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac23b5fe-fa19-48d4-81bc-49726024f171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重复序列数量： 0\n",
      "ESM  重复序列数量： 0\n",
      "Tranception 重复序列数量： 0\n",
      "the saving is Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID_x</th>\n",
       "      <th>Mutations</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>PredictedBrightness</th>\n",
       "      <th>LogLikelihood</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Sequence ID_y</th>\n",
       "      <th>Tranception_LogLikelihood</th>\n",
       "      <th>Tranception_Perplexity</th>\n",
       "      <th>Brightness_z</th>\n",
       "      <th>ESM_LL_z</th>\n",
       "      <th>Tranception_LL_z</th>\n",
       "      <th>Final_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate_2</td>\n",
       "      <td>N149H:N212E:L64F:F130T:K131T:N135K</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>4.569467</td>\n",
       "      <td>-0.482935</td>\n",
       "      <td>1.620825</td>\n",
       "      <td>Candidate_2</td>\n",
       "      <td>-4.394590</td>\n",
       "      <td>81.011387</td>\n",
       "      <td>13.995412</td>\n",
       "      <td>-0.453788</td>\n",
       "      <td>0.308435</td>\n",
       "      <td>4.616686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>N149H:E222K:A206N:F130T:K140I:K126E</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>4.663651</td>\n",
       "      <td>-0.475299</td>\n",
       "      <td>1.608495</td>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>-4.496346</td>\n",
       "      <td>89.688818</td>\n",
       "      <td>15.166114</td>\n",
       "      <td>1.030583</td>\n",
       "      <td>-2.789420</td>\n",
       "      <td>4.469093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate_3</td>\n",
       "      <td>N149H:M153I:E222K:T203G:K126Q</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>4.421463</td>\n",
       "      <td>-0.488530</td>\n",
       "      <td>1.629919</td>\n",
       "      <td>Candidate_3</td>\n",
       "      <td>-4.439451</td>\n",
       "      <td>84.728376</td>\n",
       "      <td>12.155747</td>\n",
       "      <td>-1.541269</td>\n",
       "      <td>-1.057299</td>\n",
       "      <td>3.185726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate_4</td>\n",
       "      <td>N149D:M153V:K214Q:V68E:Y145F</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.907406</td>\n",
       "      <td>-0.479723</td>\n",
       "      <td>1.615627</td>\n",
       "      <td>Candidate_4</td>\n",
       "      <td>-4.381681</td>\n",
       "      <td>79.972316</td>\n",
       "      <td>5.766112</td>\n",
       "      <td>0.170630</td>\n",
       "      <td>0.701432</td>\n",
       "      <td>2.212725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candidate_9</td>\n",
       "      <td>E222Q:T203L:I128K:I136M:N135S:K140I</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.705425</td>\n",
       "      <td>-0.472896</td>\n",
       "      <td>1.604635</td>\n",
       "      <td>Candidate_9</td>\n",
       "      <td>-4.380309</td>\n",
       "      <td>79.862710</td>\n",
       "      <td>3.255525</td>\n",
       "      <td>1.497538</td>\n",
       "      <td>0.743196</td>\n",
       "      <td>1.832086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sequence ID_x                            Mutations  \\\n",
       "0   Candidate_2   N149H:N212E:L64F:F130T:K131T:N135K   \n",
       "1   Candidate_1  N149H:E222K:A206N:F130T:K140I:K126E   \n",
       "2   Candidate_3        N149H:M153I:E222K:T203G:K126Q   \n",
       "3   Candidate_4         N149D:M153V:K214Q:V68E:Y145F   \n",
       "4   Candidate_9  E222Q:T203L:I128K:I136M:N135S:K140I   \n",
       "\n",
       "                                            Sequence  PredictedBrightness  \\\n",
       "0  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             4.569467   \n",
       "1  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             4.663651   \n",
       "2  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             4.421463   \n",
       "3  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.907406   \n",
       "4  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.705425   \n",
       "\n",
       "   LogLikelihood  Perplexity Sequence ID_y  Tranception_LogLikelihood  \\\n",
       "0      -0.482935    1.620825   Candidate_2                  -4.394590   \n",
       "1      -0.475299    1.608495   Candidate_1                  -4.496346   \n",
       "2      -0.488530    1.629919   Candidate_3                  -4.439451   \n",
       "3      -0.479723    1.615627   Candidate_4                  -4.381681   \n",
       "4      -0.472896    1.604635   Candidate_9                  -4.380309   \n",
       "\n",
       "   Tranception_Perplexity  Brightness_z  ESM_LL_z  Tranception_LL_z  \\\n",
       "0               81.011387     13.995412 -0.453788          0.308435   \n",
       "1               89.688818     15.166114  1.030583         -2.789420   \n",
       "2               84.728376     12.155747 -1.541269         -1.057299   \n",
       "3               79.972316      5.766112  0.170630          0.701432   \n",
       "4               79.862710      3.255525  1.497538          0.743196   \n",
       "\n",
       "   Final_Score  \n",
       "0     4.616686  \n",
       "1     4.469093  \n",
       "2     3.185726  \n",
       "3     2.212725  \n",
       "4     1.832086  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟读取前面三个来源的分数文件\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 文件路径（假设用户上传的文件）\n",
    "brightness_path = \"Top2000_Brightness_Selected_FineTune.csv\"\n",
    "esm_score_path = \"esm1b_scores.csv\"\n",
    "tranception_score_path = \"tranception_scores.csv\"\n",
    "\n",
    "# 读取数据\n",
    "brightness_df = pd.read_csv(brightness_path)\n",
    "esm_df = pd.read_csv(esm_score_path)\n",
    "tranception_df = pd.read_csv(tranception_score_path)\n",
    "# 可选：检查重复行\n",
    "print(\"重复序列数量：\", brightness_df[\"Sequence\"].duplicated().sum())\n",
    "print(\"ESM  重复序列数量：\", esm_df[\"Sequence\"].duplicated().sum())\n",
    "print(\"Tranception 重复序列数量：\", tranception_df[\"Sequence\"].duplicated().sum())\n",
    "\n",
    "# 合并三方数据\n",
    "merged_df = brightness_df.merge(esm_df, on=\"Sequence\", how=\"inner\").merge(tranception_df, on=\"Sequence\", how=\"inner\")\n",
    "\n",
    "# 标准化三项打分：亮度、ESM-1b LL、Tranception LL\n",
    "scaler = StandardScaler()\n",
    "merged_df[[\"Brightness_z\", \"ESM_LL_z\", \"Tranception_LL_z\"]] = scaler.fit_transform(\n",
    "    merged_df[[\"PredictedBrightness\", \"LogLikelihood\", \"Tranception_LogLikelihood\"]]\n",
    ")\n",
    "\n",
    "# 融合打分：默认等权重 1:1:1\n",
    "merged_df[\"Final_Score\"] = merged_df[[\"Brightness_z\", \"ESM_LL_z\", \"Tranception_LL_z\"]].mean(axis=1)\n",
    "\n",
    "# 选出 Top300\n",
    "top300_df = merged_df.sort_values(\"Final_Score\", ascending=False).head(300).reset_index(drop=True)\n",
    "\n",
    "# 展示输出\n",
    "\n",
    "top300_df.to_csv(\"top300_fused_scored.csv\", index=False)\n",
    "print(\"the saving is Done\")\n",
    "top300_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cee30884890fb5",
   "metadata": {},
   "source": [
    "### 7.2 结构一致性打分筛选（ESMFold + ProteinMPNN）\n",
    "\n",
    "本子阶段从结构角度对 Top2000 候选序列进行评估，旨在筛选出结构可信、序列与结构一致性较高的 Top300 条候选序列，用于后续 AlphaFold 精预测和实验候选优选。\n",
    "\n",
    "---\n",
    "\n",
    "#### 流程概述：\n",
    "\n",
    "本流程可分为两个子模块：\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.2.1 结构预测（ESMFold）\n",
    "\n",
    "- 使用 [ESMFold](https://github.com/facebookresearch/esm) 离线模型（`esmfold_v1_offline`）对 Top2000 序列逐条进行结构预测；\n",
    "- 自动跳过已完成预测的 `.pdb` 文件，支持断点续跑；\n",
    "- 每条序列输出为一个标准 `.pdb` 文件，保存在 `esmfold_outputs/` 文件夹中；\n",
    "- 最终得到约 1900+ 条合法结构文件用于后续打分。\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.2.2 结构一致性打分（ProteinMPNN）\n",
    "\n",
    "- 加载 ProteinMPNN 原始模型权重（`v_48_020.pt`）；\n",
    "- 对每个结构 `.pdb` 文件执行：\n",
    "  - **逆折叠重建**：使用 ProteinMPNN 对结构中的 backbone 进行序列预测；\n",
    "  - **打分输出**：\n",
    "    - `MPNN_LL`: 基于 mask 的局部对数似然；\n",
    "    - `MPNN_Global_LL`: 全局打分指标，衡量整条序列与结构的一致性；\n",
    "- 将打分结果与原亮度预测结果合并。\n",
    "\n",
    "---\n",
    "\n",
    "#### 融合打分与筛选策略：\n",
    "\n",
    "- 使用 `StandardScaler` 对 `PredictedBrightness` 与 `MPNN_Global_LL` 进行标准化；\n",
    "- 构建融合得分：\n",
    "  ```math\n",
    "  FusionScore = Brightness_z + MPNN_z\n",
    "  ```\n",
    "  表示在亮度与结构一致性之间寻找均衡的高质量候选；\n",
    "- 按照 `FusionScore` 降序排列，选出 Top300；\n",
    "- 保存为 `brightness_mpnn_top300.csv`。\n",
    "\n",
    "---\n",
    "\n",
    "#### 阶段亮点：\n",
    "\n",
    "- **结构级约束强化**：相比仅依赖语言模型，MPNN 能有效过滤出结构稳定性与序列天然性双优的蛋白；\n",
    "- **完整打分闭环**：通过“序列 → 结构 → 逆折叠”的闭环流程，建立从功能预测到结构可靠性的多重验证；\n",
    "- **并行执行灵活性强**：ESMFold 与 MPNN 可并行部署，适配集群或高性能 GPU 环境。\n",
    "\n",
    "---\n",
    "\n",
    "#### 与 7.1 的整合：\n",
    "\n",
    "最终将与 7.1 中的语言模型 Top300 序列进行 **并集融合**（共约 400~600 条），以进入第八阶段\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe139399-f5f8-46f4-a2bb-e48ec829190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"/home/ma-user/work/esmfold_v1_offline.zip\"\n",
    "extract_dir = \"/home/ma-user/work/esmfold_v1_offline\"\n",
    "\n",
    "# 解压\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"解压完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1434ed7-314f-47a1-bb08-0bd6ffeb1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import EsmForProteinFolding, AutoTokenizer\n",
    "\n",
    "# ====== 路径配置 ======\n",
    "CSV_PATH = \"Top2000_Brightness_Selected_FineTune.csv\"\n",
    "OUT_DIR = \"esmfold_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ====== 加载模型与Tokenizer ======\n",
    "model_dir = \"/home/ma-user/work/esmfold_v1_offline\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EsmForProteinFolding.from_pretrained(model_dir, use_safetensors=False).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# ====== 读取序列 ======\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "sequences = df[\"Sequence\"].tolist()\n",
    "ids = df[\"Sequence ID\"].tolist() if \"Sequence ID\" in df.columns else [f\"seq_{i}\" for i in range(len(sequences))]\n",
    "\n",
    "# ====== 获取已完成的 PDB 文件名 ======\n",
    "existing_pdbs = set(f.split(\".\")[0] for f in os.listdir(OUT_DIR) if f.endswith(\".pdb\"))\n",
    "\n",
    "# ====== 筛选未完成的序列 ======\n",
    "pending_items = [(i, s) for i, s in zip(ids, sequences) if i not in existing_pdbs]\n",
    "print(f\"已完成：{len(existing_pdbs)} 条；待预测：{len(pending_items)} 条\")\n",
    "\n",
    "# ====== 开始预测并保存 PDB ======\n",
    "for seq_id, seq in tqdm(pending_items, desc=\"Continuing with ESMFold\"):\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            pdb_str = model.infer_pdb(seq)\n",
    "        out_path = os.path.join(OUT_DIR, f\"{seq_id}.pdb\")\n",
    "        with open(out_path, \"w\") as f:\n",
    "            f.write(pdb_str)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed on {seq_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c801a442-f446-4713-9034-0378d783d179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ProteinMPNN'...\n",
      "remote: Enumerating objects: 634, done.\u001B[K\n",
      "remote: Counting objects: 100% (239/239), done.\u001B[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001B[K\n",
      "remote: Total 634 (delta 185), reused 154 (delta 154), pack-reused 395 (from 1)\u001B[K\n",
      "Receiving objects: 100% (634/634), 119.91 MiB | 678.00 KiB/s, done.\n",
      "Resolving deltas: 100% (285/285), done.\n",
      "/home/ma-user/work/ProteinMPNN\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/dauparas/ProteinMPNN.git\n",
    "%cd ProteinMPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f6092e-bcbc-4632-a953-49bf71d73c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功读取结构！包含序列链数： 5\n"
     ]
    }
   ],
   "source": [
    "from ProteinMPNN.protein_mpnn_utils import parse_PDB, StructureDatasetPDB\n",
    "\n",
    "pdb_dict = parse_PDB(\"Candidate_1001.pdb\", ca_only=False)\n",
    "dataset = StructureDatasetPDB(pdb_dict, max_length=10000)\n",
    "\n",
    "print(\"成功读取结构！包含序列链数：\", len(pdb_dict[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a64dc7c-c963-4aca-8966-015b1333269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "结构合法性检查: 100%|██████████| 2000/2000 [00:18<00:00, 110.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 合法结构数: 2000\n",
      " 无效结构数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ProteinMPNN.protein_mpnn_utils import parse_PDB, StructureDatasetPDB\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm  # 添加 tqdm\n",
    "\n",
    "# === 数据路径配置 ===\n",
    "df = pd.read_csv(\"Top2000_Brightness_Selected_FineTune.csv\")\n",
    "pdb_dir = \"esmfold_outputs\"\n",
    "jsonl_valid_path = \"mpnn_valid_input.jsonl\"\n",
    "\n",
    "valid_entries = []\n",
    "invalid_ids = []\n",
    "\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"结构合法性检查\"):\n",
    "    seq_id = row[\"Sequence ID\"]\n",
    "    seq = row[\"Sequence\"]\n",
    "    pdb_path = os.path.join(pdb_dir, f\"{seq_id}.pdb\")\n",
    "    \n",
    "    if not os.path.isfile(pdb_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        pdb_dict = parse_PDB(pdb_path,input_chain_list=[\"A\"],ca_only=False)\n",
    "        dataset = StructureDatasetPDB(pdb_dict, max_length=10000)\n",
    "        valid_entries.append({\n",
    "            \"name\": seq_id,\n",
    "            \"pdb_path\": pdb_path,\n",
    "            \"seq\": seq\n",
    "        })\n",
    "    except Exception as e:\n",
    "        invalid_ids.append(seq_id)\n",
    "        print(f\"[跳过] {seq_id}: 结构无法用于MPNN ({e})\")\n",
    "\n",
    "# === 写入 JSONL 文件 ===\n",
    "with open(jsonl_valid_path, \"w\") as f:\n",
    "    for entry in valid_entries:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"\\n 合法结构数: {len(valid_entries)}\")\n",
    "print(f\" 无效结构数: {len(invalid_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcfe1e4-b56e-4d47-a13a-d55c10302ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "chain_id_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "fixed_positions_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "pssm_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "omit_AA_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "bias_AA_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "tied_positions_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "bias by residue dictionary is not loaded, or not provided\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Number of edges: 48\n",
      "Training noise level: 0.2A\n",
      "Score for Candidate_1001 from PDB, mean: 2.3080, std: 0.0000, sample size: 1,  global score, mean: 2.3080, std: 0.0000, sample size: 1\n"
     ]
    }
   ],
   "source": [
    "!python ProteinMPNN/protein_mpnn_run.py \\\n",
    "  --pdb_path Candidate_1001.pdb \\\n",
    "  --pdb_path_chains A \\\n",
    "  --out_folder mpnn_outputs \\\n",
    "  --score_only 1 \\\n",
    "  --num_seq_per_target 1 \\\n",
    "  --batch_size 1 \\\n",
    "  --model_name v_48_020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa1f29fd-af40-4fbb-9fcc-c52a34867379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast MPNN scoring: 100%|██████████| 2000/2000 [14:19<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Saved to fast_mpnn_scores.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from ProteinMPNN.protein_mpnn_utils import ProteinMPNN, parse_PDB, StructureDatasetPDB, tied_featurize, _scores\n",
    "\n",
    "# ===== 配置 =====\n",
    "pdb_dir = \"esmfold_outputs\"\n",
    "model_path = \"ProteinMPNN/vanilla_model_weights/v_48_020.pt\"\n",
    "output_csv = \"fast_mpnn_scores.csv\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ca_only = False\n",
    "\n",
    "# === 固定参数（来源于 run.py）===\n",
    "hidden_dim = 128\n",
    "num_layers = 3\n",
    "backbone_noise = 0.0\n",
    "\n",
    "# === 加载 checkpoint ===\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# === 正确初始化模型 ===\n",
    "model = ProteinMPNN(\n",
    "    ca_only=ca_only,\n",
    "    num_letters=21,\n",
    "    node_features=hidden_dim,\n",
    "    edge_features=hidden_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_encoder_layers=num_layers,\n",
    "    num_decoder_layers=num_layers,\n",
    "    augment_eps=backbone_noise,\n",
    "    k_neighbors=checkpoint[\"num_edges\"]\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# ===== 定义打分函数 =====\n",
    "def fast_score(pdb_path):\n",
    "    try:\n",
    "        pdb_data = parse_PDB(pdb_path, input_chain_list=None, ca_only=False)\n",
    "\n",
    "        dataset = StructureDatasetPDB(pdb_data)\n",
    "\n",
    "        protein = dataset[0]\n",
    "\n",
    "        batch = [protein]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X, S, mask, lengths, chain_M, chain_encoding_all, _, _, _, _, chain_M_pos, _, residue_idx, *_ = \\\n",
    "            tied_featurize(batch, device, None, None, None, None, None, ca_only)\n",
    "\n",
    "            randn = torch.randn(chain_M.shape, device=device)\n",
    "            log_probs = model(X, S, mask, chain_M * chain_M_pos,\n",
    "            residue_idx, chain_encoding_all, randn)\n",
    "\n",
    "            native_score = _scores(S, log_probs, mask * chain_M * chain_M_pos).mean().item()\n",
    "            global_score = _scores(S, log_probs, mask).mean().item()\n",
    "\n",
    "        return native_score, global_score\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {os.path.basename(pdb_path)}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ===== 批量打分 =====\n",
    "records = []\n",
    "pdb_list = sorted([f for f in os.listdir(pdb_dir) if f.endswith(\".pdb\")])\n",
    "for fname in tqdm(pdb_list, desc=\"Fast MPNN scoring\"):\n",
    "    path = os.path.join(pdb_dir, fname)\n",
    "    seq_id = fname.replace(\".pdb\", \"\")\n",
    "    mpnn_ll, mpnn_global = fast_score(path)\n",
    "    records.append({\n",
    "        \"Sequence ID\": seq_id,\n",
    "        \"MPNN_LL\": mpnn_ll,\n",
    "        \"MPNN_Global_LL\": mpnn_global\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Done! Saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dc62ea7-41f4-487f-86e6-95427c4245b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>MPNN_LL</th>\n",
       "      <th>MPNN_Global_LL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>2.211623</td>\n",
       "      <td>2.211623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate_10</td>\n",
       "      <td>2.153116</td>\n",
       "      <td>2.153116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate_100</td>\n",
       "      <td>2.187891</td>\n",
       "      <td>2.187891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate_1000</td>\n",
       "      <td>2.288170</td>\n",
       "      <td>2.288170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candidate_1001</td>\n",
       "      <td>2.322528</td>\n",
       "      <td>2.322528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sequence ID   MPNN_LL  MPNN_Global_LL\n",
       "0     Candidate_1  2.211623        2.211623\n",
       "1    Candidate_10  2.153116        2.153116\n",
       "2   Candidate_100  2.187891        2.187891\n",
       "3  Candidate_1000  2.288170        2.288170\n",
       "4  Candidate_1001  2.322528        2.322528"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13019d72-30f1-4d23-84ff-178d0dc885ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProteinMPNN.protein_mpnn_utils import parse_PDB\n",
    "\n",
    "# 示例：检查 Candidate_1000.pdb\n",
    "pdb_path = \"esmfold_outputs/Candidate_1000.pdb\"\n",
    "pdb_data = parse_PDB(pdb_path, input_chain_list=[\"A\"], ca_only=False)\n",
    "seq = pdb_data[0][\"seq\"]\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWYX-'\n",
    "alphabet_set = set(alphabet)\n",
    "\n",
    "bad_chars = set(seq).difference(alphabet_set)\n",
    "print(f\"❗非法字符: {bad_chars}\")\n",
    "\n",
    "print(pdb_data[0].keys())\n",
    "print(pdb_data[0]['coords_chain_A'].keys())\n",
    "dataset = StructureDatasetPDB(pdb_data)\n",
    "from ProteinMPNN.protein_mpnn_utils import StructureDatasetPDB\n",
    "for d in pdb_data:\n",
    "    try:\n",
    "        dataset = StructureDatasetPDB([d])\n",
    "        print(f\"{d['name']} 成功: {len(dataset)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{d['name']} 失败: {e}\")\n",
    "print(len(dataset))\n",
    "protein = dataset[0]\n",
    "batch = [protein]\n",
    "with torch.no_grad():\n",
    "    X, S, mask, lengths, chain_M, chain_encoding_all, _, _, _, _, chain_M_pos, _, residue_idx, *_ = \\\n",
    "    tied_featurize(batch, device, None, None, None, None, None, ca_only)\n",
    "\n",
    "    randn = torch.randn(chain_M.shape, device=device)\n",
    "    log_probs = model(X, S, mask, chain_M * chain_M_pos,\n",
    "    residue_idx, chain_encoding_all, randn)\n",
    "\n",
    "    native_score = _scores(S, log_probs, mask * chain_M * chain_M_pos).mean().item()\n",
    "    global_score = _scores(S, log_probs, mask).mean().item()\n",
    "\n",
    "print(\"Sequence:\", protein[\"seq\"])\n",
    "print(\"Residue indices:\", residue_idx)\n",
    "print(\"Chain encoding:\", chain_encoding_all)\n",
    "print(\"native:\",native_score)\n",
    "print(\"global:\",global_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50aeafb9-ad6f-4d50-82f0-0bc8602159d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XYZ type: <class 'numpy.ndarray'>\n",
      "XYZ shape: (238, 4, 3)\n",
      "Seq: ['MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVHRIELKGIDFKEDGNILGHKLEYNINSHNVYIMADKQKNGIKLNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSIQSELSKDPNERRDHMVLLEFVTAAGITHGMDELYK']\n"
     ]
    }
   ],
   "source": [
    "from ProteinMPNN.protein_mpnn_utils import parse_PDB_biounits\n",
    "\n",
    "pdb_path = \"esmfold_outputs/Candidate_1000.pdb\"\n",
    "xyz, seq = parse_PDB_biounits(pdb_path, atoms=['N','CA','C','O'], chain='A')\n",
    "\n",
    "print(\"XYZ type:\", type(xyz))\n",
    "print(\"XYZ shape:\", getattr(xyz, \"shape\", None))\n",
    "print(\"Seq:\", seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0032f8ab-385a-42d4-8d81-40938097a960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存融合亮度与MPNN得分的 Top300 到 brightness_mpnn_top300.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>Mutations</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>PredictedBrightness</th>\n",
       "      <th>MPNN_LL</th>\n",
       "      <th>MPNN_Global_LL</th>\n",
       "      <th>Brightness_z</th>\n",
       "      <th>MPNN_z</th>\n",
       "      <th>FusionScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>N149H:E222K:A206N:F130T:K140I:K126E</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>4.663651</td>\n",
       "      <td>2.211623</td>\n",
       "      <td>2.211623</td>\n",
       "      <td>15.166114</td>\n",
       "      <td>-0.041436</td>\n",
       "      <td>15.124678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate_2</td>\n",
       "      <td>N149H:N212E:L64F:F130T:K131T:N135K</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>4.569467</td>\n",
       "      <td>2.235711</td>\n",
       "      <td>2.235711</td>\n",
       "      <td>13.995412</td>\n",
       "      <td>0.198875</td>\n",
       "      <td>14.194287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate_3</td>\n",
       "      <td>N149H:M153I:E222K:T203G:K126Q</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>4.421463</td>\n",
       "      <td>2.175850</td>\n",
       "      <td>2.175850</td>\n",
       "      <td>12.155747</td>\n",
       "      <td>-0.398312</td>\n",
       "      <td>11.757435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate_4</td>\n",
       "      <td>N149D:M153V:K214Q:V68E:Y145F</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.907406</td>\n",
       "      <td>2.216319</td>\n",
       "      <td>2.216319</td>\n",
       "      <td>5.766112</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>5.771531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candidate_12</td>\n",
       "      <td>M153L:K214Q:V68E:I128T:K126Q</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>3.678702</td>\n",
       "      <td>2.406655</td>\n",
       "      <td>2.406655</td>\n",
       "      <td>2.923361</td>\n",
       "      <td>1.904268</td>\n",
       "      <td>4.827629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sequence ID                            Mutations  \\\n",
       "0   Candidate_1  N149H:E222K:A206N:F130T:K140I:K126E   \n",
       "1   Candidate_2   N149H:N212E:L64F:F130T:K131T:N135K   \n",
       "2   Candidate_3        N149H:M153I:E222K:T203G:K126Q   \n",
       "3   Candidate_4         N149D:M153V:K214Q:V68E:Y145F   \n",
       "4  Candidate_12         M153L:K214Q:V68E:I128T:K126Q   \n",
       "\n",
       "                                            Sequence  PredictedBrightness  \\\n",
       "0  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             4.663651   \n",
       "1  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             4.569467   \n",
       "2  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             4.421463   \n",
       "3  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.907406   \n",
       "4  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...             3.678702   \n",
       "\n",
       "    MPNN_LL  MPNN_Global_LL  Brightness_z    MPNN_z  FusionScore  \n",
       "0  2.211623        2.211623     15.166114 -0.041436    15.124678  \n",
       "1  2.235711        2.235711     13.995412  0.198875    14.194287  \n",
       "2  2.175850        2.175850     12.155747 -0.398312    11.757435  \n",
       "3  2.216319        2.216319      5.766112  0.005419     5.771531  \n",
       "4  2.406655        2.406655      2.923361  1.904268     4.827629  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ===== 读取两个文件 =====\n",
    "mpnn_df = pd.read_csv(\"fast_mpnn_scores.csv\")\n",
    "brightness_df = pd.read_csv(\"Top2000_Brightness_Selected_FineTune.csv\")\n",
    "\n",
    "# ===== 合并，按 Sequence ID 对齐 =====\n",
    "merged_df = brightness_df.merge(mpnn_df, on=\"Sequence ID\", how=\"inner\")\n",
    "\n",
    "# ===== 只融合 Brightness + MPNN_Global_LL 两项得分 =====\n",
    "scaler = StandardScaler()\n",
    "merged_df[[\"Brightness_z\", \"MPNN_z\"]] = scaler.fit_transform(\n",
    "    merged_df[[\"PredictedBrightness\", \"MPNN_Global_LL\"]]\n",
    ")\n",
    "\n",
    "# ===== 简单加权求和（也可以改成加权平均）=====\n",
    "merged_df[\"FusionScore\"] = merged_df[\"Brightness_z\"] + merged_df[\"MPNN_z\"]\n",
    "\n",
    "# ===== 排序选 Top300 =====\n",
    "top300 = merged_df.sort_values(by=\"FusionScore\", ascending=False).head(300).reset_index(drop=True)\n",
    "\n",
    "# ===== 保存结果 =====\n",
    "top300.to_csv(\"brightness_mpnn_top300.csv\", index=False)\n",
    "print(\"已保存融合亮度与MPNN得分的 Top300 到 brightness_mpnn_top300.csv\")\n",
    "top300.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6da5d08761b1b",
   "metadata": {},
   "source": [
    "### 7.3 Top300 序列并集融合策略说明\n",
    "\n",
    "经过第七阶段的双通路筛选：\n",
    "\n",
    "- **7.1 路线**：基于序列语言建模（ESM-1b + Tranception + 亮度）打分选出 Top300；\n",
    "- **7.2 路线**：基于结构建模（ESMFold + ProteinMPNN + 亮度）打分选出 Top300；\n",
    "\n",
    "二者从不同维度衡量序列的设计质量：\n",
    "\n",
    "- 语言建模路线更侧重于“序列的合理性、语法性、保守性与功能预测”；\n",
    "- 结构建模路线更关注“序列与结构的一致性”与“潜在的折叠稳定性”。\n",
    "\n",
    "为兼顾 **蛋白序列的功能潜力** 与 **结构可行性**，我们采用如下融合策略：\n",
    "\n",
    "------\n",
    "\n",
    "#### 融合方法：并集取 TopN 备选\n",
    "\n",
    "- 对两个 Top300 列表以 `Sequence` 字段为主键进行 **并集合并**；\n",
    "- 去除重复序列（部分序列可能被两个方法同时筛中）；\n",
    "- 最终形成约 **400~600 条高质量备选序列集合**（记作 `Top_Union_Set`）；\n",
    "- 保存为标准 `.csv` 文件（如 `top_candidates_union.csv`），用于后续 AlphaFold 高精预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eafbae06-381f-4dc9-8a78-9d020744bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存包含突变和序列的并集，共 510 条记录。\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Mutations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate_2</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149H:N212E:L64F:F130T:K131T:N135K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149H:E222K:A206N:F130T:K140I:K126E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate_3</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149H:M153I:E222K:T203G:K126Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate_4</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149D:M153V:K214Q:V68E:Y145F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candidate_9</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>E222Q:T203L:I128K:I136M:N135S:K140I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sequence ID                                           Sequence  \\\n",
       "0  Candidate_2  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "1  Candidate_1  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "2  Candidate_3  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "3  Candidate_4  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "4  Candidate_9  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "\n",
       "                             Mutations  \n",
       "0   N149H:N212E:L64F:F130T:K131T:N135K  \n",
       "1  N149H:E222K:A206N:F130T:K140I:K126E  \n",
       "2        N149H:M153I:E222K:T203G:K126Q  \n",
       "3         N149D:M153V:K214Q:V68E:Y145F  \n",
       "4  E222Q:T203L:I128K:I136M:N135S:K140I  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取两个 Top300 文件\n",
    "top300_plm = pd.read_csv(\"top300_fused_scored.csv\")\n",
    "top300_mpnn = pd.read_csv(\"brightness_mpnn_top300.csv\")\n",
    "\n",
    "# 标准化列名\n",
    "top300_plm = top300_plm.rename(columns={\"Sequence ID_x\": \"Sequence ID\"})\n",
    "top300_mpnn = top300_mpnn.rename(columns={\"Sequence ID\": \"Sequence ID\"})\n",
    "\n",
    "# 合并两个数据（保留所有行），去重\n",
    "combined_df = pd.concat([top300_plm, top300_mpnn], ignore_index=True)\n",
    "combined_df = combined_df.drop_duplicates(subset=[\"Sequence ID\"])\n",
    "\n",
    "# 取并集中的记录\n",
    "union_ids = set(top300_plm[\"Sequence ID\"]).union(set(top300_mpnn[\"Sequence ID\"]))\n",
    "union_df = combined_df[combined_df[\"Sequence ID\"].isin(union_ids)]\n",
    "\n",
    "# 只保留所需字段\n",
    "output_df = union_df[[\"Sequence ID\", \"Sequence\", \"Mutations\"]].reset_index(drop=True)\n",
    "\n",
    "# 保存为 CSV\n",
    "output_df.to_csv(\"top_candidates_union.csv\", index=False)\n",
    "print(f\"已保存包含突变和序列的并集，共 {len(output_df)} 条记录。\")\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f7d821b11c02c",
   "metadata": {},
   "source": [
    "### 第八阶段：基于 ESM-Embedding 的聚类筛选与结构验证准备\n",
    "\n",
    "为从 Top600 候选序列（7.1 与 7.2 筛选并集）中进一步提升序列的多样性与代表性，我们设计了如下筛选流程，选出结构预测优先验证的 Top40 条候选序列。\n",
    "\n",
    "------\n",
    "\n",
    "#### 8.1 ESM 表征生成\n",
    "\n",
    "使用蛋白语言模型 `esm2_t33_650M_UR50D` 对所有候选序列进行 embedding 特征提取：\n",
    "\n",
    "- 每条序列被映射为一个 640 维嵌入向量；\n",
    "- 使用模型与训练阶段一致性确保嵌入特征可用于下游亮度预测器、聚类等分析；\n",
    "- 若嵌入中存在 `NaN`，则自动剔除对应序列，保障数据稳定性。\n",
    "\n",
    "> 嵌入向量形状为 `(N, 640)`，共提取约 500 条候选序列嵌入。\n",
    "\n",
    "------\n",
    "\n",
    "#### 8.2 KMeans 聚类筛选\n",
    "\n",
    "为保证最终候选的**结构和功能多样性**，我们采用无监督聚类算法进行降维筛选：\n",
    "\n",
    "- 使用 KMeans 聚类算法将嵌入划分为 **40 个类别**；\n",
    "- 在每个簇内随机选取 1 条代表序列，构成 `diverse_top40_by_cluster.csv`；\n",
    "- 所有代表序列都具备较高亮度预测分数，且在嵌入空间上互异性较高。\n",
    "\n",
    "该策略有效避免了 Top300U300 中存在的大量“局部冗余”或“突变过拟合”问题，有助于提升结构预测阶段候选的**代表性**与**成功率**。\n",
    "\n",
    "------\n",
    "\n",
    "#### 8.3 FASTA 导出与结构验证准备\n",
    "\n",
    "将最终 Top40 序列导出为标准 `FASTA` 格式文件：\n",
    "\n",
    "- 每条序列以 `>Sequence ID` 作为标识符；\n",
    "- 可直接用于 **ColabFold** 或本地 `AlphaFold2` 结构预测；\n",
    "- 保存文件为 `top40_candidates.fasta`，作为结构评估阶段的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8849e9b1-c20e-4de7-8e89-0d39951997d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: pandas in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26e33ec-909a-430a-a8a5-93af5a8858c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尝试加载用于预测的 ESM 模型: esm2_t33_650M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /home/ma-user/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /home/ma-user/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用于预测的 ESM 模型 'esm2_t33_650M_UR50D' 已加载到 cuda，耗时 81.24 秒。\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import esm # 假设 esm 库已导入\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "top_union_df = pd.read_csv(\"top_candidates_union.csv\")\n",
    "candidate_list = top_union_df[\"Sequence\"].astype(str).tolist()\n",
    "mutation_list = top_union_df[\"Mutations\"].astype(str).tolist()\n",
    "\n",
    "# 假设 model, candidate_list, mutation_list, exclusion_sequences,\n",
    "# TOP_N_SELECT, BATCH_SIZE, get_esm_embeddings (来自上一步) 已经定义好\n",
    "# 并且 avGFP_WT_sequence, candidate_position_pool_0based, MAX_MUTATIONS, N_CANDIDATES_TO_GENERATE 也已定义\n",
    "\n",
    "# --- 修正：定义用于预测的 ESM 模型名称 ---\n",
    "# !!! 关键：这里必须使用与训练 rf_model 时相同的 ESM 模型 !!!\n",
    "# 根据之前的日志，rf_model 是用 640 维嵌入训练的 (来自 esm2_t30_150M_UR50D)\n",
    "PREDICTION_ESM_MODEL_NAME = \"esm2_t33_650M_UR50D\" # <--- 确认这个模型与训练时一致\n",
    "\n",
    "print(f\"尝试加载用于预测的 ESM 模型: {PREDICTION_ESM_MODEL_NAME}\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # 加载指定用于预测的 ESM 模型和字母表\n",
    "    esm_model_pred, alphabet_pred = esm.pretrained.load_model_and_alphabet(PREDICTION_ESM_MODEL_NAME)\n",
    "    batch_converter_pred = alphabet_pred.get_batch_converter()\n",
    "    # 重新确定设备 (优先使用 GPU)\n",
    "    DEVICE_pred = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    esm_model_pred.to(DEVICE_pred)\n",
    "    esm_model_pred.eval() # 设置模型为评估模式 (关闭 dropout 等)\n",
    "    print(f\"用于预测的 ESM 模型 '{PREDICTION_ESM_MODEL_NAME}' 已加载到 {DEVICE_pred}，耗时 {time.time() - start_time:.2f} 秒。\")\n",
    "except Exception as e:\n",
    "    print(f\"加载 ESM 模型 {PREDICTION_ESM_MODEL_NAME} 时出错: {e}\")\n",
    "    print(\"请确保 'fair-esm' 已安装，模型名称正确，并且有足够的内存。\")\n",
    "    # 根据需要处理错误，例如退出\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13dcdfd7-aecb-4e64-8f30-3499aeaa60ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用预测模型为候选序列生成 ESM 嵌入...\n",
      "检测到 GPU，使用 GPU 批次大小: 16\n",
      "正在为 510 个序列生成嵌入，共 32 个批次（批次大小: 16，设备: cuda）...\n",
      "  已处理批次 10/32... (耗时: 9.28 秒)\n",
      "  已处理批次 20/32... (耗时: 18.15 秒)\n",
      "  已处理批次 30/32... (耗时: 27.04 秒)\n",
      "  已处理批次 32/32... (耗时: 28.71 秒)\n",
      "嵌入生成在 28.71 秒内完成。\n",
      "平均每个序列耗时: 0.0563 秒\n",
      "候选序列嵌入的形状: (510, 1280)\n"
     ]
    }
   ],
   "source": [
    "# --- 6.1 为候选序列生成 ESM 嵌入 ---\n",
    "print(\"\\n使用预测模型为候选序列生成 ESM 嵌入...\")\n",
    "candidate_embeddings_np = np.array([]) # 初始化为空 numpy 数组\n",
    "if candidate_list:\n",
    "    # 确定批次大小 (如果使用GPU，可以尝试更大的批次)\n",
    "    prediction_batch_size = 8 # 默认使用 CPU 批次大小 (来自之前的定义)\n",
    "    if DEVICE_pred == torch.device(\"cuda\"):\n",
    "        # 如果是GPU，可以尝试更大的批次，例如 8, 16 或 32，取决于GPU内存和模型大小\n",
    "        # 对于较大的模型如 150M，可能需要较小的批次大小\n",
    "        prediction_batch_size = 16 # 示例值 (根据你的 GPU 内存调整，与训练时用的GPU_BATCH_SIZE可以不同)\n",
    "        print(f\"检测到 GPU，使用 GPU 批次大小: {prediction_batch_size}\")\n",
    "    else:\n",
    "        print(f\"使用 CPU 批次大小: {prediction_batch_size}\")\n",
    "\n",
    "    # *** 修复点：使用正确的函数名 'get_esm_embeddings' ***\n",
    "    # 传递用于预测的模型、字母表、转换器和设备\n",
    "    candidate_embeddings_tensor = get_esm_embeddings( # <-- 使用正确的函数名\n",
    "        candidate_list,\n",
    "        esm_model_pred,         # 使用预测模型\n",
    "        alphabet_pred,        # 使用预测模型的字母表\n",
    "        batch_converter_pred, # 使用预测模型的转换器\n",
    "        DEVICE_pred,          # 使用预测模型所在的设备 (可能是 cuda 或 cpu)\n",
    "        batch_size=prediction_batch_size # 使用调整后的批次大小\n",
    "    )\n",
    "\n",
    "    # 将嵌入结果转换为 NumPy 数组以用于 scikit-learn 模型\n",
    "    # .cpu() 确保数据在 CPU 上，然后转换为 numpy\n",
    "    candidate_embeddings_np = candidate_embeddings_tensor.cpu().numpy()\n",
    "\n",
    "    print(f\"候选序列嵌入的形状: {candidate_embeddings_np.shape}\") # 检查维度是否正确 (应为 N x 640)\n",
    "\n",
    "    # 检查是否有 NaN (如果嵌入过程中出错)\n",
    "    if np.isnan(candidate_embeddings_np).any():\n",
    "        print(\"警告：在候选序列嵌入中发现 NaN 值。将移除相应的候选序列。\")\n",
    "        nan_mask = np.isnan(candidate_embeddings_np).any(axis=1)\n",
    "        # 需要同时过滤 candidate_list, mutation_list 和 embeddings\n",
    "        # 使用列表推导式进行过滤\n",
    "        original_count = len(candidate_list)\n",
    "        candidate_list = [seq for i, seq in enumerate(candidate_list) if not nan_mask[i]]\n",
    "        mutation_list = [mut for i, mut in enumerate(mutation_list) if not nan_mask[i]]\n",
    "        candidate_embeddings_np = candidate_embeddings_np[~nan_mask]\n",
    "        print(f\"因 NaN 嵌入移除了 {original_count - len(candidate_list)} 个候选序列。\")\n",
    "        print(f\"剩余候选序列数量: {len(candidate_list)}\")\n",
    "        print(f\"过滤后的候选序列嵌入形状: {candidate_embeddings_np.shape}\")\n",
    "\n",
    "else:\n",
    "    # candidate_embeddings_np 已经初始化为空数组\n",
    "    print(\"上一步没有生成候选序列，跳过预测。\")\n",
    "np.save(\"top_union_candidate_embeddings.npy\", candidate_embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21b4cb8c-a4cd-42f0-a1ad-b2cb8003e849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sequence ID', 'Sequence', 'Mutations'], dtype='object')\n",
      "Index(['Sequence ID', 'Mutations', 'Sequence', 'PredictedBrightness'], dtype='object')\n",
      "Index(['Sequence ID', 'Sequence_x', 'Mutations_x', 'Mutations_y', 'Sequence_y',\n",
      "       'PredictedBrightness'],\n",
      "      dtype='object')\n",
      " 共载入 510 条候选序列用于聚类，嵌入 shape: (510, 1280)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Mutations</th>\n",
       "      <th>PredictedBrightness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate_2</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149H:N212E:L64F:F130T:K131T:N135K</td>\n",
       "      <td>4.569467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149H:E222K:A206N:F130T:K140I:K126E</td>\n",
       "      <td>4.663651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate_3</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149H:M153I:E222K:T203G:K126Q</td>\n",
       "      <td>4.421463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate_4</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149D:M153V:K214Q:V68E:Y145F</td>\n",
       "      <td>3.907406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candidate_9</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>E222Q:T203L:I128K:I136M:N135S:K140I</td>\n",
       "      <td>3.705425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sequence ID                                           Sequence  \\\n",
       "0  Candidate_2  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "1  Candidate_1  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "2  Candidate_3  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "3  Candidate_4  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "4  Candidate_9  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "\n",
       "                             Mutations  PredictedBrightness  \n",
       "0   N149H:N212E:L64F:F130T:K131T:N135K             4.569467  \n",
       "1  N149H:E222K:A206N:F130T:K140I:K126E             4.663651  \n",
       "2        N149H:M153I:E222K:T203G:K126Q             4.421463  \n",
       "3         N149D:M153V:K214Q:V68E:Y145F             3.907406  \n",
       "4  E222Q:T203L:I128K:I136M:N135S:K140I             3.705425  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 1. 导入依赖 ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# === 2. 加载 embedding 和元信息 ===\n",
    "embedding_path = \"top_union_candidate_embeddings.npy\"\n",
    "union_id_path = \"top_candidates_union.csv\"\n",
    "source_data_path = \"Top2000_Brightness_Selected_FineTune.csv\"  # 含 Sequence 列\n",
    "\n",
    "# 读取 embedding（shape: N x 640）\n",
    "embeddings = np.load(embedding_path)\n",
    "\n",
    "# 读取并集 ID + 合并 Sequence\n",
    "union_df = pd.read_csv(union_id_path)  # 需要包含 'Sequence ID'\n",
    "print(union_df.columns)\n",
    "source_df = pd.read_csv(source_data_path)  # 包含 'Sequence ID' 和 'Sequence'\n",
    "print(source_df.columns)\n",
    "merged_df = union_df.merge(source_df, on=\"Sequence ID\", how=\"inner\")\n",
    "print(merged_df.columns)\n",
    "merged_df = merged_df.rename(columns={\"Sequence_x\": \"Sequence\"})\n",
    "merged_df = merged_df.rename(columns={\"Mutations_x\": \"Mutations\"})\n",
    "merged_df=merged_df[[\"Sequence ID\", \"Sequence\", \"Mutations\",\"PredictedBrightness\"]].reset_index(drop=True)\n",
    "assert len(merged_df) == embeddings.shape[0], \"ID数量与嵌入数量不一致，请检查文件对应顺序！\"\n",
    "print(f\" 共载入 {len(merged_df)} 条候选序列用于聚类，嵌入 shape: {embeddings.shape}\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88d6fa0c-4ddf-42db-a0c2-b7bc883ede1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 对候选序列的 embedding 进行 KMeans 聚类，簇数: 40 ...\n",
      "聚类完成，样本已标注 Cluster 标签。\n",
      " 正在从每个类中选出代表性序列（每类随机1条）...\n",
      " 已保存最终 Top40 候选序列至：diverse_top40_by_cluster.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Mutations</th>\n",
       "      <th>PredictedBrightness</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate_22</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVKGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149E:K214Q:S30K:V163G:I136M:N135S</td>\n",
       "      <td>3.631204</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate_246</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149Q:Y145F:I128M:N135H:N121S</td>\n",
       "      <td>3.518986</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate_68</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>M153L:K214R:N212D:T203I:I136L:N135S</td>\n",
       "      <td>3.585208</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate_1</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "      <td>N149H:E222K:A206N:F130T:K140I:K126E</td>\n",
       "      <td>4.663651</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candidate_1335</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVNGEGEGDATYGKLTLKF...</td>\n",
       "      <td>V68G:L64K:S30N:A206M:N135H:N121D</td>\n",
       "      <td>3.410332</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sequence ID                                           Sequence  \\\n",
       "0    Candidate_22  MSKGEELFTGVVPILVELDGDVNGHKFSVKGEGEGDATYGKLTLKF...   \n",
       "1   Candidate_246  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "2    Candidate_68  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "3     Candidate_1  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...   \n",
       "4  Candidate_1335  MSKGEELFTGVVPILVELDGDVNGHKFSVNGEGEGDATYGKLTLKF...   \n",
       "\n",
       "                             Mutations  PredictedBrightness  Cluster  \n",
       "0   N149E:K214Q:S30K:V163G:I136M:N135S             3.631204        0  \n",
       "1        N149Q:Y145F:I128M:N135H:N121S             3.518986        1  \n",
       "2  M153L:K214R:N212D:T203I:I136L:N135S             3.585208        2  \n",
       "3  N149H:E222K:A206N:F130T:K140I:K126E             4.663651        3  \n",
       "4     V68G:L64K:S30N:A206M:N135H:N121D             3.410332        4  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 3. 执行聚类 ===\n",
    "n_clusters = 40\n",
    "print(f\"\\n 对候选序列的 embedding 进行 KMeans 聚类，簇数: {n_clusters} ...\")\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "merged_df[\"Cluster\"] = cluster_labels\n",
    "print(\"聚类完成，样本已标注 Cluster 标签。\")\n",
    "\n",
    "# === 4. 每类选出一个代表序列 ===\n",
    "print(\" 正在从每个类中选出代表性序列（每类随机1条）...\")\n",
    "top100_df = merged_df.groupby(\"Cluster\").apply(lambda x: x.sample(n=1, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# === 5. 保存结果 ===\n",
    "output_path = \"diverse_top40_by_cluster.csv\"\n",
    "top100_df.to_csv(output_path, index=False)\n",
    "print(f\" 已保存最终 Top40 候选序列至：{output_path}\")\n",
    "top100_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15b8b3a7-e56c-4f02-800b-358993098661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 FASTA 文件：top40_candidates.fasta\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"diverse_top40_by_cluster.csv\")\n",
    "\n",
    "with open(\"top40_candidates.fasta\", \"w\") as f:\n",
    "    for i, row in df.iterrows():\n",
    "        seq_id = row[\"Sequence ID\"]\n",
    "        seq = row[\"Sequence\"]\n",
    "        f.write(f\">{seq_id}\\n{seq}\\n\")\n",
    "\n",
    "print(\"已保存 FASTA 文件：top40_candidates.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b565835ad3c79",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  第九阶段:使用云计算平台ColabFold：https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/batch/AlphaFold2_batch.ipynb进行最后的结构筛选\n",
    "\n",
    "### ColabFold v1.5.5: AlphaFold2 w/ MMseqs2 BATCH\n",
    "\n",
    "![img](https://raw.githubusercontent.com/sokrypton/ColabFold/main/.github/ColabFold_Marv_Logo_Small.png)\n",
    "\n",
    "Easy to use AlphaFold2 protein structure [(Jumper et al. 2021)](https://www.google.com/url?q=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41586-021-03819-2) and complex [(Evans et al. 2021)](https://www.google.com/url?q=https%3A%2F%2Fwww.biorxiv.org%2Fcontent%2F10.1101%2F2021.10.04.463034v1) prediction using multiple sequence alignments generated through MMseqs2. For details, refer to our manuscript:\n",
    "\n",
    "[Mirdita M, Schütze K, Moriwaki Y, Heo L, Ovchinnikov S, Steinegger M. ColabFold: Making protein folding accessible to all. *Nature Methods*, 2022](https://www.google.com/url?q=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41592-022-01488-1)\n",
    "\n",
    "**Usage**\n",
    "\n",
    "`input_dir` directory with only fasta files or MSAs stored in Google Drive. MSAs need to be A3M formatted and have an `.a3m` extention. For MSAs MMseqs2 will not be called.\n",
    "\n",
    "`result_dir` results will be written to the result directory in Google Drive\n",
    "\n",
    "Old versions: [v1.4](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.4.0/batch/AlphaFold2_batch.ipynb), [v1.5.1](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.1/batch/AlphaFold2_batch.ipynb), [v1.5.2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.2/batch/AlphaFold2_batch.ipynb), [v1.5.3-patch](https://colab.research.google.com/github/sokrypton/ColabFold/blob/56c72044c7d51a311ca99b953a71e552fdc042e1/batch/AlphaFold2_batch.ipynb)\n",
    "\n",
    "**For more details, see [bottom](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/batch/AlphaFold2_batch.ipynb#Instructions) of the notebook and checkout the [ColabFold GitHub](https://github.com/sokrypton/ColabFold).**\n",
    "\n",
    "------\n",
    "\n",
    "### News\n",
    "\n",
    "- **2023/07/31: The ColabFold MSA server is back to normal. It was using older DB (UniRef30 2202/PDB70 220313) from 27th ~8:30 AM CEST to 31st ~11:10 AM CEST.**\n",
    "- **2023/06/12: New databases! UniRef30 updated to 2023_02 and PDB to 230517. We now use PDB100 instead of PDB70 (see notes in the [main](https://www.google.com/url?q=https%3A%2F%2Fcolabfold.com) notebook).**\n",
    "- **2023/06/12: We introduced a new default pairing strategy: Previously, for multimer predictions with more than 2 chains, we only pair if all sequences taxonomically match (\"complete\" pairing). The new default \"greedy\" strategy pairs any taxonomically matching subsets.**\n",
    "\n",
    "### 一、快速入门（Quick Start）\n",
    "\n",
    "1. **上传输入数据：**\n",
    "\n",
    "   - 将你要预测的 `.fasta` 文件上传到 Google Drive 中的一个文件夹（比如 `MyDrive/fastas/`）。\n",
    "   - ![bdb040cdcddf10d8e524ab4fdcaf82d](bdb040cdcddf10d8e524ab4fdcaf82d.jpg)\n",
    "\n",
    "2. **设置参数路径：**\n",
    "\n",
    "   - 在 Colab Notebook 中设置：\n",
    "\n",
    "     ```\n",
    "     input_dir = \"/content/drive/MyDrive/fastas\"\n",
    "     output_dir = \"/content/drive/MyDrive/results\"\n",
    "     ```\n",
    "\n",
    "3. **运行所有代码块：**\n",
    "\n",
    "   - 在菜单栏点击 `Runtime -> Run all`（运行时 -> 运行所有）。\n",
    "   - ![9f0b21e871d036241110a34a085d0c1](9f0b21e871d036241110a34a085d0c1.jpg)\n",
    "\n",
    "------\n",
    "\n",
    "###  二、结果输出（Result zip file contents）\n",
    "\n",
    "每一个预测结果都会被打包成一个 zip 文件 `jobname.result.zip`，内容包括：\n",
    "\n",
    "- ## 目录结构说明（`Candidate_22/`）\n",
    "\n",
    "- ![8b0ea0f3032a306017f3391a81efef6](8b0ea0f3032a306017f3391a81efef6.jpg)\n",
    "\n",
    "  | 文件或文件夹                                   | 类型     | 说明                                                        |\n",
    "  | ---------------------------------------------- | -------- | ----------------------------------------------------------- |\n",
    "  | `Candidate_22_env/`                            | 文件夹   | 多序列比对（MSA）环境相关中间文件。用于 AlphaFold2 模型输入 |\n",
    "  | `Candidate_22_coverage.png`                    | 图像     | 展示每个氨基酸位置匹配到多少条 MSA 序列（越高越好）         |\n",
    "  | `Candidate_22_pae.png`                         | 图像     | PAE 图，预测比对误差（预测的残基-残基对误差）               |\n",
    "  | `Candidate_22_plddt.png`                       | 图像     | pLDDT（结构置信度）图，值越高越可靠                         |\n",
    "  | `Candidate_22_predicted_aligned_error_v1.json` | JSON     | 每对残基的预测比对误差矩阵                                  |\n",
    "  | `Candidate_22_scores_rank_00X_*.json`          | JSON     | 每个模型预测的结构置信度评分信息，含各指标（pLDDT、PAE等）  |\n",
    "  | `Candidate_22_unrelaxed_rank_00X_*.pdb`        | 结构文件 | 五种模型结构（未进行 AMBER 能量最小化）                     |\n",
    "  | `Candidate_22.a3m`                             | 文本     | 最终使用的 A3M 格式的 MSA 输入文件                          |\n",
    "  | `Candidate_22.done.txt`                        | 文本     | 指示任务已完成的标志文件                                    |\n",
    "  | `log.txt`                                      | 文本     | 运行日志，记录参数、时间、模型等                            |\n",
    "  | `top_10_FinalSeq-xxxxxx.zip`                   | 压缩文件 | 历次任务汇总压缩结果包，每个 zip 包含多个候选结构文件及图表 |\n",
    "\n",
    "# 可视化解读\n",
    "\n",
    "## 图1：Predicted IDDT per position（每个位置的预测置信度）\n",
    "\n",
    "- **纵轴**：pLDDT（预测的局部距离差分测试得分，Predicted Local Distance Difference Test），量纲为 0～100，数值越高表示模型在该残基上的预测越可靠；\n",
    "- **横轴**：氨基酸的位置编号；\n",
    "- **多条曲线（rank_1 ~ rank_5）**：表示不同模型/seed 对同一蛋白预测结果的对比。\n",
    "\n",
    "#### 解读建议：\n",
    "\n",
    "- pLDDT > 90：高置信度；\n",
    "- 70~90：中等置信度；\n",
    "- < 70：低置信度，结构可变；\n",
    "- 图中有两个位置明显 pLDDT 下跌，可能是无序区、环状连接区或构象不稳定区；\n",
    "- 多个 rank 曲线整体高度一致 → 预测结果稳定可靠。\n",
    "\n",
    "------\n",
    "\n",
    "## 图2：Sequence Coverage（MSA覆盖图）\n",
    "\n",
    "- **纵轴**：MSA中匹配的序列编号（共约700多个）；\n",
    "- **横轴**：目标蛋白的氨基酸位置；\n",
    "- **颜色条**：表示每条匹配序列在该位置与目标序列的一致性（蓝色为高度一致，红色为不一致）；\n",
    "- **黑线**：每个位置被多少条序列覆盖（越高越好）；\n",
    "\n",
    "#### 解读建议：\n",
    "\n",
    "- 理想状态：黑线整体高于300，覆盖良好；\n",
    "- 图中间段（约70～90和150～180）有两个低谷 → 匹配序列变少、信号较弱 → 会影响结构预测精度；\n",
    "- 但整体覆盖图仍较密集，支持结构预测。\n",
    "\n",
    "------\n",
    "\n",
    "## 图3：Predicted Alignment Error（预测对齐误差 PAE）\n",
    "\n",
    "- 每一张图对应一个 rank（1~5），表示一个模型的预测结果；\n",
    "- 每个图是一个 2D 矩阵，横纵轴都是氨基酸位置；\n",
    "- 色条表示预测误差（单位 Å）：\n",
    "  - **深蓝色 (0-5Å)**：预测非常自信；\n",
    "  - **白到红 (15-30Å)**：不确定性较高；\n",
    "- 对角线：自对齐（无意义），你应关注**是否有大片红色区域** → 结构域间不确定。\n",
    "\n",
    "####  解读建议：\n",
    "\n",
    "- 每个图都基本呈现全蓝色，误差较低；\n",
    "- 下方靠近 C 端可能存在结构变动，但整体误差很低 → 可接受；\n",
    "- 多个 rank 一致性好 → 模型稳定。\n",
    "![img](str.png)\n",
    "\n",
    "\n",
    "## 蛋白结构候选序列打分与筛选方案说明\n",
    "\n",
    "### 1. 背景与目标\n",
    "\n",
    "在蛋白设计任务中，我们为每条候选序列（Candidate）预测了其三维结构，并评估其结构的可靠性与质量。为了从大量候选中筛选出结构最优的 Top 序列，需制定一套科学、可量化的打分与排序策略。\n",
    "\n",
    "------\n",
    "\n",
    "### 2. 打分指标来源\n",
    "\n",
    "我们使用 [AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2) 推理得到每个候选序列的 3D 结构，其结构质量通过以下两个核心指标反映：\n",
    "\n",
    "| 指标      | 全称                                     | 作用                                            |\n",
    "| --------- | ---------------------------------------- | ----------------------------------------------- |\n",
    "| **pLDDT** | predicted Local Distance Difference Test | 每个氨基酸位置的预测置信度（0~100，越高越可靠） |\n",
    "| **PAE**   | Predicted Alignment Error                | 不同残基之间距离预测误差的上界（越低越可靠）    |\n",
    "\n",
    "------\n",
    "\n",
    "### 3. 排序方法\n",
    "\n",
    "### 平均加权策略（Weighted Avg across 5 models）\n",
    "\n",
    "- 对每个 Candidate 的 5 个模型计算平均 pLDDT 和最大 PAE。\n",
    "\n",
    "- 综合打分计算方式示例：\n",
    "\n",
    "  Score=Avg(pLDDT)−α⋅Max(PAE)\\text{Score} = \\text{Avg(pLDDT)} - \\alpha \\cdot \\text{Max(PAE)}Score=Avg(pLDDT)−α⋅Max(PAE)\n",
    "\n",
    "  其中 α\\alphaα 为权重参数，经验上取 α=0.5\\alpha = 0.5α=0.5。\n",
    "\n",
    "- **优点**：\n",
    "\n",
    "  - 综合考虑结构整体可信度和跨残基精度；\n",
    "  - 抵抗偶发高分模型影响，更稳健；\n",
    "  - 更适合后续结构分析与实验验证。\n",
    "\n",
    "------\n",
    "\n",
    "### 4. 最终筛选流程\n",
    "\n",
    "1. **结构预测**：对每个序列使用 AlphaFold2 预测结构，保留 5 个模型结果；\n",
    "2. **结构打分**：提取每个模型的平均 pLDDT 和最大 PAE；\n",
    "3. **候选评分**：对每个 candidate 加权求平均后打分；\n",
    "4. **排序筛选**：按加权得分从高到低排序，选出 Top6；\n",
    "5. **合并信息**：将筛选出的结构结果与突变信息和完整序列进行整合。\n",
    "\n",
    "------\n",
    "\n",
    "### 5. 示例打分（候选 107）：\n",
    "\n",
    "| 模型     | Avg_pLDDT | Max_PAE   | 加权得分  |\n",
    "| -------- | --------- | --------- | --------- |\n",
    "| 模型1    | 96.22     | 29.66     | 81.39     |\n",
    "| 模型2    | 95.65     | 30.12     | 80.59     |\n",
    "| ...      | ...       | ...       | ...       |\n",
    "| **平均** | **95.93** | **29.57** | **81.15** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f86bea-2c93-4b8b-98ae-0cccbc2e24dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解压完成，文件保存于：/home/ma-user/work/top10_finalseq_unzipped\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 设置 ZIP 文件路径\n",
    "zip_path = '/home/ma-user/work/top_10_FinalSeq-20250601T022428Z-1-001.zip'\n",
    "\n",
    "# 设置解压目标目录\n",
    "output_dir = '/home/ma-user/work/top10_finalseq_unzipped'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 解压缩\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(output_dir)\n",
    "\n",
    "print(f\"解压完成，文件保存于：{output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14ba35e-5a3c-4425-ad15-b78c644f6900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Avg_pLDDT</th>\n",
       "      <th>Max_PAE</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>622</td>\n",
       "      <td>001</td>\n",
       "      <td>96.277941</td>\n",
       "      <td>29.656250</td>\n",
       "      <td>Candidate_622_scores_rank_001_alphafold2_ptm_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>001</td>\n",
       "      <td>96.227899</td>\n",
       "      <td>29.078125</td>\n",
       "      <td>Candidate_123_scores_rank_001_alphafold2_ptm_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>001</td>\n",
       "      <td>96.221555</td>\n",
       "      <td>29.656250</td>\n",
       "      <td>Candidate_107_scores_rank_001_alphafold2_ptm_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>823</td>\n",
       "      <td>001</td>\n",
       "      <td>96.221008</td>\n",
       "      <td>29.984375</td>\n",
       "      <td>Candidate_823_scores_rank_001_alphafold2_ptm_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>275</td>\n",
       "      <td>001</td>\n",
       "      <td>96.212017</td>\n",
       "      <td>29.953125</td>\n",
       "      <td>Candidate_275_scores_rank_001_alphafold2_ptm_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>640</td>\n",
       "      <td>001</td>\n",
       "      <td>96.211134</td>\n",
       "      <td>29.765625</td>\n",
       "      <td>Candidate_640_scores_rank_001_alphafold2_ptm_m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Candidate Rank  Avg_pLDDT    Max_PAE  \\\n",
       "22       622  001  96.277941  29.656250   \n",
       "3        123  001  96.227899  29.078125   \n",
       "1        107  001  96.221555  29.656250   \n",
       "30       823  001  96.221008  29.984375   \n",
       "11       275  001  96.212017  29.953125   \n",
       "23       640  001  96.211134  29.765625   \n",
       "\n",
       "                                             Filename  \n",
       "22  Candidate_622_scores_rank_001_alphafold2_ptm_m...  \n",
       "3   Candidate_123_scores_rank_001_alphafold2_ptm_m...  \n",
       "1   Candidate_107_scores_rank_001_alphafold2_ptm_m...  \n",
       "30  Candidate_823_scores_rank_001_alphafold2_ptm_m...  \n",
       "11  Candidate_275_scores_rank_001_alphafold2_ptm_m...  \n",
       "23  Candidate_640_scores_rank_001_alphafold2_ptm_m...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 假设所有 json 文件都在当前路径下的某个目录（你可替换为实际路径）\n",
    "search_dir = \"top10_finalseq_unzipped/top_10_FinalSeq\"\n",
    "\n",
    "# 遍历文件夹收集所有 candidate 的 json 文件路径\n",
    "json_files = []\n",
    "for root, dirs, files in os.walk(search_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\") and \"scores_rank\" in file:\n",
    "            json_files.append(os.path.join(root, file))\n",
    "\n",
    "# 提取每个文件的平均 plddt 和 max_pae\n",
    "results = []\n",
    "for path in json_files:\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    plddt = data.get(\"plddt\", [])\n",
    "    max_pae = data.get(\"max_pae\", None)\n",
    "    avg_plddt = sum(plddt) / len(plddt) if plddt else None\n",
    "\n",
    "    filename = os.path.basename(path)\n",
    "    candidate_id = filename.split(\"_\")[1]  # Candidate_1 -> 1\n",
    "    rank = filename.split(\"_rank_\")[1].split(\"_\")[0]  # 001, 002, ...\n",
    "    \n",
    "    results.append({\n",
    "        \"Candidate\": candidate_id,\n",
    "        \"Rank\": rank,\n",
    "        \"Avg_pLDDT\": avg_plddt,\n",
    "        \"Max_PAE\": max_pae,\n",
    "        \"Filename\": filename\n",
    "    })\n",
    "\n",
    "# 构造 DataFrame 并按 Candidate 分组，取每组中 plddt 最大的条目\n",
    "df = pd.DataFrame(results)\n",
    "top_per_candidate = df.sort_values([\"Candidate\", \"Avg_pLDDT\"], ascending=[True, False]).groupby(\"Candidate\").first().reset_index()\n",
    "\n",
    "# 再从所有 candidate 中选出前6个平均 plddt 最高的 candidate\n",
    "top6 = top_per_candidate.sort_values(\"Avg_pLDDT\", ascending=False).head(6)\n",
    "top6.to_csv(\"Final_Top6_Seq.csv\",index=False)\n",
    "top6.head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09d9389e-a841-4734-8c25-e393fbdcbeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate</th>\n",
       "      <th>Avg_pLDDT</th>\n",
       "      <th>Max_PAE</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>95.933412</td>\n",
       "      <td>29.571875</td>\n",
       "      <td>95.933412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>770</td>\n",
       "      <td>95.919521</td>\n",
       "      <td>29.631250</td>\n",
       "      <td>95.919521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>622</td>\n",
       "      <td>95.905218</td>\n",
       "      <td>29.709375</td>\n",
       "      <td>95.905218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>275</td>\n",
       "      <td>95.868664</td>\n",
       "      <td>29.875000</td>\n",
       "      <td>95.868664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>823</td>\n",
       "      <td>95.865445</td>\n",
       "      <td>29.890625</td>\n",
       "      <td>95.865445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>95.854462</td>\n",
       "      <td>29.331250</td>\n",
       "      <td>95.854462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Candidate  Avg_pLDDT    Max_PAE      Score\n",
       "1        107  95.933412  29.571875  95.933412\n",
       "27       770  95.919521  29.631250  95.919521\n",
       "22       622  95.905218  29.709375  95.905218\n",
       "11       275  95.868664  29.875000  95.868664\n",
       "30       823  95.865445  29.890625  95.865445\n",
       "3        123  95.854462  29.331250  95.854462"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 设置 JSON 文件目录\n",
    "search_dir = \"top10_finalseq_unzipped/top_10_FinalSeq\"\n",
    "\n",
    "# 收集所有 JSON 文件路径\n",
    "json_files = []\n",
    "for root, dirs, files in os.walk(search_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\") and \"scores_rank\" in file:\n",
    "            json_files.append(os.path.join(root, file))\n",
    "\n",
    "# 解析每个 JSON 文件\n",
    "records = []\n",
    "for path in json_files:\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    plddt = data.get(\"plddt\", [])\n",
    "    max_pae = data.get(\"max_pae\", None)\n",
    "    avg_plddt = sum(plddt) / len(plddt) if plddt else None\n",
    "\n",
    "    filename = os.path.basename(path)\n",
    "    # 修复 candidate ID 提取逻辑\n",
    "    candidate_id = filename.split(\"_scores_rank\")[0].replace(\"Candidate_\", \"\")\n",
    "    \n",
    "    records.append({\n",
    "        \"Candidate\": candidate_id,\n",
    "        \"Avg_pLDDT\": avg_plddt,\n",
    "        \"Max_PAE\": max_pae,\n",
    "        \"Filename\": filename\n",
    "    })\n",
    "\n",
    "# 转为 DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# 对每个 Candidate 聚合计算平均值（5个模型）\n",
    "agg_df = df.groupby(\"Candidate\").agg({\n",
    "    \"Avg_pLDDT\": \"mean\",\n",
    "    \"Max_PAE\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# 构造综合得分\n",
    "agg_df[\"Score\"] = agg_df[\"Avg_pLDDT\"]\n",
    "\n",
    "# 排序选出前6个\n",
    "top6 = agg_df.sort_values(\"Score\", ascending=False).head(6)\n",
    "\n",
    "# 输出为 CSV 文件\n",
    "top6.to_csv(\"/home/ma-user/work/Final_Top6_Weighted.csv\", index=False)\n",
    "\n",
    "top6.head(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab04bba5582af04",
   "metadata": {},
   "source": [
    "# 最后整理出Top6的Mutatuons和Sequence，我们的工作到此完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca40ff08-9aab-40aa-a90f-d38082179934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>Mutations</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate_107</td>\n",
       "      <td>V68L:E222Q:S30A:Y145L:N135S</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVAGEGEGDATYGKLTLKF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate_770</td>\n",
       "      <td>N212S:V68L:A206S:K131E:N121H</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate_622</td>\n",
       "      <td>M153A:V68L:Y145F:N135D:N121S</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate_275</td>\n",
       "      <td>V68L:S30K:Y145F:A206V:N135S</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVKGEGEGDATYGKLTLKF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candidate_823</td>\n",
       "      <td>N149D:E222A:A206E:I136T:N135H:K126E</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Candidate_123</td>\n",
       "      <td>V68L:Y145F:I128T:N135D:N121S</td>\n",
       "      <td>MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sequence ID                            Mutations  \\\n",
       "0  Candidate_107          V68L:E222Q:S30A:Y145L:N135S   \n",
       "1  Candidate_770         N212S:V68L:A206S:K131E:N121H   \n",
       "2  Candidate_622         M153A:V68L:Y145F:N135D:N121S   \n",
       "3  Candidate_275          V68L:S30K:Y145F:A206V:N135S   \n",
       "4  Candidate_823  N149D:E222A:A206E:I136T:N135H:K126E   \n",
       "5  Candidate_123         V68L:Y145F:I128T:N135D:N121S   \n",
       "\n",
       "                                            Sequence  \n",
       "0  MSKGEELFTGVVPILVELDGDVNGHKFSVAGEGEGDATYGKLTLKF...  \n",
       "1  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
       "2  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
       "3  MSKGEELFTGVVPILVELDGDVNGHKFSVKGEGEGDATYGKLTLKF...  \n",
       "4  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  \n",
       "5  MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKF...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取两个文件\n",
    "weighted_df = pd.read_csv(\"Final_Top6_Weighted.csv\")\n",
    "cluster_df = pd.read_csv(\"diverse_top40_by_cluster.csv\")\n",
    "\n",
    "# candidate 与 sequence id 关联，转换为字符串以确保匹配一致\n",
    "weighted_df[\"Candidate\"] = weighted_df[\"Candidate\"].astype(str)\n",
    "cluster_df[\"Sequence ID\"] = cluster_df[\"Sequence ID\"].astype(str)\n",
    "# 去除 cluster_df[\"Sequence ID\"] 的 \"Candidate_\" 前缀，使其与 weighted_df[\"Candidate\"] 一致\n",
    "cluster_df[\"Candidate_ID\"] = cluster_df[\"Sequence ID\"].str.replace(\"Candidate_\", \"\", regex=False)\n",
    "\n",
    "# 合并\n",
    "merged_df = pd.merge(weighted_df, cluster_df, left_on=\"Candidate\", right_on=\"Candidate_ID\", how=\"inner\")\n",
    "\n",
    "# 提取并重命名需要的列\n",
    "final_df = merged_df[[\"Sequence ID\", \"Mutations\", \"Sequence\"]]\n",
    "\n",
    "final_df.to_csv(\"FINAL_TOP6_Sequence_Confirmed\",index=False)\n",
    "\n",
    "final_df.head(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
