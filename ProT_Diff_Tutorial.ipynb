{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ProT-Diff æ¨¡å‹ - é¢å‘è¿‡ç¨‹ç¼–ç¨‹\n",
        "\n",
        "ç†è§£ProT-Diffæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç”¨å‡½æ•°å…ˆä»£æ›¿å¤æ‚çš„ç±»ç»“æ„ã€‚\n",
        "\n",
        "## æ¨¡å‹æ¦‚è¿°\n",
        "ProT-Diffæ˜¯ä¸€ä¸ªç”¨äºç”ŸæˆæŠ—èŒè‚½(AMP)çš„æ‰©æ•£æ¨¡å‹ï¼š\n",
        "1. ä½¿ç”¨ProtT5ç¼–ç å™¨å°†è‚½åºåˆ—è½¬æ¢ä¸ºåµŒå…¥å‘é‡\n",
        "2. åœ¨åµŒå…¥ç©ºé—´è®­ç»ƒæ‰©æ•£è¿‡ç¨‹\n",
        "3. ä»è®­ç»ƒå¥½çš„æ‰©æ•£æ¨¡å‹é‡‡æ ·æ–°çš„åµŒå…¥\n",
        "4. ä½¿ç”¨ProtT5è§£ç å™¨å°†åµŒå…¥è½¬æ¢å›æ°¨åŸºé…¸åºåˆ—\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬1æ­¥ï¼šç¯å¢ƒè®¾ç½®å’Œä¾èµ–å¯¼å…¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPyç‰ˆæœ¬: 1.26.4\n",
            "PyTorchç‰ˆæœ¬: 2.8.0+cu128\n",
            "Transformersç‰ˆæœ¬: 4.42.4\n",
            "ä½¿ç”¨è®¾å¤‡: cuda\n"
          ]
        }
      ],
      "source": [
        "# ç¯å¢ƒè®¾ç½®è¯´æ˜ï¼š\n",
        "# å¦‚æœé‡åˆ°numpyå…¼å®¹æ€§é—®é¢˜ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¿®å¤ï¼š\n",
        "# conda install numpy=1.24.3 scipy=1.10.1 scikit-learn=1.3.0 -c conda-forge --yes\n",
        "# å®‰è£…å¿…è¦çš„åŒ…ï¼ˆå¦‚æœè¿˜æ²¡å®‰è£…ï¼‰ï¼š\n",
        "# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "# conda install transformers accelerate einops pandas scikit-learn tqdm matplotlib -c conda-forge\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import T5Tokenizer, T5EncoderModel, T5ForConditionalGeneration\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "from einops import rearrange\n",
        "\n",
        "# æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§\n",
        "print(f\"NumPyç‰ˆæœ¬: {np.__version__}\")\n",
        "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"Transformersç‰ˆæœ¬: {transformers.__version__}\")\n",
        "except:\n",
        "    print(\"Transformersç‰ˆæœ¬: å¯¼å…¥å¤±è´¥\")\n",
        "\n",
        "# è®¾ç½®è®¾å¤‡\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
        "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬2æ­¥ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®\n",
        "\n",
        "æˆ‘ä»¬æ”¯æŒå¤šç§æ•°æ®æºï¼š\n",
        "- **æ¼”ç¤ºæ•°æ®**ï¼š8æ¡ç¤ºä¾‹åºåˆ—ï¼Œç”¨äºå¿«é€Ÿå­¦ä¹ \n",
        "- **AMPæ•°æ®é›†**ï¼šçœŸå®çš„æŠ—èŒè‚½åºåˆ—æ•°æ®ï¼Œç”¨äºfinetuning\n",
        "- **Non-AMPæ•°æ®é›†**ï¼šéæŠ—èŒè‚½åºåˆ—\n",
        "\n",
        "### æ•°æ®è¦æ±‚ï¼š\n",
        "- åºåˆ—é•¿åº¦ï¼š5-100ä¸ªæ°¨åŸºé…¸\n",
        "- åªåŒ…å«20ç§æ ‡å‡†æ°¨åŸºé…¸ï¼šACDEFGHIKLMNPQRSTVWY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: Cannot convert numpy.ndarray to numpy.ndarray\n",
            "å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•è¯»å–...\n",
            "å±•ç¤º 5 æ¡åºåˆ—ï¼š\n",
            "1. MLSCWKGYI (é•¿åº¦: 9)\n",
            "2. GSPPVFDRVVVNPQLYENRLNQTRLGT (é•¿åº¦: 27)\n",
            "3. MIIWLPPLVAAVTTYLLCEYLYYYGRDEH (é•¿åº¦: 29)\n",
            "4. MEIEDRIDLSERGHDTLEQKR (é•¿åº¦: 21)\n",
            "5. PVWIMAHMVNAVAQIDEFVNL (é•¿åº¦: 21)\n",
            "\n",
            "æ€»å…± 108776 æ¡åºåˆ—\n",
            "æœ€å¤§é•¿åº¦: 100\n",
            "æœ€å°é•¿åº¦: 4\n",
            "ä¸­ä½æ•°é•¿åº¦: 35.00\n",
            "å¹³å‡é•¿åº¦: 31.79\n"
          ]
        }
      ],
      "source": [
        "# ç¤ºä¾‹æŠ—èŒè‚½åºåˆ—ï¼ˆé•¿åº¦5-48ï¼ŒåªåŒ…å«20ç§æ°¨åŸºé…¸ï¼‰\n",
        "example_sequences = [\n",
        "    \"GIGKFLKKAKKFGKAFVKILKK\",  # 22ä¸ªæ°¨åŸºé…¸\n",
        "    \"KKLFKKILKYL\",             # 11ä¸ªæ°¨åŸºé…¸\n",
        "    \"GLFDIVKKVVGAL\",           # 13ä¸ªæ°¨åŸºé…¸\n",
        "    \"RWKIFKKIERVGQHTRDAT\",     # 19ä¸ªæ°¨åŸºé…¸\n",
        "    \"KWKLFKKIPKFLHLAKKF\",      # 18ä¸ªæ°¨åŸºé…¸\n",
        "    \"FLPIIAKLLSGLL\",           # 13ä¸ªæ°¨åŸºé…¸\n",
        "    \"KLAKLAKKLAKLAK\",          # 14ä¸ªæ°¨åŸºé…¸\n",
        "    \"GIGAVLKVLTTGLPALIS\"       # 18ä¸ªæ°¨åŸºé…¸\n",
        "]\n",
        "\n",
        "def load_sequences(use_example=True, dataset_path=None):\n",
        "    \"\"\"\n",
        "    é€‰æ‹©ç¤ºä¾‹æ•°æ®æˆ–åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†ã€‚\n",
        "    dataset_path éœ€è¦æ˜¯ä¸€ä¸ªåŒ…å«è‚½åºåˆ—çš„æ–‡ä»¶ï¼ˆcsvï¼‰ã€‚\n",
        "    \"\"\"\n",
        "    if use_example:\n",
        "        return example_sequences\n",
        "    else:\n",
        "        if dataset_path is None:\n",
        "            raise ValueError(\"è¯·æä¾› dataset_path å‚æ•°æ¥åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†\")\n",
        "        if dataset_path.endswith(\".csv\"):\n",
        "            try:\n",
        "                df = pd.read_csv(dataset_path)\n",
        "                # å¦‚æœæœ‰'sequence'åˆ—ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨ç¬¬ä¸€åˆ—\n",
        "                if 'sequence' in df.columns:\n",
        "                    sequences = df['sequence'].tolist()\n",
        "                else:\n",
        "                    sequences = df.iloc[:, 0].tolist()\n",
        "                # è¿‡æ»¤æ‰ç©ºå€¼å’Œéå­—ç¬¦ä¸²å€¼\n",
        "                sequences = [str(seq) for seq in sequences if pd.notna(seq) and str(seq).strip() != '']\n",
        "                return sequences\n",
        "            except Exception as e:\n",
        "                print(f\"è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
        "                print(\"å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•è¯»å–...\")\n",
        "                # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥æŒ‰è¡Œè¯»å–\n",
        "                with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "                    lines = f.readlines()\n",
        "                    # è·³è¿‡æ ‡é¢˜è¡Œï¼Œæå–ç¬¬ä¸€åˆ—\n",
        "                    sequences = []\n",
        "                    for i, line in enumerate(lines):\n",
        "                        if i == 0:  # è·³è¿‡æ ‡é¢˜\n",
        "                            continue\n",
        "                        parts = line.strip().split(',')\n",
        "                        if len(parts) > 0 and parts[0]:\n",
        "                            sequences.append(parts[0])\n",
        "                    return sequences\n",
        "        else:\n",
        "            raise ValueError(\"ç›®å‰ä»…æ”¯æŒ csv æ ¼å¼æ–‡ä»¶\")\n",
        "\n",
        "# ===== ä½¿ç”¨ç¤ºä¾‹æ•°æ® =====\n",
        "#sequences = load_sequences(use_example=True)\n",
        "\n",
        "# ===== æˆ–è€…ä½¿ç”¨ä½ è‡ªå·±çš„æ•°æ®é›† =====\n",
        "sequences = load_sequences(use_example=False, dataset_path=\"data/Non-AMP/final_non_amps.csv\")\n",
        "\n",
        "print(\"å±•ç¤º 5 æ¡åºåˆ—ï¼š\")\n",
        "sample_seqs = sequences[:5]\n",
        "for i, seq in enumerate(sample_seqs, 1):\n",
        "    print(f\"{i}. {seq} (é•¿åº¦: {len(seq)})\")\n",
        "\n",
        "# ç»Ÿè®¡ä¿¡æ¯\n",
        "lengths = [len(seq) for seq in sequences]\n",
        "total_count = len(sequences)\n",
        "max_len = max(lengths)\n",
        "min_len = min(lengths)\n",
        "median_len = np.median(lengths)\n",
        "mean_len = np.mean(lengths)\n",
        "print(f\"\\næ€»å…± {total_count} æ¡åºåˆ—\")\n",
        "print(f\"æœ€å¤§é•¿åº¦: {max_len}\")\n",
        "print(f\"æœ€å°é•¿åº¦: {min_len}\")\n",
        "print(f\"ä¸­ä½æ•°é•¿åº¦: {median_len:.2f}\")\n",
        "print(f\"å¹³å‡é•¿åº¦: {mean_len:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬3æ­¥ï¼šåŠ è½½ProtT5æ¨¡å‹\n",
        "\n",
        "ProtT5æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦åŠ è½½ç¼–ç å™¨å’Œè§£ç å™¨ï¼š\n",
        "- **ç¼–ç å™¨**ï¼šå°†æ°¨åŸºé…¸åºåˆ—è½¬æ¢ä¸ºåµŒå…¥å‘é‡\n",
        "- **è§£ç å™¨**ï¼šå°†åµŒå…¥å‘é‡è½¬æ¢å›æ°¨åŸºé…¸åºåˆ—\n",
        "cd /root/clash\n",
        "nohup ./clash -d . > clash.log 2>&1 &\n",
        "export http_proxy=http://127.0.0.1:7890\n",
        "export https_proxy=http://127.0.0.1:7890\n",
        "curl -I https://www.google.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# ====== é…ç½® ======\n",
        "MODEL_NAME = \"Rostlab/prot_t5_xl_half_uniref50-enc\"  # åŠç²¾åº¦ encoder-only\n",
        "TOKENIZER_NAME = \"Rostlab/prot_t5_xl_uniref50\"       # Tokenizer ç”¨å…¨é‡ç‰ˆ\n",
        "MAX_LEN = 48\n",
        "BATCH_SIZE = 16\n",
        "DEVICE = device  # ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„device\n",
        "\n",
        "# ====== å®ç”¨å‡½æ•° ======\n",
        "AA_20 = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "def clean_seq(seq: str) -> str:\n",
        "    s = re.sub(r\"[^A-Z]\", \"\", seq.upper())\n",
        "    s = \"\".join([c for c in s if c in AA_20])\n",
        "    return s\n",
        "\n",
        "def space_separate(seq: str) -> str:\n",
        "    return \" \".join(list(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "HF_HUB_ENABLE_HF_TRANSFER = 1\n",
            "HF transfer enabled?  False\n"
          ]
        }
      ],
      "source": [
        "# ==== 0) å®‰è£…ä¾èµ–ï¼ˆå·²è£…è¿‡å¯è·³è¿‡ï¼‰ ====\n",
        "%env HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "import os\n",
        "print(\"HF_HUB_ENABLE_HF_TRANSFER =\", os.environ.get(\"HF_HUB_ENABLE_HF_TRANSFER\"))\n",
        "\n",
        "from huggingface_hub import constants\n",
        "print(\"HF transfer enabled? \", constants.HF_HUB_ENABLE_HF_TRANSFER)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b2e19f36d244d93a9109e6de6b449ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model stored at: /root/autodl-tmp/prot_t5_xl_uniref50\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, time\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# 0) ç¼“å­˜ç›®å½•\n",
        "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/huggingface\"\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/huggingface/transformers\"\n",
        "pathlib.Path(os.environ[\"HF_HOME\"]).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(os.environ[\"TRANSFORMERS_CACHE\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) ä»£ç†\n",
        "proxy = \"http://127.0.0.1:7890\"\n",
        "for var in [\"HTTP_PROXY\",\"HTTPS_PROXY\",\"http_proxy\",\"https_proxy\"]:\n",
        "    os.environ[var] = proxy\n",
        "\n",
        "# 2) åŠ é€Ÿ + ç¦ç”¨ Xet\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"  # â† å¼ºåˆ¶ç¦ç”¨ Xet\n",
        "os.environ[\"HF_HUB_ETAG_TIMEOUT\"] = \"60\"\n",
        "\n",
        "# 3) ä¸‹è½½\n",
        "repo_id = \"Rostlab/prot_t5_xl_uniref50\"\n",
        "dest_dir = \"/root/autodl-tmp/prot_t5_xl_uniref50\"\n",
        "token = None  # å¦‚æœéœ€è¦ç™»å½•ï¼Œæ”¹æˆä½ çš„ token\n",
        "\n",
        "retries, delay_s = 5, 15\n",
        "last_err = None\n",
        "\n",
        "for i in range(1, retries + 1):\n",
        "    try:\n",
        "        local_dir = snapshot_download(\n",
        "            repo_id=repo_id,\n",
        "            local_dir=dest_dir,\n",
        "            resume_download=True,\n",
        "            max_workers=1,\n",
        "            etag_timeout=60,\n",
        "            token=token\n",
        "        )\n",
        "        print(f\"Model stored at: {local_dir}\")\n",
        "        last_err = None\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "        print(f\"[retry {i}/{retries}] {e}\")\n",
        "        time.sleep(delay_s)\n",
        "\n",
        "if last_err:\n",
        "    raise last_err\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ­£åœ¨åŠ è½½ ProtT5 ç¼–ç å™¨/Tokenizer ...\n",
            "âœ“ ProtT5 ç¼–ç å™¨åŠ è½½æˆåŠŸ\n",
            "âœ“ å‚æ•°é‡: 1,208,141,824\n",
            "âœ“ åµŒå…¥ç»´åº¦ d_model: 1024\n",
            "âœ“ ä½¿ç”¨è®¾å¤‡: cuda\n"
          ]
        }
      ],
      "source": [
        "# ==== 4) åŠ è½½ Tokenizer ä¸ Encoderï¼ˆç›´æ¥ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„ï¼‰ ====\n",
        "import re, math\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "\n",
        "# è®¾å¤‡\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæ›¿æ¢æˆä½ çš„å®é™…ä¸‹è½½ç›®å½•ï¼‰\n",
        "LOCAL_MODEL_DIR = \"/root/autodl-tmp/prot_t5_xl_uniref50\"\n",
        "\n",
        "print(\"æ­£åœ¨åŠ è½½ ProtT5 ç¼–ç å™¨/Tokenizer ...\")\n",
        "\n",
        "# ç›´æ¥ä»æœ¬åœ°è·¯å¾„åŠ è½½\n",
        "tokenizer = T5Tokenizer.from_pretrained(LOCAL_MODEL_DIR, do_lower_case=False)\n",
        "encoder_model = T5EncoderModel.from_pretrained(LOCAL_MODEL_DIR)\n",
        "\n",
        "# ç²¾åº¦ä¸è®¾å¤‡\n",
        "if device == \"cuda\":\n",
        "    # prot_t5_xl è¾ƒå¤§ï¼ŒåŠç²¾åº¦å¯æ˜¾è‘—çœæ˜¾å­˜\n",
        "    encoder_model = encoder_model.half().to(device)\n",
        "else:\n",
        "    encoder_model = encoder_model.float().to(device)\n",
        "\n",
        "encoder_model.eval()\n",
        "\n",
        "print(\"âœ“ ProtT5 ç¼–ç å™¨åŠ è½½æˆåŠŸ\")\n",
        "print(f\"âœ“ å‚æ•°é‡: {sum(p.numel() for p in encoder_model.parameters()):,}\")\n",
        "print(f\"âœ“ åµŒå…¥ç»´åº¦ d_model: {encoder_model.config.d_model}\")\n",
        "print(f\"âœ“ ä½¿ç”¨è®¾å¤‡: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬4æ­¥ï¼šåºåˆ—ç¼–ç  - å°†æ°¨åŸºé…¸åºåˆ—è½¬æ¢ä¸ºåµŒå…¥å‘é‡\n",
        "\n",
        "è¿™ä¸€æ­¥æˆ‘ä»¬è¦å®ç°ï¼š\n",
        "1. å•ä¸ªåºåˆ—ç¼–ç å‡½æ•°ï¼šå°†æ°¨åŸºé…¸åºåˆ—ç¼–ç ä¸º(L,1024)çš„åµŒå…¥\n",
        "2. é›¶å¡«å……åˆ°å›ºå®šé•¿åº¦ï¼šå¡«å……åˆ°(48,1024)çš„å›ºå®šå½¢çŠ¶\n",
        "3. æ‰¹é‡ç¼–ç ï¼šå¤„ç†æ‰€æœ‰è®­ç»ƒåºåˆ—\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[å•æ¡ç¼–ç è‡ªæµ‹]\n",
            "åºåˆ—: GIGKFLKKAKKFGKAFVKILKK\n",
            "çœŸå®é•¿åº¦: 22\n",
            "åµŒå…¥å½¢çŠ¶: (22, 1024)\n",
            "æ•°æ®ç±»å‹: torch.float16\n",
            "æ•°å€¼èŒƒå›´: [-0.893066, 0.850586]\n",
            "\n",
            "[æ‰¹é‡ç¼–ç è‡ªæµ‹]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "354d677151af428786c4958510f39fb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "æ‰¹é‡ç¼–ç :   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ç¼–ç äº† 3 ä¸ªåºåˆ—\n",
            "åºåˆ— 1: é•¿åº¦=22, åµŒå…¥å½¢çŠ¶=(22, 1024)\n",
            "  åŸå§‹åºåˆ—: GIGKFLKKAKKFGKAFVKIL...\n",
            "åºåˆ— 2: é•¿åº¦=11, åµŒå…¥å½¢çŠ¶=(11, 1024)\n",
            "  åŸå§‹åºåˆ—: KKLFKKILKYL\n",
            "åºåˆ— 3: é•¿åº¦=13, åµŒå…¥å½¢çŠ¶=(13, 1024)\n",
            "  åŸå§‹åºåˆ—: GLFDIVKKVVGAL\n",
            "\n",
            "[å‘åå…¼å®¹æ€§æµ‹è¯• - å›ºå®šé•¿åº¦å¡«å……]\n",
            "å›ºå®šé•¿åº¦åµŒå…¥å½¢çŠ¶: (3, 48, 1024)\n",
            "âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç°åœ¨ç¼–ç å‡½æ•°ä¼šä¿ç•™çœŸå®é•¿åº¦ä¿¡æ¯\n"
          ]
        }
      ],
      "source": [
        "# ====== å•åºåˆ—ç¼–ç ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰ ======\n",
        "@torch.inference_mode()\n",
        "def encode_sequence(sequence: str,\n",
        "                    tokenizer: T5Tokenizer,\n",
        "                    encoder_model: T5EncoderModel,\n",
        "                    device: str,\n",
        "                    max_length: int = MAX_LEN) -> tuple[torch.Tensor, int]:\n",
        "    \"\"\"\n",
        "    ç¼–ç å•ä¸ªåºåˆ—ï¼Œè¿”å›åµŒå…¥å’ŒçœŸå®é•¿åº¦\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (embeddings, real_length)\n",
        "            - embeddings: (real_length, d_model) çœŸå®é•¿åº¦çš„åµŒå…¥ï¼Œä¸åšå¡«å……\n",
        "            - real_length: int åºåˆ—çš„çœŸå®é•¿åº¦\n",
        "    \"\"\"\n",
        "    seq = clean_seq(sequence)\n",
        "    real_length = len(seq)\n",
        "    assert 1 <= real_length <= max_length, f\"åºåˆ—é•¿åº¦å¿…é¡» 1~{max_length}ï¼Œå½“å‰ {real_length}\"\n",
        "    \n",
        "    spaced = space_separate(seq)\n",
        "    inputs = tokenizer(\n",
        "        spaced,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length + 2,  # é¢„ç•™ special tokens\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = encoder_model(**inputs).last_hidden_state.squeeze(0)  # (seq_len_w_special, d_model)\n",
        "    \n",
        "    # å»æ‰é¦–å°¾ special tokensï¼Œä¿ç•™çœŸå®é•¿åº¦\n",
        "    outputs = outputs[1:real_length+1]  # (real_length, d_model)\n",
        "    \n",
        "    return outputs, real_length  # (real_length, d_model), int\n",
        "\n",
        "# ====== æ‰¹é‡ç¼–ç ï¼ˆæ¨èï¼‰ ======\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, sequences: List[str]):\n",
        "        self.seqs = [clean_seq(s) for s in sequences]\n",
        "        # è¿‡æ»¤è¿‡çŸ­/è¿‡é•¿\n",
        "        self.seqs = [s for s in self.seqs if 1 <= len(s) <= MAX_LEN]\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.seqs[idx]\n",
        "\n",
        "def collate_fn(batch: List[str]):\n",
        "    # æŠŠ batch çš„åºåˆ—æ‹¼æˆ spaced æ–‡æœ¬ï¼Œäº¤ç»™ tokenizer åš padding\n",
        "    spaced = [space_separate(s) for s in batch]\n",
        "    enc = tokenizer(\n",
        "        spaced,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN + 2,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    return enc, batch  # è¿”å›åŸåºåˆ—ä»¥ä¾¿è°ƒè¯•\n",
        "\n",
        "@torch.inference_mode()\n",
        "def encode_batch(sequences: List[str],\n",
        "                 encoder_model: T5EncoderModel,\n",
        "                 batch_size: int = BATCH_SIZE,\n",
        "                 device: str = DEVICE,\n",
        "                 max_length: int = MAX_LEN) -> tuple[List[torch.Tensor], List[int]]:\n",
        "    \"\"\"\n",
        "    æ‰¹é‡ç¼–ç åºåˆ—ï¼Œè¿”å›åµŒå…¥åˆ—è¡¨å’Œé•¿åº¦åˆ—è¡¨\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (embeddings_list, lengths_list)\n",
        "            - embeddings_list: List[torch.Tensor] æ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º (real_length, d_model)\n",
        "            - lengths_list: List[int] æ¯ä¸ªåºåˆ—çš„çœŸå®é•¿åº¦\n",
        "    \"\"\"\n",
        "    ds = SeqDataset(sequences)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    embs_list = []\n",
        "    lengths_list = []\n",
        "    \n",
        "    for inputs, raw_batch in tqdm(dl, desc=\"æ‰¹é‡ç¼–ç \", leave=False):\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        out = encoder_model(**inputs).last_hidden_state  # (B, Lw, d_model)\n",
        "        \n",
        "        # ä¸ºæ¯ä¸ªåºåˆ—å•ç‹¬å¤„ç†ï¼Œä¿ç•™çœŸå®é•¿åº¦\n",
        "        for i, raw_seq in enumerate(raw_batch):\n",
        "            real_length = len(raw_seq)\n",
        "            lengths_list.append(real_length)\n",
        "            \n",
        "            # å»æ‰ special tokensï¼Œä¿ç•™çœŸå®é•¿åº¦çš„åµŒå…¥\n",
        "            seq_emb = out[i, 1:real_length+1, :]  # (real_length, d_model)\n",
        "            \n",
        "            # ç§»åˆ° CPU èŠ‚çœæ˜¾å­˜\n",
        "            embs_list.append(seq_emb.float().cpu())\n",
        "    \n",
        "    return embs_list, lengths_list\n",
        "\n",
        "# ====== è¾…åŠ©å‡½æ•°ï¼šå¦‚éœ€å›ºå®šé•¿åº¦å¯ä½¿ç”¨æ­¤å‡½æ•° ======\n",
        "def pad_embeddings_to_fixed_length(embs_list: List[torch.Tensor], \n",
        "                                   lengths_list: List[int],\n",
        "                                   target_length: int = MAX_LEN) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    å°†å˜é•¿åµŒå…¥åˆ—è¡¨å¡«å……åˆ°å›ºå®šé•¿åº¦ï¼ˆå‘åå…¼å®¹ï¼‰\n",
        "    \n",
        "    Args:\n",
        "        embs_list: å˜é•¿åµŒå…¥åˆ—è¡¨\n",
        "        lengths_list: å¯¹åº”çš„é•¿åº¦åˆ—è¡¨\n",
        "        target_length: ç›®æ ‡é•¿åº¦\n",
        "        \n",
        "    Returns:\n",
        "        torch.Tensor: (N, target_length, d_model) å›ºå®šé•¿åº¦çš„åµŒå…¥å¼ é‡\n",
        "    \"\"\"\n",
        "    if not embs_list:\n",
        "        return torch.empty(0, target_length, embs_list[0].shape[-1])\n",
        "    \n",
        "    d_model = embs_list[0].shape[-1]\n",
        "    N = len(embs_list)\n",
        "    \n",
        "    # åˆ›å»ºå›ºå®šé•¿åº¦çš„å¼ é‡\n",
        "    padded_embs = torch.zeros(N, target_length, d_model, dtype=embs_list[0].dtype)\n",
        "    \n",
        "    for i, (emb, length) in enumerate(zip(embs_list, lengths_list)):\n",
        "        actual_length = min(length, target_length)\n",
        "        padded_embs[i, :actual_length] = emb[:actual_length]\n",
        "    \n",
        "    return padded_embs\n",
        "\n",
        "# ====== å¿«é€Ÿè‡ªæµ‹ ======\n",
        "if 'example_sequences' not in globals():\n",
        "    example_sequences = [\"GIGKFLKKAKKFGKAFVKILKK\", \"LLKKLLKKLLKKLL\"]\n",
        "if 'sequences' not in globals():\n",
        "    sequences = example_sequences * 10\n",
        "\n",
        "print(\"\\n[å•æ¡ç¼–ç è‡ªæµ‹]\")\n",
        "emb1, length1 = encode_sequence(example_sequences[0], tokenizer, encoder_model, DEVICE, MAX_LEN)\n",
        "print(f\"åºåˆ—: {example_sequences[0]}\")\n",
        "print(f\"çœŸå®é•¿åº¦: {length1}\")\n",
        "print(f\"åµŒå…¥å½¢çŠ¶: {tuple(emb1.shape)}\")\n",
        "print(f\"æ•°æ®ç±»å‹: {emb1.dtype}\")\n",
        "print(f\"æ•°å€¼èŒƒå›´: [{float(emb1.min()):.6f}, {float(emb1.max()):.6f}]\")\n",
        "\n",
        "print(\"\\n[æ‰¹é‡ç¼–ç è‡ªæµ‹]\")\n",
        "test_seqs = example_sequences[:3]  # æµ‹è¯•å‰3ä¸ªåºåˆ—\n",
        "embs_list, lengths_list = encode_batch(test_seqs, encoder_model, 2, DEVICE, MAX_LEN)\n",
        "print(f\"ç¼–ç äº† {len(embs_list)} ä¸ªåºåˆ—\")\n",
        "for i, (emb, length) in enumerate(zip(embs_list, lengths_list)):\n",
        "    print(f\"åºåˆ— {i+1}: é•¿åº¦={length}, åµŒå…¥å½¢çŠ¶={tuple(emb.shape)}\")\n",
        "    print(f\"  åŸå§‹åºåˆ—: {test_seqs[i][:20]}{'...' if len(test_seqs[i]) > 20 else ''}\")\n",
        "\n",
        "print(\"\\n[å‘åå…¼å®¹æ€§æµ‹è¯• - å›ºå®šé•¿åº¦å¡«å……]\")\n",
        "padded_embs = pad_embeddings_to_fixed_length(embs_list, lengths_list, MAX_LEN)\n",
        "print(f\"å›ºå®šé•¿åº¦åµŒå…¥å½¢çŠ¶: {tuple(padded_embs.shape)}\")\n",
        "print(\"âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç°åœ¨ç¼–ç å‡½æ•°ä¼šä¿ç•™çœŸå®é•¿åº¦ä¿¡æ¯\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… ç¼–ç å‡½æ•°ä¿®æ”¹å®Œæˆ\n",
        "\n",
        "### ğŸ”„ ä¸»è¦å˜åŒ–ï¼š\n",
        "\n",
        "1. **`encode_sequence()`** ç°åœ¨è¿”å› `(embeddings, real_length)`\n",
        "   - `embeddings`: å½¢çŠ¶ä¸º `(real_length, d_model)` çš„çœŸå®é•¿åº¦åµŒå…¥\n",
        "   - `real_length`: åºåˆ—çš„å®é™…é•¿åº¦\n",
        "\n",
        "2. **`encode_batch()`** ç°åœ¨è¿”å› `(embeddings_list, lengths_list)`\n",
        "   - `embeddings_list`: æ¯ä¸ªå…ƒç´ ä¸º `(real_length, d_model)` çš„åµŒå…¥åˆ—è¡¨\n",
        "   - `lengths_list`: å¯¹åº”çš„çœŸå®é•¿åº¦åˆ—è¡¨\n",
        "\n",
        "3. **æ–°å¢è¾…åŠ©å‡½æ•° `pad_embeddings_to_fixed_length()`**\n",
        "   - ç”¨äºå‘åå…¼å®¹ï¼Œå°†å˜é•¿åµŒå…¥å¡«å……åˆ°å›ºå®šé•¿åº¦\n",
        "   - è¿”å› `(N, target_length, d_model)` çš„å›ºå®šå½¢çŠ¶å¼ é‡\n",
        "\n",
        "### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹ï¼š\n",
        "\n",
        "```python\n",
        "# å•åºåˆ—ç¼–ç  - ä¿ç•™çœŸå®é•¿åº¦\n",
        "emb, length = encode_sequence(\"GIGKFLKKAKKF\", tokenizer, encoder_model, device)\n",
        "print(f\"åµŒå…¥å½¢çŠ¶: {emb.shape}\")  # (12, 1024) è€Œä¸æ˜¯ (48, 1024)\n",
        "\n",
        "# æ‰¹é‡ç¼–ç  - ä¿ç•™æ‰€æœ‰çœŸå®é•¿åº¦\n",
        "embs_list, lengths = encode_batch(sequences, encoder_model)\n",
        "# æ¯ä¸ª embs_list[i] çš„å½¢çŠ¶éƒ½æ˜¯ (lengths[i], 1024)\n",
        "\n",
        "# å¦‚éœ€å›ºå®šé•¿åº¦ï¼ˆå‘åå…¼å®¹ï¼‰\n",
        "fixed_embs = pad_embeddings_to_fixed_length(embs_list, lengths, 48)\n",
        "print(f\"å›ºå®šé•¿åº¦å½¢çŠ¶: {fixed_embs.shape}\")  # (N, 48, 1024)\n",
        "```\n",
        "\n",
        "### âœ¨ ä¼˜åŠ¿ï¼š\n",
        "\n",
        "- **å†…å­˜æ•ˆç‡**ï¼šä¸æµªè´¹ç©ºé—´åœ¨å¡«å……çš„é›¶å‘é‡ä¸Š\n",
        "- **çœŸå®ä¿¡æ¯**ï¼šä¿ç•™æ¯ä¸ªåºåˆ—çš„å®é™…é•¿åº¦\n",
        "- **çµæ´»æ€§**ï¼šå¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©æ˜¯å¦ä½¿ç”¨å›ºå®šé•¿åº¦\n",
        "- **å‘åå…¼å®¹**ï¼šæä¾›è¾…åŠ©å‡½æ•°æ”¯æŒåŸæœ‰çš„å›ºå®šé•¿åº¦éœ€æ±‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ­£åœ¨ç¼–ç æ‰€æœ‰è®­ç»ƒåºåˆ—...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0626ba0309ae46b6a76600c869602e80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "æ‰¹é‡ç¼–ç :   0%|          | 0/6231 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ æ‰€æœ‰åºåˆ—ç¼–ç å®Œæˆ!\n",
            "âœ“ ç¼–ç åºåˆ—æ•°é‡: 99687\n",
            "âœ“ é•¿åº¦ç»Ÿè®¡: æœ€å°=4, æœ€å¤§=48, å¹³å‡=30.0\n",
            "âœ“ æ•°æ®ç±»å‹: torch.float32\n",
            "âœ“ æ•°å€¼èŒƒå›´: [-1.900, 1.873]\n",
            "\n",
            "æ­£åœ¨ä¿å­˜åµŒå…¥æ•°æ®...\n",
            "âœ“ åµŒå…¥æ•°æ®å·²ä¿å­˜åˆ°: embedding_non_amp.pt\n",
            "âœ“ æ–‡ä»¶å¤§å°: 11678.0 MB\n"
          ]
        }
      ],
      "source": [
        "# æ‰¹é‡ç¼–ç æ‰€æœ‰è®­ç»ƒåºåˆ—\n",
        "print(\"æ­£åœ¨ç¼–ç æ‰€æœ‰è®­ç»ƒåºåˆ—...\")\n",
        "\n",
        "# ä½¿ç”¨é«˜æ•ˆæ‰¹é‡ç¼–ç ï¼ˆç®€åŒ–æ ¼å¼ï¼šç›´æ¥ä¿å­˜åµŒå…¥åˆ—è¡¨ï¼‰\n",
        "if os.path.exists(\"embedding_non_amp.pt\"):\n",
        "    train_embeddings_list = torch.load(\"embedding_non_amp.pt\", map_location=\"cpu\")\n",
        "    train_lengths = [emb.size(0) for emb in train_embeddings_list]  # ä»åµŒå…¥æ¨æ–­é•¿åº¦\n",
        "    print(f\"âœ“ ä»ç¼“å­˜åŠ è½½äº† {len(train_embeddings_list)} ä¸ªåºåˆ—çš„åµŒå…¥\")\n",
        "else:\n",
        "    train_embeddings_list, train_lengths = encode_batch(sequences, encoder_model, BATCH_SIZE, DEVICE, MAX_LEN)\n",
        "    print(f\"âœ“ æ‰€æœ‰åºåˆ—ç¼–ç å®Œæˆ!\")\n",
        "    print(f\"âœ“ ç¼–ç åºåˆ—æ•°é‡: {len(train_embeddings_list)}\")\n",
        "    print(f\"âœ“ é•¿åº¦ç»Ÿè®¡: æœ€å°={min(train_lengths)}, æœ€å¤§={max(train_lengths)}, å¹³å‡={sum(train_lengths)/len(train_lengths):.1f}\")\n",
        "    \n",
        "    # æ£€æŸ¥æ•°æ®ç±»å‹å’ŒèŒƒå›´\n",
        "    if train_embeddings_list:\n",
        "        sample_emb = train_embeddings_list[0]\n",
        "        print(f\"âœ“ æ•°æ®ç±»å‹: {sample_emb.dtype}\")\n",
        "        all_values = torch.cat(train_embeddings_list, dim=0)\n",
        "        print(f\"âœ“ æ•°å€¼èŒƒå›´: [{all_values.min():.3f}, {all_values.max():.3f}]\")\n",
        "\n",
        "    # ä¿å­˜åµŒå…¥æ•°æ®ï¼ˆç®€åŒ–æ ¼å¼ï¼šåªä¿å­˜åµŒå…¥åˆ—è¡¨ï¼‰\n",
        "    print(\"\\næ­£åœ¨ä¿å­˜åµŒå…¥æ•°æ®...\")\n",
        "    torch.save(train_embeddings_list, \"embedding_non_amp.pt\")\n",
        "    print(f\"âœ“ åµŒå…¥æ•°æ®å·²ä¿å­˜åˆ°: embedding_non_amp.pt\")\n",
        "    \n",
        "    # è®¡ç®—æ€»æ–‡ä»¶å¤§å°\n",
        "    total_elements = sum(emb.numel() for emb in train_embeddings_list)\n",
        "    element_size = train_embeddings_list[0].element_size() if train_embeddings_list else 4\n",
        "    file_size_mb = total_elements * element_size / 1024 / 1024\n",
        "    print(f\"âœ“ æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB\")\n",
        "\n",
        "# # å¦‚æœéœ€è¦å›ºå®šé•¿åº¦æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰\n",
        "# print(\"\\n[å¯é€‰] è½¬æ¢ä¸ºå›ºå®šé•¿åº¦æ ¼å¼:\")\n",
        "# train_embeddings_fixed = pad_embeddings_to_fixed_length(train_embeddings_list, train_lengths, MAX_LEN)\n",
        "# print(f\"âœ“ å›ºå®šé•¿åº¦åµŒå…¥å½¢çŠ¶: {train_embeddings_fixed.shape}\")\n",
        "# print(\"âœ“ ç°åœ¨æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨å˜é•¿æ ¼å¼ï¼ˆtrain_embeddings_list, train_lengthsï¼‰æˆ–å›ºå®šé•¿åº¦æ ¼å¼ï¼ˆtrain_embeddings_fixedï¼‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: Cannot convert numpy.ndarray to numpy.ndarray\n",
            "å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•è¯»å–...\n",
            "7956\n"
          ]
        }
      ],
      "source": [
        "sequence = load_sequences(use_example=False, dataset_path=\"data/AMP/final_AMP.csv\")\n",
        "print(len(sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ­£åœ¨ç¼–ç æ‰€æœ‰è®­ç»ƒåºåˆ—...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1caef3f37606401e856656fed444465b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "æ‰¹é‡ç¼–ç :   0%|          | 0/483 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ æ‰€æœ‰åºåˆ—ç¼–ç å®Œæˆ!\n",
            "âœ“ ç¼–ç åºåˆ—æ•°é‡: 7720\n",
            "âœ“ é•¿åº¦ç»Ÿè®¡: æœ€å°=5, æœ€å¤§=48, å¹³å‡=20.2\n",
            "âœ“ æ•°æ®ç±»å‹: torch.float32\n",
            "âœ“ æ•°å€¼èŒƒå›´: [-1.619, 1.542]\n",
            "\n",
            "æ­£åœ¨ä¿å­˜åµŒå…¥æ•°æ®...\n",
            "âœ“ åµŒå…¥æ•°æ®å·²ä¿å­˜åˆ°: embedding_amp.pt\n",
            "âœ“ æ–‡ä»¶å¤§å°: 610.4 MB\n"
          ]
        }
      ],
      "source": [
        "# æ‰¹é‡ç¼–ç æ‰€æœ‰è®­ç»ƒåºåˆ—\n",
        "print(\"æ­£åœ¨ç¼–ç æ‰€æœ‰è®­ç»ƒåºåˆ—...\")\n",
        "\n",
        "# ä½¿ç”¨é«˜æ•ˆæ‰¹é‡ç¼–ç ï¼ˆç®€åŒ–æ ¼å¼ï¼šç›´æ¥ä¿å­˜åµŒå…¥åˆ—è¡¨ï¼‰\n",
        "if os.path.exists(\"embedding_amp.pt\"):\n",
        "    train_embeddings_list = torch.load(\"embedding_amp.pt\", map_location=\"cpu\")\n",
        "    train_lengths = [emb.size(0) for emb in train_embeddings_list]  # ä»åµŒå…¥æ¨æ–­é•¿åº¦\n",
        "    print(f\"âœ“ ä»ç¼“å­˜åŠ è½½äº† {len(train_embeddings_list)} ä¸ªåºåˆ—çš„åµŒå…¥\")\n",
        "else:\n",
        "    train_embeddings_list, train_lengths = encode_batch(sequence, encoder_model, BATCH_SIZE, DEVICE, MAX_LEN)\n",
        "    print(f\"âœ“ æ‰€æœ‰åºåˆ—ç¼–ç å®Œæˆ!\")\n",
        "    print(f\"âœ“ ç¼–ç åºåˆ—æ•°é‡: {len(train_embeddings_list)}\")\n",
        "    print(f\"âœ“ é•¿åº¦ç»Ÿè®¡: æœ€å°={min(train_lengths)}, æœ€å¤§={max(train_lengths)}, å¹³å‡={sum(train_lengths)/len(train_lengths):.1f}\")\n",
        "    \n",
        "    # æ£€æŸ¥æ•°æ®ç±»å‹å’ŒèŒƒå›´\n",
        "    if train_embeddings_list:\n",
        "        sample_emb = train_embeddings_list[0]\n",
        "        print(f\"âœ“ æ•°æ®ç±»å‹: {sample_emb.dtype}\")\n",
        "        all_values = torch.cat(train_embeddings_list, dim=0)\n",
        "        print(f\"âœ“ æ•°å€¼èŒƒå›´: [{all_values.min():.3f}, {all_values.max():.3f}]\")\n",
        "\n",
        "    # ä¿å­˜åµŒå…¥æ•°æ®ï¼ˆç®€åŒ–æ ¼å¼ï¼šåªä¿å­˜åµŒå…¥åˆ—è¡¨ï¼‰\n",
        "    print(\"\\næ­£åœ¨ä¿å­˜åµŒå…¥æ•°æ®...\")\n",
        "    torch.save(train_embeddings_list, \"embedding_amp.pt\")\n",
        "    print(f\"âœ“ åµŒå…¥æ•°æ®å·²ä¿å­˜åˆ°: embedding_amp.pt\")\n",
        "    \n",
        "    # è®¡ç®—æ€»æ–‡ä»¶å¤§å°\n",
        "    total_elements = sum(emb.numel() for emb in train_embeddings_list)\n",
        "    element_size = train_embeddings_list[0].element_size() if train_embeddings_list else 4\n",
        "    file_size_mb = total_elements * element_size / 1024 / 1024\n",
        "    print(f\"âœ“ æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬5æ­¥ï¼šç†è§£æ‰©æ•£è¿‡ç¨‹åŸºç¡€\n",
        "\n",
        "æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³ï¼š\n",
        "1. **å‰å‘è¿‡ç¨‹**ï¼šé€æ­¥å‘å¹²å‡€æ•°æ®æ·»åŠ å™ªå£°ï¼Œç›´åˆ°å˜æˆçº¯å™ªå£°\n",
        "2. **åå‘è¿‡ç¨‹**ï¼šè®­ç»ƒç¥ç»ç½‘ç»œå­¦ä¹ ä»å™ªå£°ä¸­æ¢å¤æ•°æ®\n",
        "\n",
        "æˆ‘ä»¬éœ€è¦ç†è§£ï¼š\n",
        "- å™ªå£°è°ƒåº¦ï¼ˆsqrt scheduleï¼‰\n",
        "- å‰å‘æ‰©æ•£å…¬å¼\n",
        "- å¦‚ä½•å¯è§†åŒ–å™ªå£°æ·»åŠ è¿‡ç¨‹\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬6æ­¥ï¼šæ„å»ºUNetå»å™ªç½‘ç»œ\n",
        "\n",
        "æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ª1D UNetæ¥å­¦ä¹ å»å™ªè¿‡ç¨‹ï¼š\n",
        "\n",
        "**ç½‘ç»œç»„ä»¶**ï¼š\n",
        "- æ—¶é—´åµŒå…¥ï¼ˆTime Embeddingï¼‰ï¼šè®©æ¨¡å‹çŸ¥é“å½“å‰å¤„äºå“ªä¸ªæ‰©æ•£æ­¥éª¤\n",
        "- æ®‹å·®å—ï¼ˆResBlockï¼‰ï¼šåŸºæœ¬çš„å·ç§¯æ„å»ºå—\n",
        "- è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰ï¼šåœ¨ç“¶é¢ˆå±‚å¢å¼ºç‰¹å¾è¡¨è¾¾\n",
        "- Uå‹ç»“æ„ï¼šç¼–ç å™¨-ç“¶é¢ˆ-è§£ç å™¨ï¼Œå¸¦è·³è·ƒè¿æ¥\n",
        "\n",
        "**æ¨¡å‹æ¶æ„**ï¼šTrans-UNeté£æ ¼çš„1Dç‰ˆæœ¬\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬7æ­¥ï¼šè®­ç»ƒæ‰©æ•£æ¨¡å‹\n",
        "\n",
        "è®­ç»ƒè¿‡ç¨‹çš„æ ¸å¿ƒï¼š\n",
        "1. **éšæœºé‡‡æ ·æ—¶é—´æ­¥**ï¼šä»0åˆ°2000ä¸­éšæœºé€‰æ‹©\n",
        "2. **æ·»åŠ å™ªå£°**ï¼šæ ¹æ®æ—¶é—´æ­¥å‘åŸå§‹æ•°æ®æ·»åŠ ç›¸åº”å¼ºåº¦çš„å™ªå£°\n",
        "3. **æ¨¡å‹é¢„æµ‹**ï¼šUNeté¢„æµ‹åŸå§‹æ•°æ®x0ï¼ˆè€Œä¸æ˜¯å™ªå£°ï¼‰\n",
        "4. **è®¡ç®—æŸå¤±**ï¼šMSEæŸå¤±ï¼Œæ¯”è¾ƒé¢„æµ‹çš„x0å’ŒçœŸå®x0\n",
        "5. **åå‘ä¼ æ’­**ï¼šæ›´æ–°æ¨¡å‹å‚æ•°\n",
        "\n",
        "**è®­ç»ƒé…ç½®**ï¼š\n",
        "- æ‰©æ•£æ­¥æ•°ï¼š2000æ­¥\n",
        "- é¢„æµ‹ç›®æ ‡ï¼šx0ï¼ˆåŸå§‹æ•°æ®ï¼‰\n",
        "- å™ªå£°è°ƒåº¦ï¼šsqrt schedule\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬8æ­¥ï¼šDDPMé‡‡æ ·ç”Ÿæˆæ–°åºåˆ—\n",
        "\n",
        "ä»è®­ç»ƒå¥½çš„æ‰©æ•£æ¨¡å‹é‡‡æ ·ç”Ÿæˆæ–°çš„åµŒå…¥å‘é‡ï¼š\n",
        "\n",
        "**é‡‡æ ·è¿‡ç¨‹**ï¼š\n",
        "1. **ä»çº¯å™ªå£°å¼€å§‹**ï¼šéšæœºåˆå§‹åŒ–(batch_size, 48, 1024)çš„å™ªå£°å¼ é‡\n",
        "2. **é€æ­¥å»å™ª**ï¼šä»æ—¶é—´æ­¥2000åˆ°0ï¼Œé€æ­¥å»é™¤å™ªå£°\n",
        "3. **ä¸‹é‡‡æ ·**ï¼šå°†2000æ­¥å‹ç¼©åˆ°200æ­¥ä»¥åŠ é€Ÿé‡‡æ ·\n",
        "4. **å™ªå£°é€‰æ‹©**ï¼šå¯ä»¥é€‰æ‹©é«˜æ–¯å™ªå£°æˆ–å‡åŒ€å™ªå£°\n",
        "\n",
        "**DDPMç®—æ³•**ï¼šä½¿ç”¨æ ‡å‡†çš„DDPMé€†å‘é‡‡æ ·å…¬å¼\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬9æ­¥ï¼šåºåˆ—è§£ç  - å°†åµŒå…¥è½¬æ¢å›æ°¨åŸºé…¸åºåˆ—\n",
        "\n",
        "ä½¿ç”¨ProtT5è§£ç å™¨å°†ç”Ÿæˆçš„åµŒå…¥å‘é‡è½¬æ¢å›æ°¨åŸºé…¸åºåˆ—ï¼š\n",
        "\n",
        "**è§£ç è¿‡ç¨‹**ï¼š\n",
        "1. **å»é™¤é›¶å¡«å……**ï¼šè¯†åˆ«å¹¶ç§»é™¤ä¸»è¦ä¸ºé›¶çš„è¡Œï¼ˆpaddingéƒ¨åˆ†ï¼‰\n",
        "2. **åŒ…è£…ä¸ºç¼–ç å™¨è¾“å‡º**ï¼šå°†åµŒå…¥åŒ…è£…æˆProtT5æœŸæœ›çš„æ ¼å¼\n",
        "3. **è°ƒç”¨è§£ç å™¨**ï¼šä½¿ç”¨T5çš„generateæ–¹æ³•ç”Ÿæˆtokenåºåˆ—\n",
        "4. **åå¤„ç†**ï¼šæ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬ï¼Œåªä¿ç•™20ç§æ ‡å‡†æ°¨åŸºé…¸\n",
        "\n",
        "**æ³¨æ„äº‹é¡¹**ï¼š\n",
        "- è§£ç å¯èƒ½ä¸å®Œç¾ï¼Œéœ€è¦è¿‡æ»¤æ— æ•ˆåºåˆ—\n",
        "- ç”Ÿæˆçš„åºåˆ—é•¿åº¦å¯èƒ½ä¸åŸå§‹åºåˆ—ä¸åŒ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬10æ­¥ï¼šç»“æœåˆ†æå’Œå¯è§†åŒ–\n",
        "\n",
        "åˆ†æç”Ÿæˆçš„åºåˆ—è´¨é‡ï¼š\n",
        "\n",
        "**è´¨é‡è¯„ä¼°**ï¼š\n",
        "1. **åŸºæœ¬ç»Ÿè®¡**ï¼šåºåˆ—é•¿åº¦åˆ†å¸ƒã€æ°¨åŸºé…¸ç»„æˆ\n",
        "2. **ä¸è®­ç»ƒé›†å¯¹æ¯”**ï¼šæ¯”è¾ƒç”Ÿæˆåºåˆ—å’ŒåŸå§‹è®­ç»ƒåºåˆ—çš„ç‰¹å¾\n",
        "3. **æœ‰æ•ˆæ€§æ£€æŸ¥**ï¼šè¿‡æ»¤æ‰æ— æ•ˆæˆ–è¿‡çŸ­çš„åºåˆ—\n",
        "4. **å¤šæ ·æ€§åˆ†æ**ï¼šæ£€æŸ¥ç”Ÿæˆåºåˆ—çš„å¤šæ ·æ€§\n",
        "\n",
        "**å¯è§†åŒ–**ï¼š\n",
        "- è®­ç»ƒæŸå¤±æ›²çº¿\n",
        "- ç”Ÿæˆåºåˆ—é•¿åº¦åˆ†å¸ƒ\n",
        "- æ°¨åŸºé…¸ç»„æˆçƒ­å›¾\n",
        "- è®­ç»ƒé›†vsç”Ÿæˆé›†å¯¹æ¯”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ€»ç»“å’Œä¸‹ä¸€æ­¥\n",
        "\n",
        "### ğŸ¯ æˆ‘ä»¬å®Œæˆçš„å·¥ä½œï¼š\n",
        "\n",
        "1. **æ•°æ®å‡†å¤‡**ï¼šå‡†å¤‡äº†æŠ—èŒè‚½åºåˆ—ä½œä¸ºè®­ç»ƒæ•°æ®\n",
        "2. **åºåˆ—ç¼–ç **ï¼šä½¿ç”¨ProtT5ç¼–ç å™¨å°†æ°¨åŸºé…¸åºåˆ—è½¬æ¢ä¸º1024ç»´åµŒå…¥å‘é‡\n",
        "3. **æ‰©æ•£å»ºæ¨¡**ï¼šç†è§£äº†æ‰©æ•£è¿‡ç¨‹çš„å‰å‘å’Œåå‘è¿‡ç¨‹\n",
        "4. **ç½‘ç»œæ¶æ„**ï¼šæ„å»ºäº†Trans-UNeté£æ ¼çš„1Då»å™ªç½‘ç»œ\n",
        "5. **æ¨¡å‹è®­ç»ƒ**ï¼šè®­ç»ƒæ‰©æ•£æ¨¡å‹å­¦ä¹ ä»å™ªå£°ä¸­æ¢å¤åºåˆ—åµŒå…¥\n",
        "6. **åºåˆ—ç”Ÿæˆ**ï¼šä½¿ç”¨DDPMé‡‡æ ·ç”Ÿæˆæ–°çš„åºåˆ—åµŒå…¥\n",
        "7. **åºåˆ—è§£ç **ï¼šå°†åµŒå…¥è½¬æ¢å›æ°¨åŸºé…¸åºåˆ—\n",
        "8. **ç»“æœè¯„ä¼°**ï¼šåˆ†æç”Ÿæˆåºåˆ—çš„è´¨é‡å’Œå¤šæ ·æ€§\n",
        "\n",
        "### ğŸš€ æ”¹è¿›æ–¹å‘ï¼š\n",
        "\n",
        "1. **æ›´å¤šæ•°æ®**ï¼šä½¿ç”¨æ›´å¤§çš„AMPæ•°æ®é›†è®­ç»ƒ\n",
        "2. **æ¨¡å‹ä¼˜åŒ–**ï¼šè°ƒæ•´ç½‘ç»œæ¶æ„å’Œè¶…å‚æ•°\n",
        "3. **æ¡ä»¶ç”Ÿæˆ**ï¼šæ ¹æ®ç‰¹å®šæ€§è´¨ï¼ˆå¦‚é•¿åº¦ã€æ´»æ€§ï¼‰ç”Ÿæˆåºåˆ—\n",
        "4. **è´¨é‡ç­›é€‰**ï¼šæ·»åŠ åˆ¤åˆ«å™¨æˆ–è§„åˆ™ç­›é€‰é«˜è´¨é‡åºåˆ—\n",
        "5. **é¢„è®­ç»ƒå¾®è°ƒ**ï¼šå®ç°è®ºæ–‡ä¸­çš„é¢„è®­ç»ƒ+å¾®è°ƒç­–ç•¥\n",
        "\n",
        "### ğŸ“š è¿›ä¸€æ­¥å­¦ä¹ ï¼š\n",
        "\n",
        "- æ‰©æ•£æ¨¡å‹çš„æ•°å­¦åŸç†\n",
        "- è›‹ç™½è´¨åºåˆ—çš„ç”Ÿç‰©å­¦ç‰¹æ€§\n",
        "- æŠ—èŒè‚½çš„ç»“æ„-åŠŸèƒ½å…³ç³»\n",
        "- æ›´é«˜çº§çš„ç”Ÿæˆæ¨¡å‹æŠ€æœ¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ğŸ› ï¸ å¦‚æœæ¨¡å‹åŠ è½½ä»ç„¶å¤±è´¥ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹æ‰‹åŠ¨è§£å†³æ–¹æ¡ˆï¼š\n",
        "\n",
        "# æ–¹æ¡ˆ1: æ‰‹åŠ¨æ¸…ç† transformers ç¼“å­˜\n",
        "def manual_cache_cleanup():\n",
        "    \"\"\"æ‰‹åŠ¨æ¸…ç†æ‰€æœ‰ transformers ç¼“å­˜\"\"\"\n",
        "    import shutil\n",
        "    from transformers import TRANSFORMERS_CACHE\n",
        "    \n",
        "    print(f\"Transformers ç¼“å­˜ç›®å½•: {TRANSFORMERS_CACHE}\")\n",
        "    \n",
        "    if os.path.exists(TRANSFORMERS_CACHE):\n",
        "        print(\"æ­£åœ¨æ¸…ç†ç¼“å­˜...\")\n",
        "        try:\n",
        "            shutil.rmtree(TRANSFORMERS_CACHE)\n",
        "            print(\"âœ“ ç¼“å­˜æ¸…ç†æˆåŠŸ\")\n",
        "            os.makedirs(TRANSFORMERS_CACHE, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}\")\n",
        "    else:\n",
        "        print(\"ç¼“å­˜ç›®å½•ä¸å­˜åœ¨\")\n",
        "\n",
        "# æ–¹æ¡ˆ2: æ£€æŸ¥ç½‘ç»œè¿æ¥\n",
        "def test_huggingface_connection():\n",
        "    \"\"\"æµ‹è¯•åˆ° Hugging Face çš„è¿æ¥\"\"\"\n",
        "    import requests\n",
        "    \n",
        "    urls_to_test = [\n",
        "        \"https://huggingface.co\",\n",
        "        \"https://hf-mirror.com\",\n",
        "        \"https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
        "    ]\n",
        "    \n",
        "    for url in urls_to_test:\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                print(f\"âœ“ è¿æ¥æˆåŠŸ: {url}\")\n",
        "            else:\n",
        "                print(f\"âŒ è¿æ¥å¤±è´¥: {url} (çŠ¶æ€ç : {response.status_code})\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ è¿æ¥å¤±è´¥: {url} (é”™è¯¯: {e})\")\n",
        "\n",
        "# æ–¹æ¡ˆ3: ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚æœå·²ä¸‹è½½ï¼‰\n",
        "def try_local_model():\n",
        "    \"\"\"å°è¯•ä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹\"\"\"\n",
        "    from transformers import TRANSFORMERS_CACHE\n",
        "    \n",
        "    local_paths = [\n",
        "        os.path.join(TRANSFORMERS_CACHE, \"models--Rostlab--prot_t5_xl_half_uniref50-enc\"),\n",
        "        os.path.join(TRANSFORMERS_CACHE, \"models--Rostlab--prot_t5_xl_uniref50\"),\n",
        "        \"/root/.cache/huggingface/transformers/models--Rostlab--prot_t5_xl_half_uniref50-enc\",\n",
        "        \"/root/.cache/huggingface/transformers/models--Rostlab--prot_t5_xl_uniref50\"\n",
        "    ]\n",
        "    \n",
        "    print(\"æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶...\")\n",
        "    for path in local_paths:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"âœ“ æ‰¾åˆ°æœ¬åœ°æ¨¡å‹: {path}\")\n",
        "            # åˆ—å‡ºæ–‡ä»¶\n",
        "            try:\n",
        "                files = os.listdir(path)\n",
        "                print(f\"  æ–‡ä»¶: {files[:10]}...\")  # æ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶\n",
        "            except:\n",
        "                pass\n",
        "        else:\n",
        "            print(f\"âŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {path}\")\n",
        "\n",
        "print(\"=== è¯Šæ–­å·¥å…· ===\")\n",
        "print(\"\\n1. æµ‹è¯•ç½‘ç»œè¿æ¥:\")\n",
        "test_huggingface_connection()\n",
        "\n",
        "print(\"\\n2. æ£€æŸ¥æœ¬åœ°æ¨¡å‹:\")\n",
        "try_local_model()\n",
        "\n",
        "print(\"\\n3. å¦‚éœ€æ¸…ç†ç¼“å­˜ï¼Œå–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ä»£ç :\")\n",
        "print(\"# manual_cache_cleanup()\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
