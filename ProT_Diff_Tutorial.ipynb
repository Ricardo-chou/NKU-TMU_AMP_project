{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ProT-Diff 模型 - 面向过程编程\n",
        "\n",
        "理解ProT-Diff模型的训练过程，用函数先代替复杂的类结构。\n",
        "\n",
        "## 模型概述\n",
        "ProT-Diff是一个用于生成抗菌肽(AMP)的扩散模型：\n",
        "1. 使用ProtT5编码器将肽序列转换为嵌入向量\n",
        "2. 在嵌入空间训练扩散过程\n",
        "3. 从训练好的扩散模型采样新的嵌入\n",
        "4. 使用ProtT5解码器将嵌入转换回氨基酸序列\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第1步：环境设置和依赖导入\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy版本: 1.24.3\n",
            "PyTorch版本: 2.6.0\n",
            "Transformers版本: 4.55.1\n",
            "使用设备: mps\n"
          ]
        }
      ],
      "source": [
        "# 环境设置说明：\n",
        "# 如果遇到numpy兼容性问题，请运行以下命令修复：\n",
        "# conda install numpy=1.24.3 scipy=1.10.1 scikit-learn=1.3.0 -c conda-forge --yes\n",
        "# 安装必要的包（如果还没安装）：\n",
        "# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "# conda install transformers accelerate einops pandas scikit-learn tqdm matplotlib -c conda-forge\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import T5Tokenizer, T5EncoderModel, T5ForConditionalGeneration\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "from einops import rearrange\n",
        "\n",
        "# 检查版本兼容性\n",
        "print(f\"NumPy版本: {np.__version__}\")\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"Transformers版本: {transformers.__version__}\")\n",
        "except:\n",
        "    print(\"Transformers版本: 导入失败\")\n",
        "\n",
        "# 设置设备\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
        "print(f\"使用设备: {device}\")\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第2步：准备训练数据\n",
        "\n",
        "我们支持多种数据源：\n",
        "- **演示数据**：8条示例序列，用于快速学习\n",
        "- **AMP数据集**：真实的抗菌肽序列数据，用于finetuning\n",
        "- **Non-AMP数据集**：非抗菌肽序列\n",
        "\n",
        "### 数据要求：\n",
        "- 序列长度：5-100个氨基酸\n",
        "- 只包含20种标准氨基酸：ACDEFGHIKLMNPQRSTVWY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "读取CSV文件时出错: Cannot convert numpy.ndarray to numpy.ndarray\n",
            "尝试使用更简单的方法读取...\n",
            "展示 5 条序列：\n",
            "1. MLSCWKGYI (长度: 9)\n",
            "2. GSPPVFDRVVVNPQLYENRLNQTRLGT (长度: 27)\n",
            "3. MIIWLPPLVAAVTTYLLCEYLYYYGRDEH (长度: 29)\n",
            "4. MEIEDRIDLSERGHDTLEQKR (长度: 21)\n",
            "5. PVWIMAHMVNAVAQIDEFVNL (长度: 21)\n",
            "\n",
            "总共 108776 条序列\n",
            "最大长度: 100\n",
            "最小长度: 4\n",
            "中位数长度: 35.00\n",
            "平均长度: 31.79\n"
          ]
        }
      ],
      "source": [
        "# 示例抗菌肽序列（长度5-48，只包含20种氨基酸）\n",
        "example_sequences = [\n",
        "    \"GIGKFLKKAKKFGKAFVKILKK\",  # 22个氨基酸\n",
        "    \"KKLFKKILKYL\",             # 11个氨基酸\n",
        "    \"GLFDIVKKVVGAL\",           # 13个氨基酸\n",
        "    \"RWKIFKKIERVGQHTRDAT\",     # 19个氨基酸\n",
        "    \"KWKLFKKIPKFLHLAKKF\",      # 18个氨基酸\n",
        "    \"FLPIIAKLLSGLL\",           # 13个氨基酸\n",
        "    \"KLAKLAKKLAKLAK\",          # 14个氨基酸\n",
        "    \"GIGAVLKVLTTGLPALIS\"       # 18个氨基酸\n",
        "]\n",
        "\n",
        "def load_sequences(use_example=True, dataset_path=None):\n",
        "    \"\"\"\n",
        "    选择示例数据或加载自定义数据集。\n",
        "    dataset_path 需要是一个包含肽序列的文件（csv）。\n",
        "    \"\"\"\n",
        "    if use_example:\n",
        "        return example_sequences\n",
        "    else:\n",
        "        if dataset_path is None:\n",
        "            raise ValueError(\"请提供 dataset_path 参数来加载自定义数据集\")\n",
        "        if dataset_path.endswith(\".csv\"):\n",
        "            try:\n",
        "                df = pd.read_csv(dataset_path)\n",
        "                # 如果有'sequence'列，使用它；否则使用第一列\n",
        "                if 'sequence' in df.columns:\n",
        "                    sequences = df['sequence'].tolist()\n",
        "                else:\n",
        "                    sequences = df.iloc[:, 0].tolist()\n",
        "                # 过滤掉空值和非字符串值\n",
        "                sequences = [str(seq) for seq in sequences if pd.notna(seq) and str(seq).strip() != '']\n",
        "                return sequences\n",
        "            except Exception as e:\n",
        "                print(f\"读取CSV文件时出错: {e}\")\n",
        "                print(\"尝试使用更简单的方法读取...\")\n",
        "                # 备用方法：直接按行读取\n",
        "                with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "                    lines = f.readlines()\n",
        "                    # 跳过标题行，提取第一列\n",
        "                    sequences = []\n",
        "                    for i, line in enumerate(lines):\n",
        "                        if i == 0:  # 跳过标题\n",
        "                            continue\n",
        "                        parts = line.strip().split(',')\n",
        "                        if len(parts) > 0 and parts[0]:\n",
        "                            sequences.append(parts[0])\n",
        "                    return sequences\n",
        "        else:\n",
        "            raise ValueError(\"目前仅支持 csv 格式文件\")\n",
        "\n",
        "# ===== 使用示例数据 =====\n",
        "#sequences = load_sequences(use_example=True)\n",
        "\n",
        "# ===== 或者使用你自己的数据集 =====\n",
        "sequences = load_sequences(use_example=False, dataset_path=\"data/Non-AMP/final_non_amps.csv\")\n",
        "\n",
        "print(\"展示 5 条序列：\")\n",
        "sample_seqs = sequences[:5]\n",
        "for i, seq in enumerate(sample_seqs, 1):\n",
        "    print(f\"{i}. {seq} (长度: {len(seq)})\")\n",
        "\n",
        "# 统计信息\n",
        "lengths = [len(seq) for seq in sequences]\n",
        "total_count = len(sequences)\n",
        "max_len = max(lengths)\n",
        "min_len = min(lengths)\n",
        "median_len = np.median(lengths)\n",
        "mean_len = np.mean(lengths)\n",
        "\n",
        "print(f\"\\n总共 {total_count} 条序列\")\n",
        "print(f\"最大长度: {max_len}\")\n",
        "print(f\"最小长度: {min_len}\")\n",
        "print(f\"中位数长度: {median_len:.2f}\")\n",
        "print(f\"平均长度: {mean_len:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第3步：加载ProtT5模型\n",
        "\n",
        "ProtT5是一个预训练的蛋白质语言模型，我们需要加载编码器和解码器：\n",
        "- **编码器**：将氨基酸序列转换为嵌入向量\n",
        "- **解码器**：将嵌入向量转换回氨基酸序列\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /Volumes/myAPFS/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages (0.2.1)\n",
            "Requirement already satisfied: tqdm in /Volumes/myAPFS/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages (4.67.1)\n",
            "正在加载 ProtT5 编码器/Tokenizer ...\n",
            "✓ ProtT5 编码器加载成功\n",
            "✓ 参数量: 1,208,141,824\n",
            "✓ 嵌入维度 d_model: 1024\n",
            "✓ 使用设备: mps\n"
          ]
        }
      ],
      "source": [
        "# 安装 sentencepiece（Tokenizer 必需）\n",
        "# 如果你已装过，可注释掉\n",
        "!pip install -U sentencepiece tqdm\n",
        "\n",
        "import os, re, math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "model_name = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
        "\n",
        "\n",
        "# ====== 配置 ======\n",
        "MODEL_NAME = \"Rostlab/prot_t5_xl_half_uniref50-enc\"  # 半精度 encoder-only\n",
        "TOKENIZER_NAME = \"Rostlab/prot_t5_xl_uniref50\"       # Tokenizer 用全量版\n",
        "MAX_LEN = 48\n",
        "BATCH_SIZE = 16\n",
        "DEVICE = device  # 使用之前定义的device\n",
        "\n",
        "# ====== 实用函数 ======\n",
        "AA_20 = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "def clean_seq(seq: str) -> str:\n",
        "    s = re.sub(r\"[^A-Z]\", \"\", seq.upper())\n",
        "    s = \"\".join([c for c in s if c in AA_20])\n",
        "    return s\n",
        "\n",
        "def space_separate(seq: str) -> str:\n",
        "    return \" \".join(list(seq))\n",
        "\n",
        "# ====== 加载 Tokenizer 和 编码器 ======\n",
        "print(\"正在加载 ProtT5 编码器/Tokenizer ...\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_NAME, do_lower_case=False)\n",
        "\n",
        "encoder_model = T5EncoderModel.from_pretrained(MODEL_NAME)\n",
        "if DEVICE == \"cuda\":\n",
        "    encoder_model = encoder_model.half().to(DEVICE)\n",
        "else:\n",
        "    encoder_model = encoder_model.float().to(DEVICE)\n",
        "\n",
        "encoder_model.eval()\n",
        "print(\"✓ ProtT5 编码器加载成功\")\n",
        "print(f\"✓ 参数量: {sum(p.numel() for p in encoder_model.parameters()):,}\")\n",
        "print(f\"✓ 嵌入维度 d_model: {encoder_model.config.d_model}\")\n",
        "print(f\"✓ 使用设备: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第4步：序列编码 - 将氨基酸序列转换为嵌入向量\n",
        "\n",
        "这一步我们要实现：\n",
        "1. 单个序列编码函数：将氨基酸序列编码为(L,1024)的嵌入\n",
        "2. 零填充到固定长度：填充到(48,1024)的固定形状\n",
        "3. 批量编码：处理所有训练序列\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[单条编码自测]\n",
            "shape: (48, 1024) dtype: torch.float32 range: -0.8889049887657166 0.8496513962745667\n"
          ]
        }
      ],
      "source": [
        "# ====== 单序列编码（用于快速测试） ======\n",
        "@torch.inference_mode()\n",
        "def encode_sequence(sequence: str,\n",
        "                    tokenizer: T5Tokenizer,\n",
        "                    encoder_model: T5EncoderModel,\n",
        "                    device: str,\n",
        "                    max_length: int = MAX_LEN) -> torch.Tensor:\n",
        "    seq = clean_seq(sequence)\n",
        "    assert 1 <= len(seq) <= max_length, f\"序列长度必须 1~{max_length}，当前 {len(seq)}\"\n",
        "    spaced = space_separate(seq)\n",
        "    inputs = tokenizer(\n",
        "        spaced,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length + 2,  # 预留 special tokens\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = encoder_model(**inputs).last_hidden_state.squeeze(0)  # (seq_len_w_special, d_model)\n",
        "    # 去掉首尾 special tokens\n",
        "    outputs = outputs[1:-1]  # (seq_len, d_model)\n",
        "    # pad/截断为 (max_length, d_model)\n",
        "    seq_len, d_model = outputs.shape\n",
        "    if seq_len < max_length:\n",
        "        pad = torch.zeros(max_length - seq_len, d_model, device=outputs.device, dtype=outputs.dtype)\n",
        "        out = torch.cat([outputs, pad], dim=0)\n",
        "    else:\n",
        "        out = outputs[:max_length]\n",
        "    return out  # (max_length, d_model)\n",
        "\n",
        "# ====== 批量编码（推荐） ======\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, sequences: List[str]):\n",
        "        self.seqs = [clean_seq(s) for s in sequences]\n",
        "        # 过滤过短/过长\n",
        "        self.seqs = [s for s in self.seqs if 1 <= len(s) <= MAX_LEN]\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.seqs[idx]\n",
        "\n",
        "def collate_fn(batch: List[str]):\n",
        "    # 把 batch 的序列拼成 spaced 文本，交给 tokenizer 做 padding\n",
        "    spaced = [space_separate(s) for s in batch]\n",
        "    enc = tokenizer(\n",
        "        spaced,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN + 2,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    return enc, batch  # 返回原序列以便调试\n",
        "\n",
        "@torch.inference_mode()\n",
        "def encode_batch(sequences: List[str],\n",
        "                 encoder_model: T5EncoderModel,\n",
        "                 batch_size: int = BATCH_SIZE,\n",
        "                 device: str = DEVICE,\n",
        "                 max_length: int = MAX_LEN) -> torch.Tensor:\n",
        "    ds = SeqDataset(sequences)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    embs = []\n",
        "    for inputs, raw in tqdm(dl, desc=\"批量编码\", leave=False):\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        out = encoder_model(**inputs).last_hidden_state  # (B, Lw, d_model)\n",
        "        # 去掉每条的首尾 special，再 pad 到固定长度\n",
        "        B, Lw, D = out.shape\n",
        "        # 这里简化处理：统一去掉首尾，之后按真实长度和 MAX_LEN 修整\n",
        "        trimmed = out[:, 1:-1, :]  # (B, L, D)\n",
        "        L = trimmed.size(1)\n",
        "        if L < max_length:\n",
        "            pad = torch.zeros(B, max_length - L, D, device=trimmed.device, dtype=trimmed.dtype)\n",
        "            fixed = torch.cat([trimmed, pad], dim=1)\n",
        "        else:\n",
        "            fixed = trimmed[:, :max_length, :]\n",
        "        # 移到 CPU 节省显存\n",
        "        embs.append(fixed.float().cpu())\n",
        "    embs = torch.cat(embs, dim=0)  # (N, max_length, d_model)\n",
        "    return embs\n",
        "\n",
        "# ====== 快速自测 ======\n",
        "if 'example_sequences' not in globals():\n",
        "    example_sequences = [\"GIGKFLKKAKKFGKAFVKILKK\", \"LLKKLLKKLLKKLL\"]\n",
        "if 'sequences' not in globals():\n",
        "    sequences = example_sequences * 10\n",
        "\n",
        "print(\"\\n[单条编码自测]\")\n",
        "emb1 = encode_sequence(example_sequences[0], tokenizer, encoder_model, DEVICE, MAX_LEN)\n",
        "print(\"shape:\", tuple(emb1.shape), \"dtype:\", emb1.dtype, \"range:\", float(emb1.min()), float(emb1.max()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在编码所有训练序列...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6754535312eb4aee81de180e9635d6c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "批量编码:   0%|          | 0/6231 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m正在编码所有训练序列...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 使用高效批量编码\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_embeddings = \u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ 所有序列编码完成!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ 训练数据形状: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_embeddings.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/myAPFS/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mencode_batch\u001b[39m\u001b[34m(sequences, encoder_model, batch_size, device, max_length)\u001b[39m\n\u001b[32m     77\u001b[39m         fixed = trimmed[:, :max_length, :]\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# 移到 CPU 节省显存\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     embs.append(\u001b[43mfixed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     80\u001b[39m embs = torch.cat(embs, dim=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# (N, max_length, d_model)\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embs\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# 批量编码所有训练序列\n",
        "print(\"正在编码所有训练序列...\")\n",
        "\n",
        "# 使用高效批量编码\n",
        "train_embeddings = encode_batch(sequences, encoder_model, BATCH_SIZE, DEVICE, MAX_LEN)\n",
        "\n",
        "print(f\"✓ 所有序列编码完成!\")\n",
        "print(f\"✓ 训练数据形状: {train_embeddings.shape}\")\n",
        "print(f\"✓ 数据类型: {train_embeddings.dtype}\")\n",
        "print(f\"✓ 数据范围: [{train_embeddings.min():.3f}, {train_embeddings.max():.3f}]\")\n",
        "\n",
        "# 计算有效序列长度统计\n",
        "valid_lengths = (train_embeddings.abs().sum(dim=2) > 1e-6).sum(dim=1).tolist()\n",
        "print(f\"✓ 有效序列长度样例: {valid_lengths[:10]}\")\n",
        "print(f\"✓ 平均长度: {np.mean(valid_lengths):.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第5步：理解扩散过程基础\n",
        "\n",
        "扩散模型的核心思想：\n",
        "1. **前向过程**：逐步向干净数据添加噪声，直到变成纯噪声\n",
        "2. **反向过程**：训练神经网络学习从噪声中恢复数据\n",
        "\n",
        "我们需要理解：\n",
        "- 噪声调度（sqrt schedule）\n",
        "- 前向扩散公式\n",
        "- 如何可视化噪声添加过程\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第6步：构建UNet去噪网络\n",
        "\n",
        "我们需要构建一个1D UNet来学习去噪过程：\n",
        "\n",
        "**网络组件**：\n",
        "- 时间嵌入（Time Embedding）：让模型知道当前处于哪个扩散步骤\n",
        "- 残差块（ResBlock）：基本的卷积构建块\n",
        "- 自注意力（Self-Attention）：在瓶颈层增强特征表达\n",
        "- U型结构：编码器-瓶颈-解码器，带跳跃连接\n",
        "\n",
        "**模型架构**：Trans-UNet风格的1D版本\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第7步：训练扩散模型\n",
        "\n",
        "训练过程的核心：\n",
        "1. **随机采样时间步**：从0到2000中随机选择\n",
        "2. **添加噪声**：根据时间步向原始数据添加相应强度的噪声\n",
        "3. **模型预测**：UNet预测原始数据x0（而不是噪声）\n",
        "4. **计算损失**：MSE损失，比较预测的x0和真实x0\n",
        "5. **反向传播**：更新模型参数\n",
        "\n",
        "**训练配置**：\n",
        "- 扩散步数：2000步\n",
        "- 预测目标：x0（原始数据）\n",
        "- 噪声调度：sqrt schedule\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第8步：DDPM采样生成新序列\n",
        "\n",
        "从训练好的扩散模型采样生成新的嵌入向量：\n",
        "\n",
        "**采样过程**：\n",
        "1. **从纯噪声开始**：随机初始化(batch_size, 48, 1024)的噪声张量\n",
        "2. **逐步去噪**：从时间步2000到0，逐步去除噪声\n",
        "3. **下采样**：将2000步压缩到200步以加速采样\n",
        "4. **噪声选择**：可以选择高斯噪声或均匀噪声\n",
        "\n",
        "**DDPM算法**：使用标准的DDPM逆向采样公式\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第9步：序列解码 - 将嵌入转换回氨基酸序列\n",
        "\n",
        "使用ProtT5解码器将生成的嵌入向量转换回氨基酸序列：\n",
        "\n",
        "**解码过程**：\n",
        "1. **去除零填充**：识别并移除主要为零的行（padding部分）\n",
        "2. **包装为编码器输出**：将嵌入包装成ProtT5期望的格式\n",
        "3. **调用解码器**：使用T5的generate方法生成token序列\n",
        "4. **后处理**：清理生成的文本，只保留20种标准氨基酸\n",
        "\n",
        "**注意事项**：\n",
        "- 解码可能不完美，需要过滤无效序列\n",
        "- 生成的序列长度可能与原始序列不同\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第10步：结果分析和可视化\n",
        "\n",
        "分析生成的序列质量：\n",
        "\n",
        "**质量评估**：\n",
        "1. **基本统计**：序列长度分布、氨基酸组成\n",
        "2. **与训练集对比**：比较生成序列和原始训练序列的特征\n",
        "3. **有效性检查**：过滤掉无效或过短的序列\n",
        "4. **多样性分析**：检查生成序列的多样性\n",
        "\n",
        "**可视化**：\n",
        "- 训练损失曲线\n",
        "- 生成序列长度分布\n",
        "- 氨基酸组成热图\n",
        "- 训练集vs生成集对比\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结和下一步\n",
        "\n",
        "### 🎯 我们完成的工作：\n",
        "\n",
        "1. **数据准备**：准备了抗菌肽序列作为训练数据\n",
        "2. **序列编码**：使用ProtT5编码器将氨基酸序列转换为1024维嵌入向量\n",
        "3. **扩散建模**：理解了扩散过程的前向和反向过程\n",
        "4. **网络架构**：构建了Trans-UNet风格的1D去噪网络\n",
        "5. **模型训练**：训练扩散模型学习从噪声中恢复序列嵌入\n",
        "6. **序列生成**：使用DDPM采样生成新的序列嵌入\n",
        "7. **序列解码**：将嵌入转换回氨基酸序列\n",
        "8. **结果评估**：分析生成序列的质量和多样性\n",
        "\n",
        "### 🚀 改进方向：\n",
        "\n",
        "1. **更多数据**：使用更大的AMP数据集训练\n",
        "2. **模型优化**：调整网络架构和超参数\n",
        "3. **条件生成**：根据特定性质（如长度、活性）生成序列\n",
        "4. **质量筛选**：添加判别器或规则筛选高质量序列\n",
        "5. **预训练微调**：实现论文中的预训练+微调策略\n",
        "\n",
        "### 📚 进一步学习：\n",
        "\n",
        "- 扩散模型的数学原理\n",
        "- 蛋白质序列的生物学特性\n",
        "- 抗菌肽的结构-功能关系\n",
        "- 更高级的生成模型技术\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
