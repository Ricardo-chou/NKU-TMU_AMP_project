{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ProT-Diff 模型 - 面向过程编程\n",
        "\n",
        "理解ProT-Diff模型的训练过程，用函数先代替复杂的类结构。\n",
        "\n",
        "## 模型概述\n",
        "ProT-Diff是一个用于生成抗菌肽(AMP)的扩散模型：\n",
        "1. 使用ProtT5编码器将肽序列转换为嵌入向量\n",
        "2. 在嵌入空间训练扩散过程\n",
        "3. 从训练好的扩散模型采样新的嵌入\n",
        "4. 使用ProtT5解码器将嵌入转换回氨基酸序列\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第1步：环境设置和依赖导入\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy版本: 1.26.4\n",
            "PyTorch版本: 2.8.0+cu128\n",
            "Transformers版本: 4.42.4\n",
            "使用设备: cuda\n"
          ]
        }
      ],
      "source": [
        "# 环境设置说明：\n",
        "# 如果遇到numpy兼容性问题，请运行以下命令修复：\n",
        "# conda install numpy=1.24.3 scipy=1.10.1 scikit-learn=1.3.0 -c conda-forge --yes\n",
        "# 安装必要的包（如果还没安装）：\n",
        "# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "# conda install transformers accelerate einops pandas scikit-learn tqdm matplotlib -c conda-forge\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import T5Tokenizer, T5EncoderModel, T5ForConditionalGeneration\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "from einops import rearrange\n",
        "\n",
        "# 检查版本兼容性\n",
        "print(f\"NumPy版本: {np.__version__}\")\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"Transformers版本: {transformers.__version__}\")\n",
        "except:\n",
        "    print(\"Transformers版本: 导入失败\")\n",
        "\n",
        "# 设置设备\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
        "print(f\"使用设备: {device}\")\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第2步：准备训练数据\n",
        "\n",
        "我们支持多种数据源：\n",
        "- **演示数据**：8条示例序列，用于快速学习\n",
        "- **AMP数据集**：真实的抗菌肽序列数据，用于finetuning\n",
        "- **Non-AMP数据集**：非抗菌肽序列\n",
        "\n",
        "### 数据要求：\n",
        "- 序列长度：5-100个氨基酸\n",
        "- 只包含20种标准氨基酸：ACDEFGHIKLMNPQRSTVWY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "读取CSV文件时出错: Cannot convert numpy.ndarray to numpy.ndarray\n",
            "尝试使用更简单的方法读取...\n",
            "展示 5 条序列：\n",
            "1. MLSCWKGYI (长度: 9)\n",
            "2. GSPPVFDRVVVNPQLYENRLNQTRLGT (长度: 27)\n",
            "3. MIIWLPPLVAAVTTYLLCEYLYYYGRDEH (长度: 29)\n",
            "4. MEIEDRIDLSERGHDTLEQKR (长度: 21)\n",
            "5. PVWIMAHMVNAVAQIDEFVNL (长度: 21)\n",
            "\n",
            "总共 108776 条序列\n",
            "最大长度: 100\n",
            "最小长度: 4\n",
            "中位数长度: 35.00\n",
            "平均长度: 31.79\n"
          ]
        }
      ],
      "source": [
        "# 示例抗菌肽序列（长度5-48，只包含20种氨基酸）\n",
        "example_sequences = [\n",
        "    \"GIGKFLKKAKKFGKAFVKILKK\",  # 22个氨基酸\n",
        "    \"KKLFKKILKYL\",             # 11个氨基酸\n",
        "    \"GLFDIVKKVVGAL\",           # 13个氨基酸\n",
        "    \"RWKIFKKIERVGQHTRDAT\",     # 19个氨基酸\n",
        "    \"KWKLFKKIPKFLHLAKKF\",      # 18个氨基酸\n",
        "    \"FLPIIAKLLSGLL\",           # 13个氨基酸\n",
        "    \"KLAKLAKKLAKLAK\",          # 14个氨基酸\n",
        "    \"GIGAVLKVLTTGLPALIS\"       # 18个氨基酸\n",
        "]\n",
        "\n",
        "def load_sequences(use_example=True, dataset_path=None):\n",
        "    \"\"\"\n",
        "    选择示例数据或加载自定义数据集。\n",
        "    dataset_path 需要是一个包含肽序列的文件（csv）。\n",
        "    \"\"\"\n",
        "    if use_example:\n",
        "        return example_sequences\n",
        "    else:\n",
        "        if dataset_path is None:\n",
        "            raise ValueError(\"请提供 dataset_path 参数来加载自定义数据集\")\n",
        "        if dataset_path.endswith(\".csv\"):\n",
        "            try:\n",
        "                df = pd.read_csv(dataset_path)\n",
        "                # 如果有'sequence'列，使用它；否则使用第一列\n",
        "                if 'sequence' in df.columns:\n",
        "                    sequences = df['sequence'].tolist()\n",
        "                else:\n",
        "                    sequences = df.iloc[:, 0].tolist()\n",
        "                # 过滤掉空值和非字符串值\n",
        "                sequences = [str(seq) for seq in sequences if pd.notna(seq) and str(seq).strip() != '']\n",
        "                return sequences\n",
        "            except Exception as e:\n",
        "                print(f\"读取CSV文件时出错: {e}\")\n",
        "                print(\"尝试使用更简单的方法读取...\")\n",
        "                # 备用方法：直接按行读取\n",
        "                with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "                    lines = f.readlines()\n",
        "                    # 跳过标题行，提取第一列\n",
        "                    sequences = []\n",
        "                    for i, line in enumerate(lines):\n",
        "                        if i == 0:  # 跳过标题\n",
        "                            continue\n",
        "                        parts = line.strip().split(',')\n",
        "                        if len(parts) > 0 and parts[0]:\n",
        "                            sequences.append(parts[0])\n",
        "                    return sequences\n",
        "        else:\n",
        "            raise ValueError(\"目前仅支持 csv 格式文件\")\n",
        "\n",
        "# ===== 使用示例数据 =====\n",
        "#sequences = load_sequences(use_example=True)\n",
        "\n",
        "# ===== 或者使用你自己的数据集 =====\n",
        "sequences = load_sequences(use_example=False, dataset_path=\"data/Non-AMP/final_non_amps.csv\")\n",
        "\n",
        "print(\"展示 5 条序列：\")\n",
        "sample_seqs = sequences[:5]\n",
        "for i, seq in enumerate(sample_seqs, 1):\n",
        "    print(f\"{i}. {seq} (长度: {len(seq)})\")\n",
        "\n",
        "# 统计信息\n",
        "lengths = [len(seq) for seq in sequences]\n",
        "total_count = len(sequences)\n",
        "max_len = max(lengths)\n",
        "min_len = min(lengths)\n",
        "median_len = np.median(lengths)\n",
        "mean_len = np.mean(lengths)\n",
        "print(f\"\\n总共 {total_count} 条序列\")\n",
        "print(f\"最大长度: {max_len}\")\n",
        "print(f\"最小长度: {min_len}\")\n",
        "print(f\"中位数长度: {median_len:.2f}\")\n",
        "print(f\"平均长度: {mean_len:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第3步：加载ProtT5模型\n",
        "\n",
        "ProtT5是一个预训练的蛋白质语言模型，我们需要加载编码器和解码器：\n",
        "- **编码器**：将氨基酸序列转换为嵌入向量\n",
        "- **解码器**：将嵌入向量转换回氨基酸序列\n",
        "cd /root/clash\n",
        "nohup ./clash -d . > clash.log 2>&1 &\n",
        "export http_proxy=http://127.0.0.1:7890\n",
        "export https_proxy=http://127.0.0.1:7890\n",
        "curl -I https://www.google.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# ====== 配置 ======\n",
        "MODEL_NAME = \"Rostlab/prot_t5_xl_half_uniref50-enc\"  # 半精度 encoder-only\n",
        "TOKENIZER_NAME = \"Rostlab/prot_t5_xl_uniref50\"       # Tokenizer 用全量版\n",
        "MAX_LEN = 48\n",
        "BATCH_SIZE = 16\n",
        "DEVICE = device  # 使用之前定义的device\n",
        "\n",
        "# ====== 实用函数 ======\n",
        "AA_20 = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "def clean_seq(seq: str) -> str:\n",
        "    s = re.sub(r\"[^A-Z]\", \"\", seq.upper())\n",
        "    s = \"\".join([c for c in s if c in AA_20])\n",
        "    return s\n",
        "\n",
        "def space_separate(seq: str) -> str:\n",
        "    return \" \".join(list(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "HF_HUB_ENABLE_HF_TRANSFER = 1\n",
            "HF transfer enabled?  False\n"
          ]
        }
      ],
      "source": [
        "# ==== 0) 安装依赖（已装过可跳过） ====\n",
        "%env HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "import os\n",
        "print(\"HF_HUB_ENABLE_HF_TRANSFER =\", os.environ.get(\"HF_HUB_ENABLE_HF_TRANSFER\"))\n",
        "\n",
        "from huggingface_hub import constants\n",
        "print(\"HF transfer enabled? \", constants.HF_HUB_ENABLE_HF_TRANSFER)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b2e19f36d244d93a9109e6de6b449ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model stored at: /root/autodl-tmp/prot_t5_xl_uniref50\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, time\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# 0) 缓存目录\n",
        "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/huggingface\"\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/huggingface/transformers\"\n",
        "pathlib.Path(os.environ[\"HF_HOME\"]).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(os.environ[\"TRANSFORMERS_CACHE\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) 代理\n",
        "proxy = \"http://127.0.0.1:7890\"\n",
        "for var in [\"HTTP_PROXY\",\"HTTPS_PROXY\",\"http_proxy\",\"https_proxy\"]:\n",
        "    os.environ[var] = proxy\n",
        "\n",
        "# 2) 加速 + 禁用 Xet\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"  # ← 强制禁用 Xet\n",
        "os.environ[\"HF_HUB_ETAG_TIMEOUT\"] = \"60\"\n",
        "\n",
        "# 3) 下载\n",
        "repo_id = \"Rostlab/prot_t5_xl_uniref50\"\n",
        "dest_dir = \"/root/autodl-tmp/prot_t5_xl_uniref50\"\n",
        "token = None  # 如果需要登录，改成你的 token\n",
        "\n",
        "retries, delay_s = 5, 15\n",
        "last_err = None\n",
        "\n",
        "for i in range(1, retries + 1):\n",
        "    try:\n",
        "        local_dir = snapshot_download(\n",
        "            repo_id=repo_id,\n",
        "            local_dir=dest_dir,\n",
        "            resume_download=True,\n",
        "            max_workers=1,\n",
        "            etag_timeout=60,\n",
        "            token=token\n",
        "        )\n",
        "        print(f\"Model stored at: {local_dir}\")\n",
        "        last_err = None\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "        print(f\"[retry {i}/{retries}] {e}\")\n",
        "        time.sleep(delay_s)\n",
        "\n",
        "if last_err:\n",
        "    raise last_err\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载 ProtT5 编码器/Tokenizer ...\n",
            "✓ ProtT5 编码器加载成功\n",
            "✓ 参数量: 1,208,141,824\n",
            "✓ 嵌入维度 d_model: 1024\n",
            "✓ 使用设备: cuda\n"
          ]
        }
      ],
      "source": [
        "# ==== 4) 加载 Tokenizer 与 Encoder（直接用本地缓存路径） ====\n",
        "import re, math\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "\n",
        "# 设备\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 本地模型路径（替换成你的实际下载目录）\n",
        "LOCAL_MODEL_DIR = \"/root/autodl-tmp/prot_t5_xl_uniref50\"\n",
        "\n",
        "print(\"正在加载 ProtT5 编码器/Tokenizer ...\")\n",
        "\n",
        "# 直接从本地路径加载\n",
        "tokenizer = T5Tokenizer.from_pretrained(LOCAL_MODEL_DIR, do_lower_case=False)\n",
        "encoder_model = T5EncoderModel.from_pretrained(LOCAL_MODEL_DIR)\n",
        "\n",
        "# 精度与设备\n",
        "if device == \"cuda\":\n",
        "    # prot_t5_xl 较大，半精度可显著省显存\n",
        "    encoder_model = encoder_model.half().to(device)\n",
        "else:\n",
        "    encoder_model = encoder_model.float().to(device)\n",
        "\n",
        "encoder_model.eval()\n",
        "\n",
        "print(\"✓ ProtT5 编码器加载成功\")\n",
        "print(f\"✓ 参数量: {sum(p.numel() for p in encoder_model.parameters()):,}\")\n",
        "print(f\"✓ 嵌入维度 d_model: {encoder_model.config.d_model}\")\n",
        "print(f\"✓ 使用设备: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第4步：序列编码 - 将氨基酸序列转换为嵌入向量\n",
        "\n",
        "这一步我们要实现：\n",
        "1. 单个序列编码函数：将氨基酸序列编码为(L,1024)的嵌入\n",
        "2. 零填充到固定长度：填充到(48,1024)的固定形状\n",
        "3. 批量编码：处理所有训练序列\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[单条编码自测]\n",
            "序列: GIGKFLKKAKKFGKAFVKILKK\n",
            "真实长度: 22\n",
            "嵌入形状: (22, 1024)\n",
            "数据类型: torch.float16\n",
            "数值范围: [-0.893066, 0.850586]\n",
            "\n",
            "[批量编码自测]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "354d677151af428786c4958510f39fb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "批量编码:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "编码了 3 个序列\n",
            "序列 1: 长度=22, 嵌入形状=(22, 1024)\n",
            "  原始序列: GIGKFLKKAKKFGKAFVKIL...\n",
            "序列 2: 长度=11, 嵌入形状=(11, 1024)\n",
            "  原始序列: KKLFKKILKYL\n",
            "序列 3: 长度=13, 嵌入形状=(13, 1024)\n",
            "  原始序列: GLFDIVKKVVGAL\n",
            "\n",
            "[向后兼容性测试 - 固定长度填充]\n",
            "固定长度嵌入形状: (3, 48, 1024)\n",
            "✓ 所有测试通过！现在编码函数会保留真实长度信息\n"
          ]
        }
      ],
      "source": [
        "# ====== 单序列编码（用于快速测试） ======\n",
        "@torch.inference_mode()\n",
        "def encode_sequence(sequence: str,\n",
        "                    tokenizer: T5Tokenizer,\n",
        "                    encoder_model: T5EncoderModel,\n",
        "                    device: str,\n",
        "                    max_length: int = MAX_LEN) -> tuple[torch.Tensor, int]:\n",
        "    \"\"\"\n",
        "    编码单个序列，返回嵌入和真实长度\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (embeddings, real_length)\n",
        "            - embeddings: (real_length, d_model) 真实长度的嵌入，不做填充\n",
        "            - real_length: int 序列的真实长度\n",
        "    \"\"\"\n",
        "    seq = clean_seq(sequence)\n",
        "    real_length = len(seq)\n",
        "    assert 1 <= real_length <= max_length, f\"序列长度必须 1~{max_length}，当前 {real_length}\"\n",
        "    \n",
        "    spaced = space_separate(seq)\n",
        "    inputs = tokenizer(\n",
        "        spaced,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length + 2,  # 预留 special tokens\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = encoder_model(**inputs).last_hidden_state.squeeze(0)  # (seq_len_w_special, d_model)\n",
        "    \n",
        "    # 去掉首尾 special tokens，保留真实长度\n",
        "    outputs = outputs[1:real_length+1]  # (real_length, d_model)\n",
        "    \n",
        "    return outputs, real_length  # (real_length, d_model), int\n",
        "\n",
        "# ====== 批量编码（推荐） ======\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, sequences: List[str]):\n",
        "        self.seqs = [clean_seq(s) for s in sequences]\n",
        "        # 过滤过短/过长\n",
        "        self.seqs = [s for s in self.seqs if 1 <= len(s) <= MAX_LEN]\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.seqs[idx]\n",
        "\n",
        "def collate_fn(batch: List[str]):\n",
        "    # 把 batch 的序列拼成 spaced 文本，交给 tokenizer 做 padding\n",
        "    spaced = [space_separate(s) for s in batch]\n",
        "    enc = tokenizer(\n",
        "        spaced,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN + 2,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    return enc, batch  # 返回原序列以便调试\n",
        "\n",
        "@torch.inference_mode()\n",
        "def encode_batch(sequences: List[str],\n",
        "                 encoder_model: T5EncoderModel,\n",
        "                 batch_size: int = BATCH_SIZE,\n",
        "                 device: str = DEVICE,\n",
        "                 max_length: int = MAX_LEN) -> tuple[List[torch.Tensor], List[int]]:\n",
        "    \"\"\"\n",
        "    批量编码序列，返回嵌入列表和长度列表\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (embeddings_list, lengths_list)\n",
        "            - embeddings_list: List[torch.Tensor] 每个元素形状为 (real_length, d_model)\n",
        "            - lengths_list: List[int] 每个序列的真实长度\n",
        "    \"\"\"\n",
        "    ds = SeqDataset(sequences)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    embs_list = []\n",
        "    lengths_list = []\n",
        "    \n",
        "    for inputs, raw_batch in tqdm(dl, desc=\"批量编码\", leave=False):\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        out = encoder_model(**inputs).last_hidden_state  # (B, Lw, d_model)\n",
        "        \n",
        "        # 为每个序列单独处理，保留真实长度\n",
        "        for i, raw_seq in enumerate(raw_batch):\n",
        "            real_length = len(raw_seq)\n",
        "            lengths_list.append(real_length)\n",
        "            \n",
        "            # 去掉 special tokens，保留真实长度的嵌入\n",
        "            seq_emb = out[i, 1:real_length+1, :]  # (real_length, d_model)\n",
        "            \n",
        "            # 移到 CPU 节省显存\n",
        "            embs_list.append(seq_emb.float().cpu())\n",
        "    \n",
        "    return embs_list, lengths_list\n",
        "\n",
        "# ====== 辅助函数：如需固定长度可使用此函数 ======\n",
        "def pad_embeddings_to_fixed_length(embs_list: List[torch.Tensor], \n",
        "                                   lengths_list: List[int],\n",
        "                                   target_length: int = MAX_LEN) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    将变长嵌入列表填充到固定长度（向后兼容）\n",
        "    \n",
        "    Args:\n",
        "        embs_list: 变长嵌入列表\n",
        "        lengths_list: 对应的长度列表\n",
        "        target_length: 目标长度\n",
        "        \n",
        "    Returns:\n",
        "        torch.Tensor: (N, target_length, d_model) 固定长度的嵌入张量\n",
        "    \"\"\"\n",
        "    if not embs_list:\n",
        "        return torch.empty(0, target_length, embs_list[0].shape[-1])\n",
        "    \n",
        "    d_model = embs_list[0].shape[-1]\n",
        "    N = len(embs_list)\n",
        "    \n",
        "    # 创建固定长度的张量\n",
        "    padded_embs = torch.zeros(N, target_length, d_model, dtype=embs_list[0].dtype)\n",
        "    \n",
        "    for i, (emb, length) in enumerate(zip(embs_list, lengths_list)):\n",
        "        actual_length = min(length, target_length)\n",
        "        padded_embs[i, :actual_length] = emb[:actual_length]\n",
        "    \n",
        "    return padded_embs\n",
        "\n",
        "# ====== 快速自测 ======\n",
        "if 'example_sequences' not in globals():\n",
        "    example_sequences = [\"GIGKFLKKAKKFGKAFVKILKK\", \"LLKKLLKKLLKKLL\"]\n",
        "if 'sequences' not in globals():\n",
        "    sequences = example_sequences * 10\n",
        "\n",
        "print(\"\\n[单条编码自测]\")\n",
        "emb1, length1 = encode_sequence(example_sequences[0], tokenizer, encoder_model, DEVICE, MAX_LEN)\n",
        "print(f\"序列: {example_sequences[0]}\")\n",
        "print(f\"真实长度: {length1}\")\n",
        "print(f\"嵌入形状: {tuple(emb1.shape)}\")\n",
        "print(f\"数据类型: {emb1.dtype}\")\n",
        "print(f\"数值范围: [{float(emb1.min()):.6f}, {float(emb1.max()):.6f}]\")\n",
        "\n",
        "print(\"\\n[批量编码自测]\")\n",
        "test_seqs = example_sequences[:3]  # 测试前3个序列\n",
        "embs_list, lengths_list = encode_batch(test_seqs, encoder_model, 2, DEVICE, MAX_LEN)\n",
        "print(f\"编码了 {len(embs_list)} 个序列\")\n",
        "for i, (emb, length) in enumerate(zip(embs_list, lengths_list)):\n",
        "    print(f\"序列 {i+1}: 长度={length}, 嵌入形状={tuple(emb.shape)}\")\n",
        "    print(f\"  原始序列: {test_seqs[i][:20]}{'...' if len(test_seqs[i]) > 20 else ''}\")\n",
        "\n",
        "print(\"\\n[向后兼容性测试 - 固定长度填充]\")\n",
        "padded_embs = pad_embeddings_to_fixed_length(embs_list, lengths_list, MAX_LEN)\n",
        "print(f\"固定长度嵌入形状: {tuple(padded_embs.shape)}\")\n",
        "print(\"✓ 所有测试通过！现在编码函数会保留真实长度信息\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ 编码函数修改完成\n",
        "\n",
        "### 🔄 主要变化：\n",
        "\n",
        "1. **`encode_sequence()`** 现在返回 `(embeddings, real_length)`\n",
        "   - `embeddings`: 形状为 `(real_length, d_model)` 的真实长度嵌入\n",
        "   - `real_length`: 序列的实际长度\n",
        "\n",
        "2. **`encode_batch()`** 现在返回 `(embeddings_list, lengths_list)`\n",
        "   - `embeddings_list`: 每个元素为 `(real_length, d_model)` 的嵌入列表\n",
        "   - `lengths_list`: 对应的真实长度列表\n",
        "\n",
        "3. **新增辅助函数 `pad_embeddings_to_fixed_length()`**\n",
        "   - 用于向后兼容，将变长嵌入填充到固定长度\n",
        "   - 返回 `(N, target_length, d_model)` 的固定形状张量\n",
        "\n",
        "### 💡 使用示例：\n",
        "\n",
        "```python\n",
        "# 单序列编码 - 保留真实长度\n",
        "emb, length = encode_sequence(\"GIGKFLKKAKKF\", tokenizer, encoder_model, device)\n",
        "print(f\"嵌入形状: {emb.shape}\")  # (12, 1024) 而不是 (48, 1024)\n",
        "\n",
        "# 批量编码 - 保留所有真实长度\n",
        "embs_list, lengths = encode_batch(sequences, encoder_model)\n",
        "# 每个 embs_list[i] 的形状都是 (lengths[i], 1024)\n",
        "\n",
        "# 如需固定长度（向后兼容）\n",
        "fixed_embs = pad_embeddings_to_fixed_length(embs_list, lengths, 48)\n",
        "print(f\"固定长度形状: {fixed_embs.shape}\")  # (N, 48, 1024)\n",
        "```\n",
        "\n",
        "### ✨ 优势：\n",
        "\n",
        "- **内存效率**：不浪费空间在填充的零向量上\n",
        "- **真实信息**：保留每个序列的实际长度\n",
        "- **灵活性**：可以根据需要选择是否使用固定长度\n",
        "- **向后兼容**：提供辅助函数支持原有的固定长度需求\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在编码所有训练序列...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0626ba0309ae46b6a76600c869602e80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "批量编码:   0%|          | 0/6231 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 所有序列编码完成!\n",
            "✓ 编码序列数量: 99687\n",
            "✓ 长度统计: 最小=4, 最大=48, 平均=30.0\n",
            "✓ 数据类型: torch.float32\n",
            "✓ 数值范围: [-1.900, 1.873]\n",
            "\n",
            "正在保存嵌入数据...\n",
            "✓ 嵌入数据已保存到: embedding_non_amp.pt\n",
            "✓ 文件大小: 11678.0 MB\n"
          ]
        }
      ],
      "source": [
        "# 批量编码所有训练序列\n",
        "print(\"正在编码所有训练序列...\")\n",
        "\n",
        "# 使用高效批量编码（简化格式：直接保存嵌入列表）\n",
        "if os.path.exists(\"embedding_non_amp.pt\"):\n",
        "    train_embeddings_list = torch.load(\"embedding_non_amp.pt\", map_location=\"cpu\")\n",
        "    train_lengths = [emb.size(0) for emb in train_embeddings_list]  # 从嵌入推断长度\n",
        "    print(f\"✓ 从缓存加载了 {len(train_embeddings_list)} 个序列的嵌入\")\n",
        "else:\n",
        "    train_embeddings_list, train_lengths = encode_batch(sequences, encoder_model, BATCH_SIZE, DEVICE, MAX_LEN)\n",
        "    print(f\"✓ 所有序列编码完成!\")\n",
        "    print(f\"✓ 编码序列数量: {len(train_embeddings_list)}\")\n",
        "    print(f\"✓ 长度统计: 最小={min(train_lengths)}, 最大={max(train_lengths)}, 平均={sum(train_lengths)/len(train_lengths):.1f}\")\n",
        "    \n",
        "    # 检查数据类型和范围\n",
        "    if train_embeddings_list:\n",
        "        sample_emb = train_embeddings_list[0]\n",
        "        print(f\"✓ 数据类型: {sample_emb.dtype}\")\n",
        "        all_values = torch.cat(train_embeddings_list, dim=0)\n",
        "        print(f\"✓ 数值范围: [{all_values.min():.3f}, {all_values.max():.3f}]\")\n",
        "\n",
        "    # 保存嵌入数据（简化格式：只保存嵌入列表）\n",
        "    print(\"\\n正在保存嵌入数据...\")\n",
        "    torch.save(train_embeddings_list, \"embedding_non_amp.pt\")\n",
        "    print(f\"✓ 嵌入数据已保存到: embedding_non_amp.pt\")\n",
        "    \n",
        "    # 计算总文件大小\n",
        "    total_elements = sum(emb.numel() for emb in train_embeddings_list)\n",
        "    element_size = train_embeddings_list[0].element_size() if train_embeddings_list else 4\n",
        "    file_size_mb = total_elements * element_size / 1024 / 1024\n",
        "    print(f\"✓ 文件大小: {file_size_mb:.1f} MB\")\n",
        "\n",
        "# # 如果需要固定长度格式（向后兼容）\n",
        "# print(\"\\n[可选] 转换为固定长度格式:\")\n",
        "# train_embeddings_fixed = pad_embeddings_to_fixed_length(train_embeddings_list, train_lengths, MAX_LEN)\n",
        "# print(f\"✓ 固定长度嵌入形状: {train_embeddings_fixed.shape}\")\n",
        "# print(\"✓ 现在您可以选择使用变长格式（train_embeddings_list, train_lengths）或固定长度格式（train_embeddings_fixed）\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "读取CSV文件时出错: Cannot convert numpy.ndarray to numpy.ndarray\n",
            "尝试使用更简单的方法读取...\n",
            "7956\n"
          ]
        }
      ],
      "source": [
        "sequence = load_sequences(use_example=False, dataset_path=\"data/AMP/final_AMP.csv\")\n",
        "print(len(sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在编码所有训练序列...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1caef3f37606401e856656fed444465b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "批量编码:   0%|          | 0/483 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 所有序列编码完成!\n",
            "✓ 编码序列数量: 7720\n",
            "✓ 长度统计: 最小=5, 最大=48, 平均=20.2\n",
            "✓ 数据类型: torch.float32\n",
            "✓ 数值范围: [-1.619, 1.542]\n",
            "\n",
            "正在保存嵌入数据...\n",
            "✓ 嵌入数据已保存到: embedding_amp.pt\n",
            "✓ 文件大小: 610.4 MB\n"
          ]
        }
      ],
      "source": [
        "# 批量编码所有训练序列\n",
        "print(\"正在编码所有训练序列...\")\n",
        "\n",
        "# 使用高效批量编码（简化格式：直接保存嵌入列表）\n",
        "if os.path.exists(\"embedding_amp.pt\"):\n",
        "    train_embeddings_list = torch.load(\"embedding_amp.pt\", map_location=\"cpu\")\n",
        "    train_lengths = [emb.size(0) for emb in train_embeddings_list]  # 从嵌入推断长度\n",
        "    print(f\"✓ 从缓存加载了 {len(train_embeddings_list)} 个序列的嵌入\")\n",
        "else:\n",
        "    train_embeddings_list, train_lengths = encode_batch(sequence, encoder_model, BATCH_SIZE, DEVICE, MAX_LEN)\n",
        "    print(f\"✓ 所有序列编码完成!\")\n",
        "    print(f\"✓ 编码序列数量: {len(train_embeddings_list)}\")\n",
        "    print(f\"✓ 长度统计: 最小={min(train_lengths)}, 最大={max(train_lengths)}, 平均={sum(train_lengths)/len(train_lengths):.1f}\")\n",
        "    \n",
        "    # 检查数据类型和范围\n",
        "    if train_embeddings_list:\n",
        "        sample_emb = train_embeddings_list[0]\n",
        "        print(f\"✓ 数据类型: {sample_emb.dtype}\")\n",
        "        all_values = torch.cat(train_embeddings_list, dim=0)\n",
        "        print(f\"✓ 数值范围: [{all_values.min():.3f}, {all_values.max():.3f}]\")\n",
        "\n",
        "    # 保存嵌入数据（简化格式：只保存嵌入列表）\n",
        "    print(\"\\n正在保存嵌入数据...\")\n",
        "    torch.save(train_embeddings_list, \"embedding_amp.pt\")\n",
        "    print(f\"✓ 嵌入数据已保存到: embedding_amp.pt\")\n",
        "    \n",
        "    # 计算总文件大小\n",
        "    total_elements = sum(emb.numel() for emb in train_embeddings_list)\n",
        "    element_size = train_embeddings_list[0].element_size() if train_embeddings_list else 4\n",
        "    file_size_mb = total_elements * element_size / 1024 / 1024\n",
        "    print(f\"✓ 文件大小: {file_size_mb:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第5步：理解扩散过程基础\n",
        "\n",
        "扩散模型的核心思想：\n",
        "1. **前向过程**：逐步向干净数据添加噪声，直到变成纯噪声\n",
        "2. **反向过程**：训练神经网络学习从噪声中恢复数据\n",
        "\n",
        "我们需要理解：\n",
        "- 噪声调度（sqrt schedule）\n",
        "- 前向扩散公式\n",
        "- 如何可视化噪声添加过程\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第6步：构建UNet去噪网络\n",
        "\n",
        "我们需要构建一个1D UNet来学习去噪过程：\n",
        "\n",
        "**网络组件**：\n",
        "- 时间嵌入（Time Embedding）：让模型知道当前处于哪个扩散步骤\n",
        "- 残差块（ResBlock）：基本的卷积构建块\n",
        "- 自注意力（Self-Attention）：在瓶颈层增强特征表达\n",
        "- U型结构：编码器-瓶颈-解码器，带跳跃连接\n",
        "\n",
        "**模型架构**：Trans-UNet风格的1D版本\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第7步：训练扩散模型\n",
        "\n",
        "训练过程的核心：\n",
        "1. **随机采样时间步**：从0到2000中随机选择\n",
        "2. **添加噪声**：根据时间步向原始数据添加相应强度的噪声\n",
        "3. **模型预测**：UNet预测原始数据x0（而不是噪声）\n",
        "4. **计算损失**：MSE损失，比较预测的x0和真实x0\n",
        "5. **反向传播**：更新模型参数\n",
        "\n",
        "**训练配置**：\n",
        "- 扩散步数：2000步\n",
        "- 预测目标：x0（原始数据）\n",
        "- 噪声调度：sqrt schedule\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第8步：DDPM采样生成新序列\n",
        "\n",
        "从训练好的扩散模型采样生成新的嵌入向量：\n",
        "\n",
        "**采样过程**：\n",
        "1. **从纯噪声开始**：随机初始化(batch_size, 48, 1024)的噪声张量\n",
        "2. **逐步去噪**：从时间步2000到0，逐步去除噪声\n",
        "3. **下采样**：将2000步压缩到200步以加速采样\n",
        "4. **噪声选择**：可以选择高斯噪声或均匀噪声\n",
        "\n",
        "**DDPM算法**：使用标准的DDPM逆向采样公式\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第9步：序列解码 - 将嵌入转换回氨基酸序列\n",
        "\n",
        "使用ProtT5解码器将生成的嵌入向量转换回氨基酸序列：\n",
        "\n",
        "**解码过程**：\n",
        "1. **去除零填充**：识别并移除主要为零的行（padding部分）\n",
        "2. **包装为编码器输出**：将嵌入包装成ProtT5期望的格式\n",
        "3. **调用解码器**：使用T5的generate方法生成token序列\n",
        "4. **后处理**：清理生成的文本，只保留20种标准氨基酸\n",
        "\n",
        "**注意事项**：\n",
        "- 解码可能不完美，需要过滤无效序列\n",
        "- 生成的序列长度可能与原始序列不同\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第10步：结果分析和可视化\n",
        "\n",
        "分析生成的序列质量：\n",
        "\n",
        "**质量评估**：\n",
        "1. **基本统计**：序列长度分布、氨基酸组成\n",
        "2. **与训练集对比**：比较生成序列和原始训练序列的特征\n",
        "3. **有效性检查**：过滤掉无效或过短的序列\n",
        "4. **多样性分析**：检查生成序列的多样性\n",
        "\n",
        "**可视化**：\n",
        "- 训练损失曲线\n",
        "- 生成序列长度分布\n",
        "- 氨基酸组成热图\n",
        "- 训练集vs生成集对比\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结和下一步\n",
        "\n",
        "### 🎯 我们完成的工作：\n",
        "\n",
        "1. **数据准备**：准备了抗菌肽序列作为训练数据\n",
        "2. **序列编码**：使用ProtT5编码器将氨基酸序列转换为1024维嵌入向量\n",
        "3. **扩散建模**：理解了扩散过程的前向和反向过程\n",
        "4. **网络架构**：构建了Trans-UNet风格的1D去噪网络\n",
        "5. **模型训练**：训练扩散模型学习从噪声中恢复序列嵌入\n",
        "6. **序列生成**：使用DDPM采样生成新的序列嵌入\n",
        "7. **序列解码**：将嵌入转换回氨基酸序列\n",
        "8. **结果评估**：分析生成序列的质量和多样性\n",
        "\n",
        "### 🚀 改进方向：\n",
        "\n",
        "1. **更多数据**：使用更大的AMP数据集训练\n",
        "2. **模型优化**：调整网络架构和超参数\n",
        "3. **条件生成**：根据特定性质（如长度、活性）生成序列\n",
        "4. **质量筛选**：添加判别器或规则筛选高质量序列\n",
        "5. **预训练微调**：实现论文中的预训练+微调策略\n",
        "\n",
        "### 📚 进一步学习：\n",
        "\n",
        "- 扩散模型的数学原理\n",
        "- 蛋白质序列的生物学特性\n",
        "- 抗菌肽的结构-功能关系\n",
        "- 更高级的生成模型技术\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 🛠️ 如果模型加载仍然失败，可以尝试以下手动解决方案：\n",
        "\n",
        "# 方案1: 手动清理 transformers 缓存\n",
        "def manual_cache_cleanup():\n",
        "    \"\"\"手动清理所有 transformers 缓存\"\"\"\n",
        "    import shutil\n",
        "    from transformers import TRANSFORMERS_CACHE\n",
        "    \n",
        "    print(f\"Transformers 缓存目录: {TRANSFORMERS_CACHE}\")\n",
        "    \n",
        "    if os.path.exists(TRANSFORMERS_CACHE):\n",
        "        print(\"正在清理缓存...\")\n",
        "        try:\n",
        "            shutil.rmtree(TRANSFORMERS_CACHE)\n",
        "            print(\"✓ 缓存清理成功\")\n",
        "            os.makedirs(TRANSFORMERS_CACHE, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 缓存清理失败: {e}\")\n",
        "    else:\n",
        "        print(\"缓存目录不存在\")\n",
        "\n",
        "# 方案2: 检查网络连接\n",
        "def test_huggingface_connection():\n",
        "    \"\"\"测试到 Hugging Face 的连接\"\"\"\n",
        "    import requests\n",
        "    \n",
        "    urls_to_test = [\n",
        "        \"https://huggingface.co\",\n",
        "        \"https://hf-mirror.com\",\n",
        "        \"https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
        "    ]\n",
        "    \n",
        "    for url in urls_to_test:\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                print(f\"✓ 连接成功: {url}\")\n",
        "            else:\n",
        "                print(f\"❌ 连接失败: {url} (状态码: {response.status_code})\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 连接失败: {url} (错误: {e})\")\n",
        "\n",
        "# 方案3: 使用本地模型（如果已下载）\n",
        "def try_local_model():\n",
        "    \"\"\"尝试使用本地已下载的模型\"\"\"\n",
        "    from transformers import TRANSFORMERS_CACHE\n",
        "    \n",
        "    local_paths = [\n",
        "        os.path.join(TRANSFORMERS_CACHE, \"models--Rostlab--prot_t5_xl_half_uniref50-enc\"),\n",
        "        os.path.join(TRANSFORMERS_CACHE, \"models--Rostlab--prot_t5_xl_uniref50\"),\n",
        "        \"/root/.cache/huggingface/transformers/models--Rostlab--prot_t5_xl_half_uniref50-enc\",\n",
        "        \"/root/.cache/huggingface/transformers/models--Rostlab--prot_t5_xl_uniref50\"\n",
        "    ]\n",
        "    \n",
        "    print(\"检查本地模型文件...\")\n",
        "    for path in local_paths:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"✓ 找到本地模型: {path}\")\n",
        "            # 列出文件\n",
        "            try:\n",
        "                files = os.listdir(path)\n",
        "                print(f\"  文件: {files[:10]}...\")  # 显示前10个文件\n",
        "            except:\n",
        "                pass\n",
        "        else:\n",
        "            print(f\"❌ 本地模型不存在: {path}\")\n",
        "\n",
        "print(\"=== 诊断工具 ===\")\n",
        "print(\"\\n1. 测试网络连接:\")\n",
        "test_huggingface_connection()\n",
        "\n",
        "print(\"\\n2. 检查本地模型:\")\n",
        "try_local_model()\n",
        "\n",
        "print(\"\\n3. 如需清理缓存，取消注释下面的代码:\")\n",
        "print(\"# manual_cache_cleanup()\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
