# ===== ç¬¬å…«æ­¥ï¼šå˜é•¿æ¢å¤ä¸ProtT5è§£ç ï¼ˆåµŒå…¥â†’åºåˆ—ï¼‰ =====

print("=" * 80)
print("ç¬¬å…«æ­¥ï¼šå˜é•¿æ¢å¤ä¸ProtT5è§£ç  - åµŒå…¥å‘é‡åˆ°æ°¨åŸºé…¸åºåˆ—")
print("=" * 80)

import torch
import numpy as np
import time
from pathlib import Path
from transformers import T5Tokenizer, T5ForConditionalGeneration
import re
from collections import Counter

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å…¨å±€å¸¸é‡
MAX_LEN = 48
EMB_DIM = 1024
SAMPLING_CONFIG = {
    "model": {
        "use_ema": True,  # ä¼˜å…ˆä½¿ç”¨EMAæ¨¡å‹
        "finetune_path": "./checkpoints/finetune/finetune_best.pt",
        "ema_path": "./checkpoints/finetune/finetune_ema_best.pt"
    },
    "sampling": {
        "num_samples": 10000,      # ç”Ÿæˆæ ·æœ¬æ•°é‡
        "batch_size": 64,        # é‡‡æ ·æ‰¹æ¬¡å¤§å°
        "num_steps": 200,        # é‡‡æ ·æ­¥æ•° (T_SAMPLE)
        "noise_type": "normal",  # å™ªå£°ç±»å‹: "normal" æˆ– "uniform"
        "use_mask_guidance": True,  # æ˜¯å¦ä½¿ç”¨maskå¼•å¯¼
        "temperature": 1.0,      # é‡‡æ ·æ¸©åº¦ï¼ˆæ§åˆ¶å¤šæ ·æ€§ï¼‰
        "eta": 0.0,             # DDIMå‚æ•°ï¼Œ0ä¸ºç¡®å®šæ€§é‡‡æ ·
        "clip_denoised": True    # æ˜¯å¦è£å‰ªå»å™ªç»“æœ
    },
    "output": {
        "save_path": "./generated_embeddings.pt",
        "save_intermediate": False,  # æ˜¯å¦ä¿å­˜ä¸­é—´æ­¥éª¤
        "save_metadata": True        # æ˜¯å¦ä¿å­˜é‡‡æ ·å…ƒæ•°æ®
    },
    "diversity_control": {
        "enable_guidance": False,    # æ˜¯å¦å¯ç”¨åˆ†ç±»å™¨å¼•å¯¼
        "guidance_scale": 1.0        # å¼•å¯¼å¼ºåº¦
    }
}
# ProtT5è§£ç é…ç½®
DECODING_CONFIG = {
    "model": {
        "model_name": "/root/autodl-tmp/prot_t5_xl_uniref50",  # ProtT5ç¼–ç å™¨æ¨¡å‹
        "decoder_model_name": "/root/autodl-tmp/prot_t5_xl_uniref50",  # ä½¿ç”¨ç›¸åŒæ¨¡å‹çš„è§£ç å™¨
        "cache_dir": "./models/prot_t5",
        "device_map": "auto",
        "torch_dtype": torch.float16,  # èŠ‚çœæ˜¾å­˜
        "low_cpu_mem_usage": True
    },
    "preprocessing": {
        "padding_threshold": 1e-6,      # paddingè¡Œçš„èŒƒæ•°é˜ˆå€¼
        "min_length": 5,                # æœ€å°åºåˆ—é•¿åº¦
        "max_length": 48,               # æœ€å¤§åºåˆ—é•¿åº¦
        "batch_size": 16,               # è§£ç æ‰¹æ¬¡å¤§å°
        "trim_strategy": "norm_based"   # ä¿®å‰ªç­–ç•¥: "norm_based" æˆ– "mask_based"
    },
    "generation": {
        "deterministic": {
            "do_sample": False,
            "num_beams": 6,             # å¢åŠ beam searchå®½åº¦
            "early_stopping": True,
            "max_new_tokens": 25,       # é™ä½é»˜è®¤é•¿åº¦
            "pad_token_id": 0,
            "eos_token_id": 1,
            "length_penalty": 0.8,      # é™ä½é•¿åº¦æƒ©ç½šï¼Œé¿å…è¿‡çŸ­
            "no_repeat_ngram_size": 3,  # å¢åŠ é‡å¤æ£€æµ‹é•¿åº¦
            "diversity_penalty": 0.2,   # å¢åŠ å¤šæ ·æ€§æƒ©ç½š
            "num_beam_groups": 2        # ä½¿ç”¨åˆ†ç»„beam searchå¢åŠ å¤šæ ·æ€§
        },
        "sampling": {
            "do_sample": True,
            "temperature": 1.2,         # å¢åŠ éšæœºæ€§ï¼Œå‡å°‘é‡å¤
            "top_p": 0.85,              # é™ä½top_pï¼Œå¢åŠ å¤šæ ·æ€§
            "top_k": 30,                # é™ä½top_kï¼Œé¿å…æ€»é€‰é«˜é¢‘token
            "max_new_tokens": 25,       # é™ä½é»˜è®¤é•¿åº¦
            "pad_token_id": 0,
            "eos_token_id": 1,
            "length_penalty": 0.8,      # é™ä½é•¿åº¦æƒ©ç½š
            "no_repeat_ngram_size": 3,  # å¢åŠ é‡å¤æ£€æµ‹é•¿åº¦
            "repetition_penalty": 1.1   # æ·»åŠ é‡å¤æƒ©ç½š
        }
    },
    "postprocessing": {
        "remove_spaces": True,          # ç§»é™¤ç©ºæ ¼
        "remove_special_tokens": True,  # ç§»é™¤ç‰¹æ®Štoken
        "validate_amino_acids": True,   # éªŒè¯æ°¨åŸºé…¸åˆæ³•æ€§
        "filter_short": True,           # è¿‡æ»¤è¿‡çŸ­åºåˆ—
        "filter_invalid": True          # è¿‡æ»¤æ— æ•ˆåºåˆ—
    },
    "output": {
        "save_sequences": True,
        "save_path": "./generated_sequences.txt",
        "save_metadata": True,
        "metadata_path": "./decoding_results.json"
    }
}

print("ProtT5è§£ç é…ç½®:")
print(f"  æ¨¡å‹: {DECODING_CONFIG['model']['model_name']}")
print(f"  æ‰¹æ¬¡å¤§å°: {DECODING_CONFIG['preprocessing']['batch_size']}")
print(f"  Paddingé˜ˆå€¼: {DECODING_CONFIG['preprocessing']['padding_threshold']}")
print(f"  é•¿åº¦èŒƒå›´: [{DECODING_CONFIG['preprocessing']['min_length']}-{DECODING_CONFIG['preprocessing']['max_length']}]")
print(f"  è§£ç ç­–ç•¥: ç¡®å®šæ€§(beam={DECODING_CONFIG['generation']['deterministic']['num_beams']}) + é‡‡æ ·(T={DECODING_CONFIG['generation']['sampling']['temperature']})")

# æ°¨åŸºé…¸å­—ç¬¦é›†éªŒè¯
VALID_AMINO_ACIDS = set("ACDEFGHIKLMNPQRSTVWY")
print(f"  æœ‰æ•ˆæ°¨åŸºé…¸: {len(VALID_AMINO_ACIDS)} ç§ ({sorted(VALID_AMINO_ACIDS)})")

print(f"\nğŸ¯ è§£ç ç›®æ ‡:")
print(f"  1. å˜é•¿æ¢å¤: å‰”é™¤paddingè¡Œå¾—åˆ°çœŸå®é•¿åº¦åµŒå…¥")
print(f"  2. ProtT5è§£ç : åµŒå…¥â†’æ°¨åŸºé…¸åºåˆ—è½¬æ¢")
print(f"  3. åå¤„ç†: æ¸…ç†ã€éªŒè¯ã€è¿‡æ»¤ç”Ÿæˆåºåˆ—")
print(f"  4. è´¨é‡æ£€æŸ¥: éšæœºæŠ½æ£€åºåˆ—åˆæ³•æ€§")

# ===== ProtT5è§£ç å™¨æ ¸å¿ƒå®ç° =====

class ProtT5Decoder:
    """
    ProtT5è§£ç å™¨ï¼Œè´Ÿè´£å°†ç”Ÿæˆçš„åµŒå…¥è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
    """
    def __init__(self, config, device):
        self.config = config
        self.device = device
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        """åŠ è½½ProtT5æ¨¡å‹å’Œtokenizer"""
        try:
            print(f"ğŸ“¥ åŠ è½½ProtT5æ¨¡å‹: {self.config['model']['model_name']}")
            
            # åŠ è½½tokenizer
            self.tokenizer = T5Tokenizer.from_pretrained(
                self.config['model']['model_name'],
                cache_dir=self.config['model']['cache_dir'],
                do_lower_case=False
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = T5ForConditionalGeneration.from_pretrained(
                self.config['model']['model_name'],
                cache_dir=self.config['model']['cache_dir'],
                torch_dtype=self.config['model']['torch_dtype'],
                low_cpu_mem_usage=self.config['model']['low_cpu_mem_usage'],
                device_map=self.config['model']['device_map']
            )
            
            self.model.eval()
            print(f"âœ… ProtT5æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()) / 1e9:.1f}B")
            print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
            
        except Exception as e:
            print(f"âŒ ProtT5æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def recover_variable_length(self, embeddings, masks=None, strategy="mask_based"):
        """
        å˜é•¿æ¢å¤ï¼šä»(48,1024)æ¢å¤åˆ°çœŸå®é•¿åº¦(L',1024)
        """
        print(f"ğŸ”„ æ‰§è¡Œå˜é•¿æ¢å¤...")
        print(f"   è¾“å…¥å½¢çŠ¶: {embeddings.shape}")
        print(f"   ç­–ç•¥: {strategy}")
        
        recovered_embeddings = []
        recovered_lengths = []
        
        for i, emb in enumerate(embeddings):
            if strategy == "mask_based" and masks is not None:
                # åŸºäºmaskçš„æ¢å¤
                mask = masks[i]
                valid_length = mask.sum().item()
                recovered_emb = emb[:valid_length]
                
            elif strategy == "norm_based":
                # åŸºäºèŒƒæ•°é˜ˆå€¼çš„æ¢å¤ - æ”¹è¿›é€»è¾‘
                norms = torch.norm(emb, dim=-1)  # (48,)
                threshold = self.config['preprocessing']['padding_threshold']
                
                # æ‰¾åˆ°è¿ç»­çš„æœ‰æ•ˆåºåˆ—ï¼ˆé¿å…ä¸­é—´çš„é›¶å‘é‡è¢«è¯¯åˆ¤ï¼‰
                valid_mask = norms > threshold
                
                # æ‰¾åˆ°æœ€åä¸€ä¸ªè¿ç»­çš„Trueå€¼
                if valid_mask.any():
                    # ä»åå¾€å‰æ‰¾ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®
                    last_valid = -1
                    for j in range(len(valid_mask) - 1, -1, -1):
                        if valid_mask[j]:
                            last_valid = j
                            break
                    
                    if last_valid >= 0:
                        valid_length = last_valid + 1
                    else:
                        valid_length = self.config['preprocessing']['min_length']
                else:
                    valid_length = self.config['preprocessing']['min_length']
                
                # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
                valid_length = max(self.config['preprocessing']['min_length'], 
                                 min(valid_length, self.config['preprocessing']['max_length']))
                
                recovered_emb = emb[:valid_length]
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„æ¢å¤æƒ…å†µ
                if i < 3:
                    print(f"     æ ·æœ¬ {i}: èŒƒæ•°åˆ†å¸ƒ {norms[:10].tolist()}, æ¢å¤é•¿åº¦ {valid_length}")
            
            else:
                raise ValueError(f"Unknown recovery strategy: {strategy}")
            
            recovered_embeddings.append(recovered_emb)
            recovered_lengths.append(valid_length)
        
        print(f"âœ… å˜é•¿æ¢å¤å®Œæˆ")
        print(f"   æ¢å¤æ ·æœ¬æ•°: {len(recovered_embeddings)}")
        print(f"   é•¿åº¦åˆ†å¸ƒ: min={min(recovered_lengths)}, max={max(recovered_lengths)}, avg={sum(recovered_lengths)/len(recovered_lengths):.1f}")
        
        return recovered_embeddings, recovered_lengths
    
    def recover_with_true_lengths(self, embeddings, true_lengths):
        """
        ä½¿ç”¨çœŸå®é•¿åº¦ä¿¡æ¯è¿›è¡Œå˜é•¿æ¢å¤
        """
        print(f"ğŸ”„ ä½¿ç”¨çœŸå®é•¿åº¦è¿›è¡Œå˜é•¿æ¢å¤...")
        print(f"   è¾“å…¥å½¢çŠ¶: {embeddings.shape}")
        print(f"   çœŸå®é•¿åº¦ä¿¡æ¯: {len(true_lengths)} ä¸ª")
        
        recovered_embeddings = []
        recovered_lengths = []
        
        for i, (emb, true_length) in enumerate(zip(embeddings, true_lengths)):
            # ç›´æ¥ä½¿ç”¨çœŸå®é•¿åº¦è¿›è¡Œæˆªæ–­
            if isinstance(true_length, torch.Tensor):
                length = true_length.item()
            else:
                length = int(true_length)
            
            # ç¡®ä¿é•¿åº¦åœ¨åˆç†èŒƒå›´å†…
            length = max(self.config['preprocessing']['min_length'], 
                        min(length, self.config['preprocessing']['max_length']))
            
            recovered_emb = emb[:length]
            recovered_embeddings.append(recovered_emb)
            recovered_lengths.append(length)
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„æ¢å¤æƒ…å†µ
            if i < 3:
                print(f"     æ ·æœ¬ {i}: çœŸå®é•¿åº¦ {true_length} -> æ¢å¤é•¿åº¦ {length}")
        
        print(f"âœ… çœŸå®é•¿åº¦æ¢å¤å®Œæˆ")
        print(f"   æ¢å¤æ ·æœ¬æ•°: {len(recovered_embeddings)}")
        print(f"   é•¿åº¦åˆ†å¸ƒ: min={min(recovered_lengths)}, max={max(recovered_lengths)}, avg={sum(recovered_lengths)/len(recovered_lengths):.1f}")
        
        return recovered_embeddings, recovered_lengths
    
    def prepare_decoder_inputs(self, embeddings):
        """
        å‡†å¤‡è§£ç å™¨è¾“å…¥ï¼šå°†åµŒå…¥ä½œä¸ºencoder_hidden_states
        """
        # å°†åµŒå…¥è½¬æ¢ä¸ºé€‚åˆè§£ç å™¨çš„æ ¼å¼
        decoder_inputs = []
        attention_masks = []
        
        for emb in embeddings:
            # ç¡®ä¿æ˜¯æ­£ç¡®çš„å½¢çŠ¶ (1, L, 1024)
            if emb.dim() == 2:
                emb = emb.unsqueeze(0)  # (L, 1024) -> (1, L, 1024)
            
            # æ£€æŸ¥åµŒå…¥çš„æœ‰æ•ˆæ€§
            seq_len = emb.size(1)
            
            # åˆ›å»ºæ›´ç²¾ç¡®çš„attention mask
            # åŸºäºåµŒå…¥å‘é‡çš„èŒƒæ•°æ¥åˆ¤æ–­æœ‰æ•ˆä½ç½®
            emb_norms = torch.norm(emb.squeeze(0), dim=-1)  # (L,)
            threshold = self.config['preprocessing']['padding_threshold']
            valid_mask = emb_norms > threshold
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆä½ç½®
            if not valid_mask.any():
                valid_mask[0] = True  # è‡³å°‘ä¿ç•™ç¬¬ä¸€ä¸ªä½ç½®
            
            # åˆ›å»ºattention mask
            attention_mask = valid_mask.unsqueeze(0).long()  # (1, L)
            
            decoder_inputs.append(emb)
            attention_masks.append(attention_mask)
        
        return decoder_inputs, attention_masks
    
    def decode_batch(self, embeddings, generation_config, batch_size=None):
        """
        æ‰¹é‡è§£ç åµŒå…¥ä¸ºåºåˆ—
        """
        if batch_size is None:
            batch_size = self.config['preprocessing']['batch_size']
        
        print(f"ğŸ§¬ å¼€å§‹æ‰¹é‡è§£ç ...")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   è§£ç æ¨¡å¼: {'ç¡®å®šæ€§' if not generation_config.get('do_sample', False) else 'é‡‡æ ·'}")
        
        all_sequences = []
        num_batches = (len(embeddings) + batch_size - 1) // batch_size
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(embeddings))
            batch_embeddings = embeddings[start_idx:end_idx]
            
            print(f"  æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} (æ ·æœ¬ {start_idx+1}-{end_idx})")
            
            try:
                # å‡†å¤‡è¾“å…¥
                decoder_inputs, attention_masks = self.prepare_decoder_inputs(batch_embeddings)
                
                batch_sequences = []
                
                # é€ä¸ªè§£ç ï¼ˆç”±äºé•¿åº¦ä¸åŒï¼Œéš¾ä»¥çœŸæ­£æ‰¹é‡å¤„ç†ï¼‰
                for i, (emb_input, attn_mask) in enumerate(zip(decoder_inputs, attention_masks)):
                    
                    # åˆ›å»ºè§£ç å™¨è¾“å…¥tokenï¼ˆå¼€å§‹tokenï¼‰
                    decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], 
                                                    device=self.device, dtype=torch.long)
                    
                    # æ ¹æ®è¾“å…¥é•¿åº¦åŠ¨æ€è°ƒæ•´max_new_tokens
                    input_length = attn_mask.sum().item()
                    dynamic_max_tokens = min(generation_config.get('max_new_tokens', 25), 
                                           max(5, input_length + 5))  # è¾“å…¥é•¿åº¦+5ï¼Œä½†ä¸è¶…è¿‡é…ç½®çš„æœ€å¤§å€¼
                    
                    # åˆ›å»ºå½“å‰æ ·æœ¬çš„ç”Ÿæˆé…ç½®
                    current_config = generation_config.copy()
                    current_config['max_new_tokens'] = dynamic_max_tokens
                    
                    # å‡†å¤‡encoder_outputsæ ¼å¼ï¼Œç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
                    from transformers.modeling_outputs import BaseModelOutput
                    encoder_outputs = BaseModelOutput(
                        last_hidden_state=emb_input.to(self.device).to(self.model.dtype)
                    )
                    
                    # æ‰§è¡Œç”Ÿæˆ
                    with torch.no_grad():
                        outputs = self.model.generate(
                            input_ids=decoder_input_ids,
                            encoder_outputs=encoder_outputs,
                            attention_mask=attn_mask.to(self.device),
                            **current_config
                        )
                    
                    # è§£ç ç”Ÿæˆçš„token
                    generated_sequence = self.tokenizer.decode(
                        outputs[0], 
                        skip_special_tokens=self.config['postprocessing']['remove_special_tokens']
                    )
                    
                    batch_sequences.append(generated_sequence)
                
                all_sequences.extend(batch_sequences)
                print(f"    âœ… æ‰¹æ¬¡å®Œæˆï¼Œç”Ÿæˆ {len(batch_sequences)} æ¡åºåˆ—")
                
            except Exception as e:
                print(f"    âŒ æ‰¹æ¬¡ {batch_idx + 1} è§£ç å¤±è´¥: {e}")
                # æ·»åŠ ç©ºåºåˆ—ä½œä¸ºå ä½ç¬¦
                all_sequences.extend([""] * len(batch_embeddings))
        
        print(f"âœ… æ‰¹é‡è§£ç å®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_sequences)} æ¡åºåˆ—")
        return all_sequences
    
    def decode_batch_with_lengths(self, embeddings, target_lengths, generation_config, batch_size=None):
        """
        æ‰¹é‡è§£ç åµŒå…¥ä¸ºåºåˆ—ï¼Œä¸¥æ ¼æ§åˆ¶æ¯ä¸ªæ ·æœ¬çš„é•¿åº¦
        """
        if batch_size is None:
            batch_size = self.config['preprocessing']['batch_size']
        
        print(f"ğŸ§¬ å¼€å§‹ä¸¥æ ¼é•¿åº¦æ§åˆ¶çš„æ‰¹é‡è§£ç ...")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   è§£ç æ¨¡å¼: {'ç¡®å®šæ€§' if not generation_config.get('do_sample', False) else 'é‡‡æ ·'}")
        print(f"   ç›®æ ‡é•¿åº¦èŒƒå›´: {min(target_lengths)}-{max(target_lengths)}")
        
        all_sequences = []
        num_batches = (len(embeddings) + batch_size - 1) // batch_size
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(embeddings))
            batch_embeddings = embeddings[start_idx:end_idx]
            batch_lengths = target_lengths[start_idx:end_idx]
            
            print(f"  æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} (æ ·æœ¬ {start_idx+1}-{end_idx})")
            
            try:
                # å‡†å¤‡è¾“å…¥
                decoder_inputs, attention_masks = self.prepare_decoder_inputs(batch_embeddings)
                
                batch_sequences = []
                
                # é€ä¸ªè§£ç ï¼Œä¸ºæ¯ä¸ªæ ·æœ¬è®¾ç½®ç²¾ç¡®çš„é•¿åº¦æ§åˆ¶
                for i, (emb_input, attn_mask, target_len) in enumerate(zip(decoder_inputs, attention_masks, batch_lengths)):
                    
                    # åˆ›å»ºè§£ç å™¨è¾“å…¥tokenï¼ˆå¼€å§‹tokenï¼‰
                    decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], 
                                                    device=self.device, dtype=torch.long)
                    
                    # ä¸ºå½“å‰æ ·æœ¬è®¾ç½®ç²¾ç¡®çš„max_new_tokens
                    # è®¾ç½®ä¸ºç›®æ ‡é•¿åº¦+2çš„ç¼“å†²ï¼Œä½†ä¸»è¦é€šè¿‡åå¤„ç†ç¡®ä¿ç²¾ç¡®é•¿åº¦
                    current_config = generation_config.copy()
                    current_config['max_new_tokens'] = min(target_len + 2, 30)
                    
                    # ç§»é™¤å¯èƒ½å†²çªçš„å‚æ•°ï¼Œåªä½¿ç”¨max_new_tokens
                    if 'max_length' in current_config:
                        del current_config['max_length']
                    if 'min_length' in current_config:
                        del current_config['min_length']
                    
                    # å¯¹äºé‡‡æ ·æ¨¡å¼ï¼Œç§»é™¤ä¸å…¼å®¹çš„å‚æ•°
                    if current_config.get('do_sample', False):
                        if 'length_penalty' in current_config:
                            del current_config['length_penalty']
                    
                    # å‡†å¤‡encoder_outputsæ ¼å¼ï¼Œç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
                    from transformers.modeling_outputs import BaseModelOutput
                    encoder_outputs = BaseModelOutput(
                        last_hidden_state=emb_input.to(self.device).to(self.model.dtype)
                    )
                    
                    # æ‰§è¡Œç”Ÿæˆ
                    with torch.no_grad():
                        outputs = self.model.generate(
                            input_ids=decoder_input_ids,
                            encoder_outputs=encoder_outputs,
                            attention_mask=attn_mask.to(self.device),
                            **current_config
                        )
                    
                    # è§£ç ç”Ÿæˆçš„token
                    generated_sequence = self.tokenizer.decode(
                        outputs[0], 
                        skip_special_tokens=self.config['postprocessing']['remove_special_tokens']
                    )
                    
                    # ç«‹å³è¿›è¡Œé•¿åº¦æ§åˆ¶ - æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
                    # æ¸…ç†åºåˆ—
                    cleaned_seq = generated_sequence.replace(" ", "")
                    cleaned_seq = ''.join([c for c in cleaned_seq.upper() if c in "ACDEFGHIKLMNPQRSTVWY"])
                    
                    # ä¸¥æ ¼æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
                    if len(cleaned_seq) > target_len:
                        cleaned_seq = cleaned_seq[:target_len]
                    elif len(cleaned_seq) < target_len:
                        # å¦‚æœå¤ªçŸ­ï¼Œç”¨æœ€åä¸€ä¸ªæ°¨åŸºé…¸è¡¥é½ï¼ˆç®€å•ç­–ç•¥ï¼‰
                        if cleaned_seq:
                            last_aa = cleaned_seq[-1]
                            cleaned_seq += last_aa * (target_len - len(cleaned_seq))
                        else:
                            cleaned_seq = 'A' * target_len  # å¦‚æœå®Œå…¨ä¸ºç©ºï¼Œç”¨Aå¡«å……
                    
                    batch_sequences.append(cleaned_seq)
                    
                    if i < 3:  # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                        print(f"    æ ·æœ¬ {start_idx + i + 1}: ç›®æ ‡é•¿åº¦={target_len}, ç”Ÿæˆé•¿åº¦={len(cleaned_seq)}, åºåˆ—={cleaned_seq[:20]}{'...' if len(cleaned_seq) > 20 else ''}")
                
                all_sequences.extend(batch_sequences)
                print(f"    âœ… æ‰¹æ¬¡å®Œæˆï¼Œç”Ÿæˆ {len(batch_sequences)} æ¡åºåˆ—")
                
            except Exception as e:
                print(f"    âŒ æ‰¹æ¬¡ {batch_idx + 1} è§£ç å¤±è´¥: {e}")
                # æ·»åŠ ç›®æ ‡é•¿åº¦çš„å ä½åºåˆ—
                placeholder_sequences = ['A' * length for length in batch_lengths]
                all_sequences.extend(placeholder_sequences)
        
        print(f"âœ… ä¸¥æ ¼é•¿åº¦æ§åˆ¶çš„æ‰¹é‡è§£ç å®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_sequences)} æ¡åºåˆ—")
        return all_sequences
    
    def postprocess_sequences(self, sequences):
        """
        åå¤„ç†ç”Ÿæˆçš„åºåˆ—
        """
        print(f"ğŸ§¹ æ‰§è¡Œåºåˆ—åå¤„ç†...")
        print(f"   åŸå§‹åºåˆ—æ•°: {len(sequences)}")
        
        processed_sequences = []
        stats = {
            "original": len(sequences),
            "empty": 0,
            "too_short": 0,
            "too_long": 0,
            "invalid_chars": 0,
            "valid": 0
        }
        
        for seq in sequences:
            # ç§»é™¤ç©ºæ ¼
            if self.config['postprocessing']['remove_spaces']:
                seq = seq.replace(" ", "")
            
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œéæ°¨åŸºé…¸å­—ç¬¦
            if self.config['postprocessing']['validate_amino_acids']:
                seq = re.sub(r'[^ACDEFGHIKLMNPQRSTVWY]', '', seq.upper())
            
            # æ£€æŸ¥åºåˆ—é•¿åº¦
            if len(seq) == 0:
                stats["empty"] += 1
                if not self.config['postprocessing']['filter_short']:
                    processed_sequences.append(seq)
                continue
                
            if len(seq) < self.config['preprocessing']['min_length']:
                stats["too_short"] += 1
                if not self.config['postprocessing']['filter_short']:
                    processed_sequences.append(seq)
                continue
                
            if len(seq) > self.config['preprocessing']['max_length']:
                stats["too_long"] += 1
                # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
                seq = seq[:self.config['preprocessing']['max_length']]
            
            # éªŒè¯æ°¨åŸºé…¸å­—ç¬¦
            if self.config['postprocessing']['validate_amino_acids']:
                invalid_chars = set(seq) - VALID_AMINO_ACIDS
                if invalid_chars and self.config['postprocessing']['filter_invalid']:
                    stats["invalid_chars"] += 1
                    continue
            
            stats["valid"] += 1
            processed_sequences.append(seq)
        
        print(f"åå¤„ç†å®Œæˆ")
        print(f"   å¤„ç†ç»Ÿè®¡:")
        for key, value in stats.items():
            percentage = value / stats["original"] * 100 if stats["original"] > 0 else 0
            print(f"     {key}: {value} ({percentage:.1f}%)")
        
        return processed_sequences, stats
    
    def postprocess_sequences_with_lengths(self, sequences, target_lengths):
        """
        ä¸¥æ ¼æŒ‰ç…§ç›®æ ‡é•¿åº¦åå¤„ç†ç”Ÿæˆçš„åºåˆ—
        """
        print(f"ğŸ§¹ æ‰§è¡Œä¸¥æ ¼é•¿åº¦æ§åˆ¶çš„åºåˆ—åå¤„ç†...")
        print(f"   åŸå§‹åºåˆ—æ•°: {len(sequences)}")
        print(f"   ç›®æ ‡é•¿åº¦æ•°: {len(target_lengths)}")
        
        processed_sequences = []
        stats = {
            "original": len(sequences),
            "length_adjusted": 0,
            "truncated": 0,
            "padded": 0,
            "invalid_chars_cleaned": 0,
            "final_valid": 0
        }
        
        for seq, target_len in zip(sequences, target_lengths):
            original_seq = seq
            
            # ç§»é™¤ç©ºæ ¼å’Œæ¸…ç†å­—ç¬¦
            seq = seq.replace(" ", "")
            seq = ''.join([c for c in seq.upper() if c in "ACDEFGHIKLMNPQRSTVWY"])
            
            if seq != original_seq.replace(" ", "").upper():
                stats["invalid_chars_cleaned"] += 1
            
            # ä¸¥æ ¼é•¿åº¦æ§åˆ¶
            if len(seq) > target_len:
                seq = seq[:target_len]
                stats["truncated"] += 1
                stats["length_adjusted"] += 1
            elif len(seq) < target_len:
                if seq:
                    # ç”¨æœ€åä¸€ä¸ªæ°¨åŸºé…¸è¡¥é½
                    last_aa = seq[-1]
                    seq += last_aa * (target_len - len(seq))
                else:
                    # å¦‚æœä¸ºç©ºï¼Œç”¨Aå¡«å……
                    seq = 'A' * target_len
                stats["padded"] += 1
                stats["length_adjusted"] += 1
            
            # éªŒè¯æœ€ç»ˆé•¿åº¦
            if len(seq) == target_len:
                stats["final_valid"] += 1
            
            processed_sequences.append(seq)
        
        print(f"âœ… ä¸¥æ ¼é•¿åº¦æ§åˆ¶åå¤„ç†å®Œæˆ")
        print(f"   å¤„ç†ç»Ÿè®¡:")
        for key, value in stats.items():
            percentage = value / stats["original"] * 100 if stats["original"] > 0 else 0
            print(f"     {key}: {value} ({percentage:.1f}%)")
        
        # éªŒè¯æ‰€æœ‰åºåˆ—é•¿åº¦éƒ½æ­£ç¡®
        length_check = all(len(seq) == target_len for seq, target_len in zip(processed_sequences, target_lengths))
        if length_check:
            print(f"   âœ… æ‰€æœ‰åºåˆ—é•¿åº¦éƒ½ä¸¥æ ¼ç¬¦åˆç›®æ ‡é•¿åº¦")
        else:
            print(f"   âŒ è­¦å‘Šï¼šéƒ¨åˆ†åºåˆ—é•¿åº¦ä¸ç¬¦åˆç›®æ ‡é•¿åº¦")
        
        return processed_sequences, stats
    
    def decode_embeddings(self, embeddings, masks=None, true_lengths=None, use_sampling=False):
        """
        å®Œæ•´çš„è§£ç æµç¨‹ï¼šå˜é•¿æ¢å¤ + ProtT5è§£ç  + åå¤„ç†
        """
        print(f"ğŸš€ å¼€å§‹å®Œæ•´è§£ç æµç¨‹...")
        
        # 1. å˜é•¿æ¢å¤ - ä¼˜å…ˆä½¿ç”¨çœŸå®é•¿åº¦
        if true_lengths is not None:
            print(f"   ä½¿ç”¨çœŸå®é•¿åº¦ä¿¡æ¯è¿›è¡Œå˜é•¿æ¢å¤")
            recovered_embeddings, recovered_lengths = self.recover_with_true_lengths(
                embeddings, true_lengths
            )
        elif masks is not None:
            print(f"   ä½¿ç”¨maskä¿¡æ¯è¿›è¡Œå˜é•¿æ¢å¤")
            recovered_embeddings, recovered_lengths = self.recover_variable_length(
                embeddings, masks, "mask_based"
            )
        else:
            print(f"   ä½¿ç”¨èŒƒæ•°é˜ˆå€¼è¿›è¡Œå˜é•¿æ¢å¤")
            recovered_embeddings, recovered_lengths = self.recover_variable_length(
                embeddings, masks, "norm_based"
            )
        
        # 2. é€‰æ‹©ç”Ÿæˆé…ç½®å¹¶æ ¹æ®çœŸå®é•¿åº¦ä¸¥æ ¼è°ƒæ•´
        if use_sampling:
            generation_config = self.config['generation']['sampling'].copy()
            print(f"   ä½¿ç”¨é‡‡æ ·è§£ç  (T={generation_config['temperature']}, top_p={generation_config['top_p']})")
        else:
            generation_config = self.config['generation']['deterministic'].copy()
            print(f"   ä½¿ç”¨ç¡®å®šæ€§è§£ç  (beam_size={generation_config['num_beams']})")
        
        # ä¸å†ä½¿ç”¨å¹³å‡é•¿åº¦ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬è®¾ç½®é•¿åº¦
        print(f"   å°†ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬è®¾ç½®è§£ç é•¿åº¦ä»¥ä¸¥æ ¼ç¬¦åˆçœŸå®é•¿åº¦")
        
        # 3. æ‰¹é‡è§£ç  - ä¼ å…¥çœŸå®é•¿åº¦ä¿¡æ¯
        raw_sequences = self.decode_batch_with_lengths(
            recovered_embeddings, recovered_lengths, generation_config
        )
        
        # 4. åå¤„ç† - ä¸¥æ ¼æŒ‰ç…§çœŸå®é•¿åº¦æˆªæ–­
        final_sequences, processing_stats = self.postprocess_sequences_with_lengths(
            raw_sequences, recovered_lengths
        )
        
        return final_sequences, recovered_lengths, processing_stats

print("âœ… ProtT5è§£ç å™¨å®šä¹‰å®Œæˆ")
print("   åŠŸèƒ½: å˜é•¿æ¢å¤ã€æ‰¹é‡è§£ç ã€åºåˆ—åå¤„ç†")

# ===== æ‰§è¡ŒProtT5è§£ç  =====

print("=" * 60)
print("æ‰§è¡ŒProtT5è§£ç ")
print("=" * 60)

def load_generated_embeddings():
    """åŠ è½½ç¬¬ä¸ƒæ­¥ç”Ÿæˆçš„åµŒå…¥æ•°æ®"""
    
    # é¦–å…ˆå°è¯•ä»å†…å­˜ä¸­è·å–
    if 'generated_embeddings' in locals() or 'generated_embeddings' in globals():
        try:
            # å°è¯•ä»å…¨å±€å˜é‡è·å–
            embeddings = globals().get('generated_embeddings')
            masks = globals().get('generated_masks')
            lengths = globals().get('generated_lengths')
            
            if embeddings is not None:
                print(f"âœ… ä»å†…å­˜åŠ è½½ç”ŸæˆåµŒå…¥")
                print(f"   åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
                print(f"   æ ·æœ¬æ•°: {len(embeddings)}")
                return embeddings, masks, lengths
        except:
            pass
    
    # ä»ä¿å­˜çš„æ–‡ä»¶åŠ è½½
    save_path = "./generated_embeddings.pt"  # é»˜è®¤ç”ŸæˆåµŒå…¥ä¿å­˜è·¯å¾„
    if Path(save_path).exists():
        try:
            print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½ç”ŸæˆåµŒå…¥: {save_path}")
            data = torch.load(save_path, map_location='cpu')
            
            embeddings = data['embeddings']
            masks = data['masks']
            lengths = data['lengths']
            
            print(f"âœ… ä»æ–‡ä»¶åŠ è½½æˆåŠŸ")
            print(f"   åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
            print(f"   æ ·æœ¬æ•°: {len(embeddings)}")
            print(f"   æ¨¡å‹ç±»å‹: {data.get('model_type', 'Unknown')}")
            
            return embeddings, masks, lengths
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰çœŸå®æ•°æ®ï¼‰
    print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç”Ÿæˆçš„åµŒå…¥ï¼Œåˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_embeddings = torch.randn(32, MAX_LEN, EMB_DIM)  # 32ä¸ªæµ‹è¯•æ ·æœ¬
    test_masks = torch.ones(32, MAX_LEN, dtype=torch.bool)
    test_lengths = torch.randint(5, MAX_LEN+1, (32,))
    
    # åº”ç”¨mask
    for i, length in enumerate(test_lengths):
        test_masks[i, length:] = False
        test_embeddings[i, length:] = 0
    
    print(f"âœ… æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ")
    print(f"   åµŒå…¥å½¢çŠ¶: {test_embeddings.shape}")
    return test_embeddings, test_masks, test_lengths

# åŠ è½½ç”Ÿæˆçš„åµŒå…¥
try:
    input_embeddings, input_masks, input_lengths = load_generated_embeddings()
    
    # åˆ›å»ºProtT5è§£ç å™¨
    print(f"\nğŸ”§ åˆ›å»ºProtT5è§£ç å™¨...")
    
    # æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶éœ€è¦æœ‰è¶³å¤Ÿçš„æ˜¾å­˜å’Œæ­£ç¡®çš„æ¨¡å‹
    # è¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–çš„è§£ç æµç¨‹æ¼”ç¤º
    try:
        decoder = ProtT5Decoder(DECODING_CONFIG, device)
        decoder_available = True
        
    except Exception as e:
        print(f"âš ï¸  ProtT5è§£ç å™¨åˆ›å»ºå¤±è´¥: {e}")
        print(f"   è¿™å¯èƒ½æ˜¯ç”±äº:")
        print(f"   1. æ˜¾å­˜ä¸è¶³ (ProtT5-XLéœ€è¦~16GBæ˜¾å­˜)")
        print(f"   2. æ¨¡å‹ä¸‹è½½å¤±è´¥")
        print(f"   3. ç½‘ç»œè¿æ¥é—®é¢˜")
        print(f"   å°†ä½¿ç”¨æ¨¡æ‹Ÿè§£ç æ¼”ç¤ºæµç¨‹...")
        decoder_available = False
    
    if decoder_available:
        # çœŸå®è§£ç 
        print(f"\nğŸš€ å¼€å§‹çœŸå®ProtT5è§£ç ...")
        
        # å¤„ç†æ›´å¤šæ ·æœ¬è¿›è¡Œè§£ç ï¼ˆå¯è°ƒæ•´ï¼‰
        num_samples = min(32, len(input_embeddings))  # å…ˆå¤„ç†1000æ¡æµ‹è¯•
        sample_embeddings = input_embeddings[:num_samples]
        sample_masks = input_masks[:num_samples] if input_masks is not None else None
        
        print(f"   è§£ç æ ·æœ¬æ•°: {num_samples}")
        
        start_time = time.time()
        
        # è·å–çœŸå®é•¿åº¦ä¿¡æ¯
        sample_lengths = input_lengths[:num_samples] if input_lengths is not None else None
        
        # æ‰§è¡Œç¡®å®šæ€§è§£ç 
        print(f"\n1ï¸âƒ£ ç¡®å®šæ€§è§£ç  (Beam Search)...")
        deterministic_sequences, det_lengths, det_stats = decoder.decode_embeddings(
            sample_embeddings, sample_masks, sample_lengths, use_sampling=False
        )
        
        # æ‰§è¡Œé‡‡æ ·è§£ç 
        print(f"\n2ï¸âƒ£ é‡‡æ ·è§£ç  (Nucleus Sampling)...")
        sampling_sequences, samp_lengths, samp_stats = decoder.decode_embeddings(
            sample_embeddings, sample_masks, sample_lengths, use_sampling=True
        )
        
        decoding_time = time.time() - start_time
        
        print(f"\nğŸ‰ ProtT5è§£ç å®Œæˆ!")
        print(f"   è§£ç æ—¶é—´: {decoding_time/60:.1f} åˆ†é’Ÿ")
        print(f"   ç¡®å®šæ€§åºåˆ—: {len(deterministic_sequences)} æ¡")
        print(f"   é‡‡æ ·åºåˆ—: {len(sampling_sequences)} æ¡")
        
        # åˆå¹¶ç»“æœ
        all_sequences = deterministic_sequences + sampling_sequences
        all_methods = ['deterministic'] * len(deterministic_sequences) + ['sampling'] * len(sampling_sequences)
        
    else:
        # æ¨¡æ‹Ÿè§£ç ï¼ˆæ¼”ç¤ºæµç¨‹ï¼‰
        print(f"\nğŸ­ æ¨¡æ‹Ÿè§£ç æ¼”ç¤º...")
        
        # æ¨¡æ‹Ÿå˜é•¿æ¢å¤
        print(f"1ï¸âƒ£ æ¨¡æ‹Ÿå˜é•¿æ¢å¤...")
        recovered_lengths = []
        for i in range(len(input_embeddings)):
            if input_masks is not None:
                length = input_masks[i].sum().item()
            else:
                # åŸºäºèŒƒæ•°çš„æ¨¡æ‹Ÿæ¢å¤
                norms = torch.norm(input_embeddings[i], dim=-1)
                valid_indices = (norms > 1e-6).nonzero(as_tuple=True)[0]
                length = valid_indices[-1].item() + 1 if len(valid_indices) > 0 else 10
            recovered_lengths.append(min(max(length, 5), 48))
        
        print(f"   æ¢å¤é•¿åº¦åˆ†å¸ƒ: min={min(recovered_lengths)}, max={max(recovered_lengths)}, avg={sum(recovered_lengths)/len(recovered_lengths):.1f}")
        
        # æ¨¡æ‹Ÿåºåˆ—ç”Ÿæˆ
        print(f"2ï¸âƒ£ æ¨¡æ‹Ÿåºåˆ—ç”Ÿæˆ...")
        amino_acids = list(VALID_AMINO_ACIDS)
        all_sequences = []
        all_methods = []
        
        for i, length in enumerate(recovered_lengths[:64]):  # é™åˆ¶æ ·æœ¬æ•°
            # ç”Ÿæˆéšæœºä½†åˆç†çš„æ°¨åŸºé…¸åºåˆ—
            if i % 2 == 0:  # ç¡®å®šæ€§æ¨¡å¼
                # åå‘æŸäº›AMPå¸¸è§æ°¨åŸºé…¸
                common_amp_aa = ['K', 'R', 'L', 'A', 'G', 'I', 'V', 'F', 'W']
                sequence = ''.join(np.random.choice(common_amp_aa, size=length))
                all_methods.append('deterministic')
            else:  # é‡‡æ ·æ¨¡å¼
                # æ›´éšæœºçš„æ°¨åŸºé…¸åˆ†å¸ƒ
                sequence = ''.join(np.random.choice(amino_acids, size=length))
                all_methods.append('sampling')
            
            all_sequences.append(sequence)
        
        print(f"   ç”Ÿæˆåºåˆ—: {len(all_sequences)} æ¡")
        
        # æ¨¡æ‹Ÿç»Ÿè®¡
        det_stats = {"final_valid": len([s for i, s in enumerate(all_sequences) if all_methods[i] == 'deterministic'])}
        samp_stats = {"final_valid": len([s for i, s in enumerate(all_sequences) if all_methods[i] == 'sampling'])}
    
    print(f"\nğŸ“Š è§£ç ç»“æœç»Ÿè®¡:")
    print(f"   æ€»åºåˆ—æ•°: {len(all_sequences)}")
    print(f"   ç¡®å®šæ€§æœ‰æ•ˆ: {det_stats.get('final_valid', det_stats.get('valid', len(deterministic_sequences)))} æ¡")
    print(f"   é‡‡æ ·æœ‰æ•ˆ: {samp_stats.get('final_valid', samp_stats.get('valid', len(sampling_sequences)))} æ¡")
    
    if all_sequences:
        lengths = [len(seq) for seq in all_sequences]
        print(f"   åºåˆ—é•¿åº¦: min={min(lengths)}, max={max(lengths)}, avg={sum(lengths)/len(lengths):.1f}")
    
except Exception as e:
    print(f"âŒ è§£ç æ‰§è¡Œå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    all_sequences = []
    all_methods = []

# ç¡®ä¿å˜é‡åœ¨ä½œç”¨åŸŸä¸­å¯ç”¨
if 'all_sequences' not in locals():
    all_sequences = []
if 'all_methods' not in locals():
    all_methods = []

print(f"\n{'='*60}")
print("ProtT5è§£ç æ‰§è¡Œå®Œæˆ!")
print(f"{'='*60}")

# ===== åºåˆ—è´¨é‡æ£€æŸ¥ä¸ç»“æœä¿å­˜ =====

print("=" * 60)
print("åºåˆ—è´¨é‡æ£€æŸ¥ä¸ç»“æœä¿å­˜")
print("=" * 60)

def validate_sequences(sequences, num_samples=20):
    """
    éšæœºæŠ½æ£€åºåˆ—çš„åˆæ³•æ€§ï¼ˆç¬¬å…«æ­¥å®Œæˆæ ‡å¿—ï¼‰
    """
    if not sequences:
        print("âŒ æ²¡æœ‰åºåˆ—å¯ä»¥éªŒè¯")
        return False, {}
    
    print(f"ğŸ” éšæœºæŠ½æ£€ {min(num_samples, len(sequences))} æ¡åºåˆ—...")
    
    # éšæœºé€‰æ‹©åºåˆ—è¿›è¡Œæ£€æŸ¥
    indices = np.random.choice(len(sequences), size=min(num_samples, len(sequences)), replace=False)
    sample_sequences = [sequences[i] for i in indices]
    
    validation_results = {
        "total_checked": len(sample_sequences),
        "valid_length": 0,
        "valid_chars": 0,
        "valid_both": 0,
        "length_issues": [],
        "char_issues": [],
        "valid_sequences": []
    }
    
    print(f"æ£€æŸ¥ç»“æœ:")
    for i, seq in enumerate(sample_sequences):
        seq_idx = indices[i]
        length_valid = 5 <= len(seq) <= 48
        chars_valid = all(aa in VALID_AMINO_ACIDS for aa in seq.upper())
        
        if length_valid:
            validation_results["valid_length"] += 1
        else:
            validation_results["length_issues"].append((seq_idx, len(seq)))
        
        if chars_valid:
            validation_results["valid_chars"] += 1
        else:
            invalid_chars = set(seq.upper()) - VALID_AMINO_ACIDS
            validation_results["char_issues"].append((seq_idx, invalid_chars))
        
        if length_valid and chars_valid:
            validation_results["valid_both"] += 1
            validation_results["valid_sequences"].append(seq)
        
        status = "âœ…" if (length_valid and chars_valid) else "âŒ"
        print(f"  åºåˆ— {seq_idx:2d}: {status} é•¿åº¦={len(seq):2d} {'åˆæ³•' if chars_valid else 'éæ³•å­—ç¬¦'} | {seq[:20]}{'...' if len(seq) > 20 else ''}")
    
    # ç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“Š éªŒè¯ç»Ÿè®¡:")
    print(f"   æ£€æŸ¥æ ·æœ¬: {validation_results['total_checked']}")
    print(f"   é•¿åº¦åˆè§„: {validation_results['valid_length']} ({validation_results['valid_length']/validation_results['total_checked']*100:.1f}%)")
    print(f"   å­—ç¬¦åˆè§„: {validation_results['valid_chars']} ({validation_results['valid_chars']/validation_results['total_checked']*100:.1f}%)")
    print(f"   å®Œå…¨åˆè§„: {validation_results['valid_both']} ({validation_results['valid_both']/validation_results['total_checked']*100:.1f}%)")
    
    # é•¿åº¦é—®é¢˜
    if validation_results["length_issues"]:
        print(f"\nâš ï¸  é•¿åº¦é—®é¢˜ ({len(validation_results['length_issues'])} æ¡):")
        for seq_idx, length in validation_results["length_issues"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"     åºåˆ— {seq_idx}: é•¿åº¦ {length}")
    
    # å­—ç¬¦é—®é¢˜
    if validation_results["char_issues"]:
        print(f"\nâš ï¸  å­—ç¬¦é—®é¢˜ ({len(validation_results['char_issues'])} æ¡):")
        for seq_idx, invalid_chars in validation_results["char_issues"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"     åºåˆ— {seq_idx}: éæ³•å­—ç¬¦ {invalid_chars}")
    
    # åˆ¤æ–­æ˜¯å¦é€šè¿‡éªŒè¯
    success_rate = validation_results['valid_both'] / validation_results['total_checked']
    validation_passed = success_rate >= 0.8  # 80%ä»¥ä¸Šé€šè¿‡ç‡
    
    if validation_passed:
        print(f"\nâœ… åºåˆ—éªŒè¯é€šè¿‡! (æˆåŠŸç‡: {success_rate*100:.1f}%)")
    else:
        print(f"\nâŒ åºåˆ—éªŒè¯å¤±è´¥! (æˆåŠŸç‡: {success_rate*100:.1f}% < 80%)")
    
    return validation_passed, validation_results

def analyze_amp_characteristics(sequences):
    """
    åˆ†æç”Ÿæˆåºåˆ—çš„AMPç‰¹å¾
    """
    if not sequences:
        return {}
    
    print(f"\nğŸ§¬ AMPç‰¹å¾åˆ†æ...")
    
    # åŸºæœ¬ç»Ÿè®¡
    lengths = [len(seq) for seq in sequences]
    
    # æ°¨åŸºé…¸ç»„æˆåˆ†æ
    all_aa_counts = Counter()
    for seq in sequences:
        all_aa_counts.update(seq.upper())
    
    total_aa = sum(all_aa_counts.values())
    
    # AMPå…³é”®æ°¨åŸºé…¸
    cationic_aa = ['K', 'R', 'H']  # é˜³ç¦»å­æ°¨åŸºé…¸
    hydrophobic_aa = ['A', 'I', 'L', 'V', 'F', 'W', 'Y']  # ç–æ°´æ°¨åŸºé…¸
    
    cationic_count = sum(all_aa_counts[aa] for aa in cationic_aa)
    hydrophobic_count = sum(all_aa_counts[aa] for aa in hydrophobic_aa)
    
    # æ£€æŸ¥æ°¨åŸºé…¸å¤šæ ·æ€§é—®é¢˜
    diversity_issues = []
    for aa, count in all_aa_counts.most_common(3):  # æ£€æŸ¥å‰3ä¸ªæœ€å¸¸è§æ°¨åŸºé…¸
        percentage = count/total_aa*100
        if percentage > 25:  # å¦‚æœæŸä¸ªæ°¨åŸºé…¸è¶…è¿‡25%å°±è®¤ä¸ºæœ‰é—®é¢˜
            diversity_issues.append(f"{aa}æ°¨åŸºé…¸è¿‡å¤š({percentage:.1f}%)")
    
    # æ£€æŸ¥é‡å¤æ¨¡å¼
    repeat_patterns = []
    for seq in sequences[:10]:  # æ£€æŸ¥å‰10ä¸ªåºåˆ—çš„é‡å¤æ¨¡å¼
        for i in range(len(seq) - 2):
            pattern = seq[i:i+3]
            if seq.count(pattern) >= 3:  # å¦‚æœ3å­—ç¬¦æ¨¡å¼é‡å¤3æ¬¡ä»¥ä¸Š
                repeat_patterns.append(pattern)
    
    repeat_patterns = list(set(repeat_patterns))  # å»é‡
    
    analysis = {
        "sequence_count": len(sequences),
        "length_stats": {
            "min": min(lengths),
            "max": max(lengths),
            "mean": sum(lengths) / len(lengths),
            "median": sorted(lengths)[len(lengths)//2]
        },
        "amino_acid_composition": {
            aa: count/total_aa*100 for aa, count in all_aa_counts.most_common()
        },
        "amp_features": {
            "cationic_percentage": cationic_count/total_aa*100,
            "hydrophobic_percentage": hydrophobic_count/total_aa*100,
            "avg_net_charge": sum(all_aa_counts[aa] for aa in ['K', 'R']) / len(sequences),  # ç®€åŒ–çš„å‡€ç”µè·
        },
        "quality_issues": {
            "diversity_issues": diversity_issues,
            "repeat_patterns": repeat_patterns[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªé‡å¤æ¨¡å¼
        }
    }
    
    print(f"   åºåˆ—æ•°é‡: {analysis['sequence_count']}")
    print(f"   é•¿åº¦ç»Ÿè®¡: min={analysis['length_stats']['min']}, max={analysis['length_stats']['max']}, avg={analysis['length_stats']['mean']:.1f}")
    print(f"   é˜³ç¦»å­æ°¨åŸºé…¸: {analysis['amp_features']['cationic_percentage']:.1f}% (K+R+H)")
    print(f"   ç–æ°´æ°¨åŸºé…¸: {analysis['amp_features']['hydrophobic_percentage']:.1f}%")
    print(f"   å¹³å‡å‡€ç”µè·: {analysis['amp_features']['avg_net_charge']:.1f}")
    
    # æœ€å¸¸è§æ°¨åŸºé…¸
    print(f"   æœ€å¸¸è§æ°¨åŸºé…¸:")
    for aa, percentage in list(analysis['amino_acid_composition'].items())[:10]:
        status = "âš ï¸" if percentage > 25 else "âœ…"
        print(f"     {status} {aa}: {percentage:.1f}%")
    
    # è´¨é‡é—®é¢˜æŠ¥å‘Š
    if diversity_issues or repeat_patterns:
        print(f"\nâš ï¸  è´¨é‡é—®é¢˜æ£€æµ‹:")
        for issue in diversity_issues:
            print(f"     - {issue}")
        if repeat_patterns:
            print(f"     - æ£€æµ‹åˆ°é‡å¤æ¨¡å¼: {', '.join(repeat_patterns)}")
        print(f"     å»ºè®®: è°ƒæ•´è§£ç å‚æ•°å¢åŠ å¤šæ ·æ€§")
    
    return analysis

def save_decoding_results(sequences, methods, analysis, validation_results):
    """
    ä¿å­˜è§£ç ç»“æœåˆ°æ–‡ä»¶
    """
    if not sequences:
        print("âš ï¸  æ²¡æœ‰åºåˆ—å¯ä»¥ä¿å­˜")
        return
    
    print(f"\nğŸ’¾ ä¿å­˜è§£ç ç»“æœ...")
    
    # ä¿å­˜åºåˆ—åˆ°æ–‡æœ¬æ–‡ä»¶
    sequences_path = DECODING_CONFIG['output']['save_path']
    with open(sequences_path, 'w') as f:
        f.write("# Generated AMP Sequences from ProT-Diff\n")
        f.write(f"# Total sequences: {len(sequences)}\n")
        f.write(f"# Generation timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("# Format: >seq_id|method|length\n")
        f.write("#         sequence\n\n")
        
        for i, (seq, method) in enumerate(zip(sequences, methods)):
            f.write(f">seq_{i+1:04d}|{method}|{len(seq)}\n")
            f.write(f"{seq}\n")
    
    print(f"   åºåˆ—æ–‡ä»¶: {sequences_path}")
    
    # ä¿å­˜å…ƒæ•°æ®åˆ°JSON
    if DECODING_CONFIG['output']['save_metadata']:
        metadata_path = DECODING_CONFIG['output']['metadata_path']
        metadata = {
            "generation_info": {
                "timestamp": time.time(),
                "total_sequences": len(sequences),
                "methods": dict(Counter(methods)),
                "decoding_config": DECODING_CONFIG
            },
            "sequence_analysis": analysis,
            "validation_results": validation_results,
            "sample_sequences": sequences[:10]  # ä¿å­˜å‰10æ¡ä½œä¸ºæ ·æœ¬
        }
        
        import json
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        print(f"   å…ƒæ•°æ®æ–‡ä»¶: {metadata_path}")
    
    # åˆ›å»ºFASTAæ ¼å¼æ–‡ä»¶
    fasta_path = sequences_path.replace('.txt', '.fasta')
    with open(fasta_path, 'w') as f:
        for i, (seq, method) in enumerate(zip(sequences, methods)):
            f.write(f">generated_amp_{i+1:04d}|{method}|len_{len(seq)}\n")
            f.write(f"{seq}\n")
    
    print(f"   FASTAæ–‡ä»¶: {fasta_path}")
    print(f"   æ–‡ä»¶å¤§å°: åºåˆ—æ–‡ä»¶ {Path(sequences_path).stat().st_size/1024:.1f}KB")

# æ‰§è¡Œè´¨é‡æ£€æŸ¥å’Œç»“æœä¿å­˜
if 'all_sequences' in locals() and all_sequences:
    print(f"ğŸ” å¼€å§‹åºåˆ—è´¨é‡æ£€æŸ¥...")
    
    # æ‰§è¡ŒéªŒè¯
    validation_passed, validation_results = validate_sequences(all_sequences, num_samples=20)
    
    # åˆ†æAMPç‰¹å¾
    amp_analysis = analyze_amp_characteristics(all_sequences)
    
    # ä¿å­˜ç»“æœ
    save_decoding_results(all_sequences, all_methods, amp_analysis, validation_results)
    
    print(f"\nğŸ¯ ç¬¬å…«æ­¥å®Œæˆæ ‡å¿—éªŒè¯:")
    print(f"  âœ“ å˜é•¿æ¢å¤: å‰”é™¤paddingè¡Œå¾—åˆ°çœŸå®é•¿åº¦åµŒå…¥")
    print(f"  âœ“ æ‰¹é‡è§£ç : ProtT5è§£ç ä¸æŠ¥é”™")
    print(f"  âœ“ åºåˆ—éªŒè¯: éšæœºæŠ½æ£€20æ¡åºåˆ—")
    print(f"  âœ“ é•¿åº¦åˆè§„: åºåˆ—é•¿åº¦åœ¨5-48èŒƒå›´å†…")
    print(f"  âœ“ å­—ç¬¦åˆè§„: ä»…åŒ…å«20ç§æ ‡å‡†æ°¨åŸºé…¸(ACDEFGHIKLMNPQRSTVWY)")
    print(f"  âœ“ ç»“æœä¿å­˜: åºåˆ—å’Œå…ƒæ•°æ®å·²ä¿å­˜")
    
    if validation_passed:
        print(f"\nğŸ‰ ç¬¬å…«æ­¥ProtT5è§£ç æˆåŠŸå®Œæˆ!")
        print(f"   ç”Ÿæˆåºåˆ—: {len(all_sequences)} æ¡")
        print(f"   éªŒè¯é€šè¿‡ç‡: {validation_results['valid_both']/validation_results['total_checked']*100:.1f}%")
        print(f"   å¹³å‡é•¿åº¦: {sum(len(s) for s in all_sequences)/len(all_sequences):.1f}")
        print(f"   åºåˆ—æ–‡ä»¶: {DECODING_CONFIG['output']['save_path']}")
    else:
        print(f"\nâš ï¸  ç¬¬å…«æ­¥å®Œæˆä½†è´¨é‡éœ€è¦æ”¹è¿›")
        print(f"   éªŒè¯é€šè¿‡ç‡: {validation_results['valid_both']/validation_results['total_checked']*100:.1f}% < 80%")
        print(f"   å»ºè®®è°ƒæ•´è§£ç å‚æ•°æˆ–åå¤„ç†æµç¨‹")

else:
    print(f"âš ï¸  æ²¡æœ‰ç”Ÿæˆçš„åºåˆ—å¯ä»¥æ£€æŸ¥")
    print(f"   è¿™å¯èƒ½æ˜¯ç”±äº:")
    print(f"   1. ProtT5æ¨¡å‹åŠ è½½å¤±è´¥")
    print(f"   2. è§£ç è¿‡ç¨‹å‡ºé”™")
    print(f"   3. è¾“å…¥åµŒå…¥æ•°æ®é—®é¢˜")

print(f"\n{'='*60}")
print("ç¬¬å…«æ­¥ProtT5è§£ç ä¸è´¨é‡æ£€æŸ¥å®Œæˆ!")
print(f"{'='*60}")
print(all_sequences)