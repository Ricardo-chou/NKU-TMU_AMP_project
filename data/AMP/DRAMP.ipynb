{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "DATABASE_NAME = 'DRAMP'\n",
    "ANTIBACTERIA_LINK_PAGES = 326\n",
    "bacteria_regex = r'(?P<bacterium>[A-Z]\\. [a-z]+)(?P<strain>\\s?[A-Z]+\\s?[0-9]+)?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find links to each DRAMP peptide page\n",
    "find_links_base = 'http://dramp.cpu-bioinfor.org/browse/ActivityData.php?order=antibacterial&pageNow='\n",
    "drampids = []\n",
    "for i in range(1, ANTIBACTERIA_LINK_PAGES):\n",
    "    url = find_links_base + str(i)\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    matches = re.finditer('value\\=\\\"(?P<drampid>DRAMP[0-9]+)\\\"', str(soup))\n",
    "    drampids += [m.groupdict()['drampid'] for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of bacteria names\n",
    "bacteria_list_url_base = 'http://www.thelabrat.com/protocols/Bacterialspecies/byname'\n",
    "bacteria_names = []\n",
    "import string\n",
    "for letter in string.ascii_uppercase:\n",
    "    url = bacteria_list_url_base + letter + '.shtml'\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.text\n",
    "    # start = text.index('Y | Z \\n\\n\\') + len(\"Y | Z \\n\\n\")\n",
    "    end = start + text[start:].index('\\n\\n<!--\\ngoogle_ad_client')\n",
    "    for name in text[start:end].split('\\n'):\n",
    "        if name and name.strip() and all(c.isalpha() or c == ' ' for c in name):\n",
    "            bacteria_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteria_list_url_base_2 = 'https://www.ncbi.nlm.nih.gov/books/NBK'\n",
    "for i in range(818, 844):\n",
    "    url = bacteria_list_url_base_2 + str(i)\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    str_soup = str(soup)\n",
    "    for match in re.finditer('targettype=tax', str_soup):\n",
    "        start = match.start()\n",
    "        while str_soup[start] != '>':\n",
    "            start += 1\n",
    "        start += 1\n",
    "        end = start\n",
    "        while str_soup[end] != '<':\n",
    "            end += 1\n",
    "        if str_soup[start].isupper():\n",
    "            bacteria_names.append(str_soup[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteria_names = list(set(bacteria_names + bnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(bacteria_names, open(\"bacteria_list.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_splits(soup):\n",
    "    text = soup.text\n",
    "    while text != text.replace('\\n\\n', '\\n'):\n",
    "        text = text.replace('\\n\\n', '\\n')\n",
    "    return text.split('\\n')\n",
    "\n",
    "def get_sequence(soup_split_by_line):\n",
    "    return soup_split_by_line[soup_split_by_line.index('Sequence Length') - 1]\n",
    "\n",
    "def get_modifications(soup):\n",
    "    ptm_string = '\"comments-PTM\">PTM</h4></li><li><h5> '\n",
    "    text = str(soup)\n",
    "    if ptm_string not in text:\n",
    "        return []\n",
    "    rest = text[text.index(ptm_string) + len(ptm_string):]\n",
    "    ptm = rest[:rest.index('</')]\n",
    "    if 'erminal amidation' not in rest:\n",
    "        return [\"Unknown modification: \" + ptm]\n",
    "    return ['C-Terminal Amidation']\n",
    "\n",
    "def get_references(soup_split_by_line):\n",
    "    references = []\n",
    "    reference_splits = soup_split_by_line[soup_split_by_line.index('Literature Information'):]\n",
    "    for i, line in enumerate(reference_splits):\n",
    "        if line == 'Reference':\n",
    "            pubmed_id = reference_splits[i - 1]\n",
    "            reference = reference_splits[i + 3]\n",
    "            author = reference_splits[i + 1]\n",
    "            title = reference_splits[i + 5]\n",
    "            references.append(\n",
    "                \"PubMed ID: %s. Reference: %s. Author: %s. Title: %s\"\n",
    "                % (pubmed_id, reference, author, title)\n",
    "            )\n",
    "    return references\n",
    "\n",
    "def find_longest_bacteria_matches(bacteria_names, text_section):\n",
    "    # Find bacteria names that match, using only longest form\n",
    "    # This way we don't keep the one-word abbreviations when two names are present\n",
    "    bacteria_matches = [b for b in bacteria_names if ' ' + b in text_section.lower()]\n",
    "    longest_matches = []\n",
    "    for match in bacteria_matches:\n",
    "        is_sub = False\n",
    "        for other_match in bacteria_matches:\n",
    "            if match in other_match and len(other_match) > len(match):\n",
    "                is_sub = True\n",
    "        if not is_sub:\n",
    "            longest_matches.append(match)\n",
    "    return longest_matches\n",
    "\n",
    "def bacteria_text_to_bacteria_and_strain(bacteria_names, line):\n",
    "    all_matches = []\n",
    "    matches_from_full_names = find_longest_bacteria_matches(bacteria_names, line)\n",
    "\n",
    "    strain = re.search('ATCC\\s?[0-9]+', line)\n",
    "    if strain:\n",
    "        strain = strain.group(0)\n",
    "\n",
    "    for match in matches_from_full_names:\n",
    "        if len(match.split()) > 1:\n",
    "            try:\n",
    "                all_matches.append((\n",
    "                    match[0].upper() + '. ' + match.split()[1],\n",
    "                    strain.strip() if strain else ''\n",
    "                ))\n",
    "            except:\n",
    "                print(\"????\", match, line)\n",
    "        else:\n",
    "            splits = line.lower().split()\n",
    "            for i, split in enumerate(splits):\n",
    "                if split == match:\n",
    "                    try:\n",
    "                        all_matches.append((\n",
    "                            match[0].upper() + '. ' + splits[i + 1],\n",
    "                            strain.strip() if strain else ''\n",
    "                        ))\n",
    "                    except:\n",
    "                        print(\"?????????\", match, line)\n",
    "            \n",
    "    for regex_match in re.finditer(bacteria_regex, line):\n",
    "        bacterium = regex_match.groupdict()['bacterium']\n",
    "        strain = regex_match.groupdict()['strain']\n",
    "        all_matches.append((\n",
    "            bacterium,\n",
    "            strain.strip() if strain else ''\n",
    "        ))\n",
    "\n",
    "    return all_matches\n",
    "\n",
    "def extract_unit_and_value(line):\n",
    "    value = re.search('[0-9]+\\.?[0-9]*', line)\n",
    "    if not value:\n",
    "        return ('', '')\n",
    "    unit = line[value.end():].strip().replace(')', '')\n",
    "    return (unit, value.group(0))\n",
    "\n",
    "def get_mic_data(soup_split_by_line, _bacteria_names):\n",
    "    bacteria_names_lower = [b.lower() for b in bacteria_names]\n",
    "    all_bacteria = {}\n",
    "    mic_line_index = sorted(\n",
    "        [i for i in range(len(soup_split_by_line)) if soup_split_by_line[i] == 'Target Organism']\n",
    "    )[1] + 1\n",
    "    mic_line = soup_split_by_line[mic_line_index]\n",
    "    bacterium_or_unit_and_value_fields = re.split('(\\(MIC\\s?.*?\\))', mic_line)  # Alternating between unit/values and bacteria\n",
    "    for i, mic_split in enumerate(bacterium_or_unit_and_value_fields):\n",
    "        if re.search('(\\(MIC\\s?.*?\\))', mic_split):\n",
    "            bacteria_and_strains = bacteria_text_to_bacteria_and_strain(bacteria_names_lower, bacterium_or_unit_and_value_fields[i - 1])\n",
    "            unit, value = extract_unit_and_value(mic_split)\n",
    "            for (bacterium, strain) in bacteria_and_strains:\n",
    "                all_bacteria[(bacterium, strain)] = {\n",
    "                    'unit': unit,\n",
    "                    'value': value\n",
    "                }\n",
    "    return all_bacteria\n",
    "\n",
    "def get_hemolysis_data(soup):  # Return the sentence with the hemolytic data, leaving parsing for later\n",
    "    text = soup.text\n",
    "    if 'hemoly' in text:\n",
    "        for sentence in text.split('.'):\n",
    "            if 'hemoly' in sentence:\n",
    "                return sentence\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "amps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_soup(soup):\n",
    "    ssplits = get_line_splits(soup)\n",
    "    sequence = get_sequence(ssplits)\n",
    "    if not sequence:\n",
    "        return None, None\n",
    "    if 'Patent Information' in str(soup):\n",
    "        # print('Patent Information')\n",
    "        return None, None\n",
    "    modifications = get_modifications(soup)\n",
    "    references = get_references(ssplits)\n",
    "    mic_data = get_mic_data(ssplits, bacteria_names)\n",
    "    url_sources = [url]\n",
    "    hemolysis_data = get_hemolysis_data(soup)\n",
    "    return (\n",
    "        sequence, \n",
    "        {\n",
    "            'modifications': modifications,\n",
    "            'references': references,\n",
    "            'bacteria': mic_data,\n",
    "            'url_sources': url_sources,\n",
    "            'hemolysis': hemolysis_data,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3010\n",
      "3020\n",
      "3030\n",
      "3040\n",
      "3050\n",
      "3060\n",
      "3070\n",
      "3080\n",
      "3090\n",
      "3100\n",
      "3110\n",
      "3120\n",
      "3130\n",
      "3140\n",
      "3150\n",
      "3160\n",
      "3170\n",
      "3180\n",
      "3190\n",
      "3200\n",
      "3210\n",
      "3220\n",
      "3230\n",
      "3240\n",
      "3250\n",
      "3260\n",
      "3270\n",
      "3280\n",
      "3290\n",
      "3300\n",
      "3310\n",
      "3320\n",
      "3330\n",
      "3340\n",
      "3350\n",
      "3360\n",
      "3370\n",
      "3380\n",
      "3390\n",
      "3400\n",
      "3410\n",
      "3420\n",
      "3430\n",
      "3440\n",
      "3450\n",
      "3460\n",
      "3470\n",
      "3480\n",
      "3490\n",
      "3500\n",
      "3510\n",
      "3520\n",
      "3530\n",
      "3540\n",
      "3550\n",
      "3560\n",
      "3570\n",
      "3580\n",
      "3590\n",
      "3600\n",
      "3610\n",
      "3620\n",
      "3630\n",
      "3640\n",
      "3650\n",
      "3660\n",
      "3670\n",
      "3680\n",
      "3690\n",
      "3700\n",
      "3710\n",
      "3720\n",
      "3730\n",
      "3740\n",
      "3750\n",
      "3760\n",
      "3770\n",
      "3780\n",
      "3790\n",
      "3800\n",
      "3810\n",
      "3820\n",
      "3830\n",
      "3840\n",
      "3850\n",
      "3860\n",
      "3870\n",
      "3880\n",
      "3890\n",
      "3900\n",
      "3910\n",
      "3920\n",
      "3930\n",
      "3940\n",
      "3950\n",
      "3960\n",
      "3970\n",
      "3980\n",
      "3990\n",
      "4000\n",
      "4010\n",
      "4020\n",
      "4030\n",
      "4040\n",
      "4050\n",
      "4060\n",
      "4070\n",
      "4080\n",
      "4090\n",
      "4250\n",
      "4260\n",
      "4270\n",
      "4280\n",
      "4290\n",
      "4300\n",
      "4310\n",
      "4320\n",
      "4330\n",
      "4340\n",
      "4350\n",
      "4360\n",
      "4370\n",
      "4380\n",
      "4390\n",
      "4400\n",
      "4410\n",
      "4420\n",
      "4430\n",
      "4440\n",
      "4450\n",
      "4460\n",
      "4470\n",
      "4480\n",
      "4490\n",
      "4500\n",
      "4510\n",
      "4520\n",
      "4530\n",
      "4540\n",
      "4550\n",
      "4560\n",
      "4570\n",
      "4580\n",
      "4590\n",
      "4600\n",
      "4610\n",
      "4620\n",
      "4630\n",
      "4640\n",
      "4650\n",
      "4660\n",
      "4670\n",
      "4680\n",
      "4690\n",
      "4700\n",
      "4710\n",
      "4720\n",
      "4730\n",
      "4740\n",
      "4750\n",
      "4760\n",
      "4770\n",
      "4780\n",
      "4790\n",
      "4800\n",
      "4810\n",
      "4820\n",
      "4830\n",
      "4840\n",
      "4850\n",
      "4860\n",
      "4870\n",
      "4880\n",
      "4890\n",
      "4900\n",
      "4910\n",
      "4920\n",
      "4930\n",
      "4940\n",
      "4950\n",
      "4960\n",
      "4970\n",
      "4980\n",
      "4990\n",
      "5000\n",
      "5010\n",
      "5020\n",
      "5030\n",
      "5040\n",
      "5050\n",
      "5060\n",
      "5070\n",
      "5080\n",
      "5090\n",
      "5100\n",
      "5110\n",
      "5120\n",
      "5130\n",
      "5140\n",
      "5150\n",
      "5160\n",
      "5170\n",
      "5180\n",
      "5190\n",
      "5200\n",
      "5210\n",
      "5220\n",
      "5230\n",
      "5240\n",
      "5250\n",
      "5260\n",
      "5270\n",
      "5280\n",
      "5290\n",
      "5300\n",
      "5310\n",
      "5320\n",
      "5330\n",
      "5340\n",
      "5350\n",
      "5360\n",
      "5370\n",
      "5380\n",
      "5390\n",
      "5400\n",
      "5410\n",
      "5420\n",
      "5430\n",
      "5440\n",
      "5450\n",
      "5460\n",
      "5470\n",
      "5480\n",
      "5490\n",
      "5500\n",
      "5510\n",
      "5520\n",
      "5530\n",
      "5540\n",
      "5550\n",
      "5560\n",
      "5570\n",
      "5580\n",
      "5590\n",
      "5600\n",
      "5610\n",
      "5620\n",
      "5630\n"
     ]
    }
   ],
   "source": [
    "for i, drampid in enumerate(drampids[len(amps):]):\n",
    "    url = url_base + drampid\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    try:\n",
    "        ssplits = get_line_splits(soup)\n",
    "\n",
    "        sequence = get_sequence(ssplits)\n",
    "        if not sequence:\n",
    "            continue\n",
    "\n",
    "        sequence, results = parse_soup(soup)\n",
    "        if sequence:\n",
    "            amps[sequence] = results\n",
    "\n",
    "        if (i + len(amps)) % 10 == 0:\n",
    "            print(i + len(amps))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(DATABASE_NAME + \".data\", 'w') as f:\n",
    "    f.write(str(amps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
