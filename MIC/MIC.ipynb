{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9657ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始GRAMPA数据集预处理流程...\n",
      "正在加载数据...\n",
      "数据集形状: (44797, 6)\n",
      "列名: ['bacterium', 'sequence', 'unit', 'value', 'censor', 'database']\n",
      "缺失值统计:\n",
      "bacterium       84\n",
      "sequence         0\n",
      "unit             0\n",
      "value            0\n",
      "censor       43932\n",
      "database         0\n",
      "dtype: int64\n",
      "\n",
      "数据类型:\n",
      "bacterium     object\n",
      "sequence      object\n",
      "unit          object\n",
      "value        float64\n",
      "censor        object\n",
      "database      object\n",
      "dtype: object\n",
      "\n",
      "value字段统计:\n",
      "count    44797.000000\n",
      "mean         0.105690\n",
      "std          0.539219\n",
      "min         -3.558834\n",
      "25%         -0.151399\n",
      "50%          0.098738\n",
      "75%          0.267724\n",
      "max          3.657577\n",
      "Name: value, dtype: float64\n",
      "\n",
      "censor字段统计:\n",
      "censor\n",
      "NaN    43932\n",
      ">        865\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== 步骤1: 序列合法化 ===\n",
      "原始样本数: 44797\n",
      "空序列样本数: 0\n",
      "X占比>10%的样本数: 222\n",
      "长度<5的样本数: 680\n",
      "长度5-48的样本数: 42840\n",
      "长度>48的样本数: 1277\n",
      "主训练集样本数: 42626\n",
      "短肽样本数: 672\n",
      "长肽样本数: 1277\n",
      "排除样本数: 222\n",
      "\n",
      "=== 步骤2: 菌株归一化 ===\n",
      "唯一菌株数: 1025\n",
      "Top 10菌株:\n",
      "normalized_bacterium\n",
      "escherichia_coli              7795\n",
      "staphylococcus_aureus         7329\n",
      "pseudomonas_aeruginosa        4170\n",
      "candida_albicans              2664\n",
      "bacillus_subtilis             2179\n",
      "staphylococcus_epidermidis    1242\n",
      "salmonella_typhimurium         973\n",
      "klebsiella_pneumoniae          972\n",
      "micrococcus_luteus             965\n",
      "enterococcus_faecalis          942\n",
      "Name: count, dtype: int64\n",
      "低频菌株数（<10次）: 757\n",
      "最终菌株数: 269\n",
      "归为other的样本数: 2022\n",
      "\n",
      "=== 步骤3: 重复测定处理 ===\n",
      "唯一(sequence, bacterium)对数: 31545\n",
      "重复测定统计:\n",
      "1     24514\n",
      "2      4473\n",
      "3      1636\n",
      "4       685\n",
      "5       146\n",
      "6        20\n",
      "7        24\n",
      "8        11\n",
      "9        14\n",
      "10        9\n",
      "11        2\n",
      "12        3\n",
      "13        2\n",
      "14        1\n",
      "16        1\n",
      "17        1\n",
      "18        1\n",
      "23        1\n",
      "24        1\n",
      "Name: count, dtype: int64\n",
      "最大重复次数: 24\n",
      "聚合后样本数: 31545\n",
      "平均重复测定次数: 1.35\n",
      "删失样本数: 733\n",
      "完全删失样本数: 538\n",
      "\n",
      "=== 步骤4: 删失样本处理（改进版） ===\n",
      "包含删失信息的样本数: 733\n",
      "完全删失样本数: 538\n",
      "删失样本占比: 2.32%\n",
      "删失阈值分布:\n",
      "  - 最小删失阈值: 0.907\n",
      "  - 最大删失阈值: 2.907\n",
      "  - 平均删失阈值: 1.840\n",
      "警告: 193个样本的聚合值低于删失阈值（数据不一致）\n",
      "\n",
      "=== 步骤5: 异常值稳健化 ===\n",
      "原始value范围: [-3.222, 3.180]\n",
      "Winsorize范围 (1%-99%): [-1.101, 1.920]\n",
      "被调整的样本数: 低端270个, 高端313个\n",
      "\n",
      "=== 创建序列聚合数据集 ===\n",
      "序列聚合数据集样本数: 7055\n",
      "\n",
      "=== 步骤6: 数据划分（改进版GroupKFold） ===\n",
      "唯一序列数: 7055\n",
      "各菌株的序列数分布 (Top 10):\n",
      "escherichia_coli           1812\n",
      "bacillus_subtilis          1148\n",
      "candida_albicans            872\n",
      "bacillus_cereus             292\n",
      "acinetobacter_baumannii     290\n",
      "pseudomonas_aeruginosa      252\n",
      "staphylococcus_aureus       178\n",
      "enterococcus_faecalis       147\n",
      "e._amylovora                122\n",
      "bacillus_megaterium         118\n",
      "Name: count, dtype: int64\n",
      "高频菌株数 (>=50序列): 16\n",
      "中频菌株数 (10-49序列): 50\n",
      "低频菌株数 (<10序列): 82\n",
      "训练集序列数: 4966\n",
      "验证集序列数: 706\n",
      "测试集序列数: 1383\n",
      "各split统计:\n",
      "      sequence bacterium value_winsorized       \n",
      "         count   nunique             mean    std\n",
      "split                                           \n",
      "test      6054       257            0.116  0.479\n",
      "train    22427       269            0.095  0.476\n",
      "val       3064       248            0.101  0.486\n",
      "\n",
      "各split菌株分布平衡性检查:\n",
      "train集 Top 5菌株: {'escherichia_coli': 3561, 'staphylococcus_aureus': 3244, 'pseudomonas_aeruginosa': 2025, 'candida_albicans': 1287, 'bacillus_subtilis': 1045}\n",
      "val集 Top 5菌株: {'escherichia_coli': 508, 'staphylococcus_aureus': 477, 'pseudomonas_aeruginosa': 278, 'candida_albicans': 184, 'bacillus_subtilis': 140}\n",
      "test集 Top 5菌株: {'escherichia_coli': 1000, 'staphylococcus_aureus': 888, 'pseudomonas_aeruginosa': 529, 'candida_albicans': 355, 'bacillus_subtilis': 297}\n",
      "\n",
      "=== 保存数据集 ===\n",
      "完整聚合数据集已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/grampa_aggregated_full.csv\n",
      "序列聚合数据集已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/grampa_sequence_aggregated.csv\n",
      "train集（条件回归）已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/grampa_conditional_train.csv (22427 样本)\n",
      "train集（序列回归）已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/grampa_sequence_train.csv (4966 样本)\n",
      "val集（条件回归）已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/grampa_conditional_val.csv (3064 样本)\n",
      "val集（序列回归）已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/grampa_sequence_val.csv (706 样本)\n",
      "test集（条件回归）已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/grampa_conditional_test.csv (6054 样本)\n",
      "test集（序列回归）已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/grampa_sequence_test.csv (1383 样本)\n",
      "其他长度数据集已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/grampa_other_lengths.csv (1949 样本)\n",
      "处理报告已保存: /Users/ricardozhao/PycharmProjects/AMP/processed_data/processing_report.md\n",
      "\n",
      "数据预处理完成！\n",
      "所有文件已保存到: /Users/ricardozhao/PycharmProjects/AMP/processed_data\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GRAMPA数据集预处理脚本\n",
    "根据筛选器设计.md的要求进行数据清洗与标准化\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GRAMPAPreprocessor:\n",
    "    def __init__(self, input_file, output_dir='processed_data'):\n",
    "        self.input_file = input_file\n",
    "        self.output_dir = output_dir\n",
    "        self.df = None\n",
    "        self.processed_df = None\n",
    "        \n",
    "        # 创建输出目录\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # 标准氨基酸字母表\n",
    "        self.standard_aa = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "        \n",
    "        # 菌株名称标准化映射\n",
    "        self.bacteria_mapping = {\n",
    "            'escherichia coli': 'escherichia_coli',\n",
    "            'e. coli': 'escherichia_coli',\n",
    "            'e.coli': 'escherichia_coli',\n",
    "            'staphylococcus aureus': 'staphylococcus_aureus',\n",
    "            'staph. aureus': 'staphylococcus_aureus',\n",
    "            's. aureus': 'staphylococcus_aureus',\n",
    "            's.aureus': 'staphylococcus_aureus',\n",
    "            'pseudomonas aeruginosa': 'pseudomonas_aeruginosa',\n",
    "            'p. aeruginosa': 'pseudomonas_aeruginosa',\n",
    "            'p.aeruginosa': 'pseudomonas_aeruginosa',\n",
    "            'klebsiella pneumoniae': 'klebsiella_pneumoniae',\n",
    "            'k. pneumoniae': 'klebsiella_pneumoniae',\n",
    "            'k.pneumoniae': 'klebsiella_pneumoniae',\n",
    "            'candida albicans': 'candida_albicans',\n",
    "            'c. albicans': 'candida_albicans',\n",
    "            'c.albicans': 'candida_albicans',\n",
    "            'bacillus subtilis': 'bacillus_subtilis',\n",
    "            'b. subtilis': 'bacillus_subtilis',\n",
    "            'enterococcus faecalis': 'enterococcus_faecalis',\n",
    "            'e. faecalis': 'enterococcus_faecalis',\n",
    "            'streptococcus pyogenes': 'streptococcus_pyogenes',\n",
    "            's. pyogenes': 'streptococcus_pyogenes',\n",
    "        }\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"加载数据并进行基本分析\"\"\"\n",
    "        print(\"正在加载数据...\")\n",
    "        self.df = pd.read_csv(self.input_file)\n",
    "        \n",
    "        print(f\"数据集形状: {self.df.shape}\")\n",
    "        print(f\"列名: {self.df.columns.tolist()}\")\n",
    "        print(f\"缺失值统计:\")\n",
    "        print(self.df.isnull().sum())\n",
    "        print(f\"\\n数据类型:\")\n",
    "        print(self.df.dtypes)\n",
    "        print(f\"\\nvalue字段统计:\")\n",
    "        print(self.df['value'].describe())\n",
    "        print(f\"\\ncensor字段统计:\")\n",
    "        print(self.df['censor'].value_counts(dropna=False))\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def normalize_sequence(self, sequence):\n",
    "        \"\"\"序列合法化处理\"\"\"\n",
    "        if pd.isna(sequence):\n",
    "            return None, 1.0  # 返回序列和X占比\n",
    "            \n",
    "        # 转为大写\n",
    "        seq = str(sequence).upper().strip()\n",
    "        \n",
    "        # 移除非字母字符（空格、标点等）\n",
    "        seq = re.sub(r'[^A-Z]', '', seq)\n",
    "        \n",
    "        if len(seq) == 0:\n",
    "            return None, 1.0\n",
    "            \n",
    "        # 将非标准氨基酸映射为X\n",
    "        normalized_seq = ''\n",
    "        for aa in seq:\n",
    "            if aa in self.standard_aa:\n",
    "                normalized_seq += aa\n",
    "            else:\n",
    "                normalized_seq += 'X'\n",
    "        \n",
    "        # 计算X占比\n",
    "        x_ratio = normalized_seq.count('X') / len(normalized_seq) if len(normalized_seq) > 0 else 1.0\n",
    "        \n",
    "        return normalized_seq, x_ratio\n",
    "    \n",
    "    def normalize_bacterium(self, bacterium_name):\n",
    "        \"\"\"菌株名称标准化\"\"\"\n",
    "        if pd.isna(bacterium_name):\n",
    "            return 'unknown'\n",
    "            \n",
    "        name = str(bacterium_name).lower().strip()\n",
    "        \n",
    "        # 移除株系信息（括号内容、数字、特殊符号等）\n",
    "        name = re.sub(r'\\([^)]*\\)', '', name)  # 移除括号内容\n",
    "        name = re.sub(r'\\s+\\d+.*$', '', name)  # 移除数字及后续内容\n",
    "        name = re.sub(r'\\s+strain.*$', '', name, flags=re.IGNORECASE)  # 移除strain信息\n",
    "        name = re.sub(r'\\s+atcc.*$', '', name, flags=re.IGNORECASE)  # 移除ATCC信息\n",
    "        name = name.strip()\n",
    "        \n",
    "        # 标准化映射\n",
    "        if name in self.bacteria_mapping:\n",
    "            return self.bacteria_mapping[name]\n",
    "        \n",
    "        # 对于未映射的，保留属种名\n",
    "        parts = name.split()\n",
    "        if len(parts) >= 2:\n",
    "            genus_species = f\"{parts[0]}_{parts[1]}\"\n",
    "            return genus_species.replace(' ', '_').replace('-', '_')\n",
    "        \n",
    "        return name.replace(' ', '_').replace('-', '_')\n",
    "    \n",
    "    def sequence_cleaning(self):\n",
    "        \"\"\"步骤1: 序列合法化\"\"\"\n",
    "        print(\"\\n=== 步骤1: 序列合法化 ===\")\n",
    "        \n",
    "        # 处理序列\n",
    "        seq_info = self.df['sequence'].apply(self.normalize_sequence)\n",
    "        self.df['normalized_sequence'] = [x[0] for x in seq_info]\n",
    "        self.df['x_ratio'] = [x[1] for x in seq_info]\n",
    "        \n",
    "        # 统计\n",
    "        print(f\"原始样本数: {len(self.df)}\")\n",
    "        \n",
    "        # 移除空序列\n",
    "        valid_seq_mask = self.df['normalized_sequence'].notna()\n",
    "        print(f\"空序列样本数: {(~valid_seq_mask).sum()}\")\n",
    "        \n",
    "        # 移除X占比>10%的序列\n",
    "        x_ratio_mask = self.df['x_ratio'] <= 0.1\n",
    "        print(f\"X占比>10%的样本数: {(~x_ratio_mask).sum()}\")\n",
    "        \n",
    "        # 长度筛选\n",
    "        self.df['seq_length'] = self.df['normalized_sequence'].fillna('').str.len()\n",
    "        length_5_48_mask = (self.df['seq_length'] >= 5) & (self.df['seq_length'] <= 48)\n",
    "        length_lt5_mask = (self.df['seq_length'] > 0) & (self.df['seq_length'] < 5)\n",
    "        length_gt48_mask = self.df['seq_length'] > 48\n",
    "        \n",
    "        print(f\"长度<5的样本数: {length_lt5_mask.sum()}\")\n",
    "        print(f\"长度5-48的样本数: {length_5_48_mask.sum()}\")\n",
    "        print(f\"长度>48的样本数: {length_gt48_mask.sum()}\")\n",
    "        \n",
    "        # 主训练集：5-48 aa，X占比<=10%\n",
    "        main_mask = valid_seq_mask & x_ratio_mask & length_5_48_mask\n",
    "        self.df['dataset_split'] = 'exclude'\n",
    "        self.df.loc[main_mask, 'dataset_split'] = 'main'\n",
    "        self.df.loc[valid_seq_mask & x_ratio_mask & length_lt5_mask, 'dataset_split'] = 'short'\n",
    "        self.df.loc[valid_seq_mask & x_ratio_mask & length_gt48_mask, 'dataset_split'] = 'long'\n",
    "        \n",
    "        print(f\"主训练集样本数: {main_mask.sum()}\")\n",
    "        print(f\"短肽样本数: {(self.df['dataset_split'] == 'short').sum()}\")\n",
    "        print(f\"长肽样本数: {(self.df['dataset_split'] == 'long').sum()}\")\n",
    "        print(f\"排除样本数: {(self.df['dataset_split'] == 'exclude').sum()}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def bacteria_normalization(self):\n",
    "        \"\"\"步骤2: 菌株归一化\"\"\"\n",
    "        print(\"\\n=== 步骤2: 菌株归一化 ===\")\n",
    "        \n",
    "        # 标准化菌株名\n",
    "        self.df['normalized_bacterium'] = self.df['bacterium'].apply(self.normalize_bacterium)\n",
    "        \n",
    "        # 统计菌株频次\n",
    "        bacteria_counts = self.df['normalized_bacterium'].value_counts()\n",
    "        print(f\"唯一菌株数: {len(bacteria_counts)}\")\n",
    "        print(f\"Top 10菌株:\")\n",
    "        print(bacteria_counts.head(10))\n",
    "        \n",
    "        # 长尾菌株处理（频次<10的归为other）\n",
    "        low_freq_bacteria = bacteria_counts[bacteria_counts < 10].index\n",
    "        print(f\"低频菌株数（<10次）: {len(low_freq_bacteria)}\")\n",
    "        \n",
    "        self.df['final_bacterium'] = self.df['normalized_bacterium'].copy()\n",
    "        self.df.loc[self.df['normalized_bacterium'].isin(low_freq_bacteria), 'final_bacterium'] = 'other'\n",
    "        \n",
    "        final_bacteria_counts = self.df['final_bacterium'].value_counts()\n",
    "        print(f\"最终菌株数: {len(final_bacteria_counts)}\")\n",
    "        print(f\"归为other的样本数: {(self.df['final_bacterium'] == 'other').sum()}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def handle_duplicates(self):\n",
    "        \"\"\"步骤3: 重复测定处理\"\"\"\n",
    "        print(\"\\n=== 步骤3: 重复测定处理 ===\")\n",
    "        \n",
    "        # 只处理主训练集\n",
    "        main_df = self.df[self.df['dataset_split'] == 'main'].copy()\n",
    "        \n",
    "        # 统计重复情况\n",
    "        duplicate_groups = main_df.groupby(['normalized_sequence', 'final_bacterium'])\n",
    "        duplicate_stats = duplicate_groups.size()\n",
    "        \n",
    "        print(f\"唯一(sequence, bacterium)对数: {len(duplicate_stats)}\")\n",
    "        print(f\"重复测定统计:\")\n",
    "        print(duplicate_stats.value_counts().sort_index())\n",
    "        print(f\"最大重复次数: {duplicate_stats.max()}\")\n",
    "        \n",
    "        # 对每个(sequence, bacterium)组合计算几何均值\n",
    "        def geometric_mean_log(values):\n",
    "            \"\"\"对log值计算几何均值（先转回原值，算几何均值，再取log）\"\"\"\n",
    "            # value是log10(uM)，需要转回uM\n",
    "            original_values = 10 ** values\n",
    "            # 计算几何均值\n",
    "            geom_mean = np.exp(np.mean(np.log(original_values)))\n",
    "            # 转回log10\n",
    "            return np.log10(geom_mean)\n",
    "        \n",
    "        aggregated_data = []\n",
    "        for (seq, bact), group in duplicate_groups:\n",
    "            values = group['value'].values\n",
    "            n_measurements = len(values)\n",
    "            \n",
    "            # 删失信息处理 - 改进版\n",
    "            censor_info = group['censor'].fillna('').values\n",
    "            censored_mask = censor_info == '>'\n",
    "            has_censoring = censored_mask.any()\n",
    "            \n",
    "            # 计算聚合值\n",
    "            if has_censoring:\n",
    "                # 有删失的情况：分别处理删失和非删失值\n",
    "                censored_values = values[censored_mask]\n",
    "                uncensored_values = values[~censored_mask]\n",
    "                \n",
    "                # 删失阈值：删失样本中的最大值作为下界约束\n",
    "                censoring_threshold = censored_values.max() if len(censored_values) > 0 else None\n",
    "                \n",
    "                # 聚合值：只用非删失值计算几何均值\n",
    "                if len(uncensored_values) > 0:\n",
    "                    agg_value = geometric_mean_log(uncensored_values)\n",
    "                else:\n",
    "                    # 全是删失值的情况，用删失阈值作为下界估计\n",
    "                    agg_value = censoring_threshold\n",
    "                    \n",
    "                # 标准差：只用非删失值\n",
    "                value_std = np.std(uncensored_values) if len(uncensored_values) > 1 else 0.0\n",
    "                \n",
    "            else:\n",
    "                # 无删失的情况：正常处理\n",
    "                agg_value = geometric_mean_log(values)\n",
    "                value_std = np.std(values)\n",
    "                censoring_threshold = None\n",
    "            \n",
    "            # 其他信息\n",
    "            database = group['database'].iloc[0]\n",
    "            unit = group['unit'].iloc[0]\n",
    "            seq_length = group['seq_length'].iloc[0]\n",
    "            x_ratio = group['x_ratio'].iloc[0]\n",
    "            \n",
    "            aggregated_data.append({\n",
    "                'sequence': seq,\n",
    "                'bacterium': bact,\n",
    "                'value': agg_value,\n",
    "                'n_measurements': n_measurements,\n",
    "                'n_censored': censored_mask.sum(),\n",
    "                'n_uncensored': (~censored_mask).sum(),\n",
    "                'value_std': value_std,\n",
    "                'has_censoring': has_censoring,\n",
    "                'censoring_threshold': censoring_threshold,  # 删失下界约束\n",
    "                'unit': unit,\n",
    "                'database': database,\n",
    "                'seq_length': seq_length,\n",
    "                'x_ratio': x_ratio\n",
    "            })\n",
    "        \n",
    "        self.aggregated_df = pd.DataFrame(aggregated_data)\n",
    "        print(f\"聚合后样本数: {len(self.aggregated_df)}\")\n",
    "        print(f\"平均重复测定次数: {self.aggregated_df['n_measurements'].mean():.2f}\")\n",
    "        print(f\"删失样本数: {self.aggregated_df['has_censoring'].sum()}\")\n",
    "        print(f\"完全删失样本数: {(self.aggregated_df['n_uncensored'] == 0).sum()}\")\n",
    "        \n",
    "        return self.aggregated_df\n",
    "    \n",
    "    def handle_censoring(self):\n",
    "        \"\"\"步骤4: 删失样本处理 - 改进版\"\"\"\n",
    "        print(\"\\n=== 步骤4: 删失样本处理（改进版） ===\")\n",
    "        \n",
    "        censored_count = self.aggregated_df['has_censoring'].sum()\n",
    "        fully_censored_count = (self.aggregated_df['n_uncensored'] == 0).sum()\n",
    "        \n",
    "        print(f\"包含删失信息的样本数: {censored_count}\")\n",
    "        print(f\"完全删失样本数: {fully_censored_count}\")\n",
    "        print(f\"删失样本占比: {censored_count / len(self.aggregated_df) * 100:.2f}%\")\n",
    "        \n",
    "        # 删失信息统计\n",
    "        if censored_count > 0:\n",
    "            censored_df = self.aggregated_df[self.aggregated_df['has_censoring']]\n",
    "            print(f\"删失阈值分布:\")\n",
    "            print(f\"  - 最小删失阈值: {censored_df['censoring_threshold'].min():.3f}\")\n",
    "            print(f\"  - 最大删失阈值: {censored_df['censoring_threshold'].max():.3f}\")\n",
    "            print(f\"  - 平均删失阈值: {censored_df['censoring_threshold'].mean():.3f}\")\n",
    "            \n",
    "            # 检查删失一致性：聚合值不应低于删失阈值\n",
    "            inconsistent_mask = censored_df['value'] < censored_df['censoring_threshold']\n",
    "            inconsistent_count = inconsistent_mask.sum()\n",
    "            if inconsistent_count > 0:\n",
    "                print(f\"警告: {inconsistent_count}个样本的聚合值低于删失阈值（数据不一致）\")\n",
    "        \n",
    "        # 为损失函数准备删失标记\n",
    "        self.aggregated_df['is_censored'] = self.aggregated_df['has_censoring']\n",
    "        \n",
    "        return self.aggregated_df\n",
    "    \n",
    "    def winsorize_values(self, percentile_range=(1, 99)):\n",
    "        \"\"\"步骤5: 异常值稳健化\"\"\"\n",
    "        print(\"\\n=== 步骤5: 异常值稳健化 ===\")\n",
    "        \n",
    "        values = self.aggregated_df['value']\n",
    "        \n",
    "        # 计算分位数\n",
    "        p_low = np.percentile(values, percentile_range[0])\n",
    "        p_high = np.percentile(values, percentile_range[1])\n",
    "        \n",
    "        print(f\"原始value范围: [{values.min():.3f}, {values.max():.3f}]\")\n",
    "        print(f\"Winsorize范围 ({percentile_range[0]}%-{percentile_range[1]}%): [{p_low:.3f}, {p_high:.3f}]\")\n",
    "        \n",
    "        # Winsorize处理\n",
    "        winsorized_values = np.clip(values, p_low, p_high)\n",
    "        \n",
    "        # 统计影响的样本数\n",
    "        affected_low = (values < p_low).sum()\n",
    "        affected_high = (values > p_high).sum()\n",
    "        print(f\"被调整的样本数: 低端{affected_low}个, 高端{affected_high}个\")\n",
    "        \n",
    "        self.aggregated_df['value_winsorized'] = winsorized_values\n",
    "        self.aggregated_df['value_original'] = values\n",
    "        \n",
    "        return self.aggregated_df\n",
    "    \n",
    "    def create_sequence_aggregated_dataset(self):\n",
    "        \"\"\"创建序列聚合数据集（用于模型A）\"\"\"\n",
    "        print(\"\\n=== 创建序列聚合数据集 ===\")\n",
    "        \n",
    "        # 按序列聚合，计算所有菌株的平均活性\n",
    "        seq_groups = self.aggregated_df.groupby('sequence')\n",
    "        \n",
    "        seq_aggregated_data = []\n",
    "        for seq, group in seq_groups:\n",
    "            # 使用winsorized值计算均值\n",
    "            mean_value = group['value_winsorized'].mean()\n",
    "            std_value = group['value_winsorized'].std()\n",
    "            n_bacteria = len(group)\n",
    "            total_measurements = group['n_measurements'].sum()\n",
    "            \n",
    "            # 序列特征\n",
    "            seq_length = group['seq_length'].iloc[0]\n",
    "            x_ratio = group['x_ratio'].iloc[0]\n",
    "            \n",
    "            # 删失信息\n",
    "            has_any_censoring = group['has_censoring'].any()\n",
    "            \n",
    "            seq_aggregated_data.append({\n",
    "                'sequence': seq,\n",
    "                'mean_log_mic': mean_value,\n",
    "                'std_log_mic': std_value,\n",
    "                'n_bacteria_tested': n_bacteria,\n",
    "                'total_measurements': total_measurements,\n",
    "                'seq_length': seq_length,\n",
    "                'x_ratio': x_ratio,\n",
    "                'has_censoring': has_any_censoring\n",
    "            })\n",
    "        \n",
    "        self.seq_aggregated_df = pd.DataFrame(seq_aggregated_data)\n",
    "        print(f\"序列聚合数据集样本数: {len(self.seq_aggregated_df)}\")\n",
    "        \n",
    "        return self.seq_aggregated_df\n",
    "    \n",
    "    def create_stratified_bacteria_splits(self):\n",
    "        \"\"\"创建按菌株分层的序列分组\"\"\"\n",
    "        # 为每个序列计算主要菌株（出现最多的菌株）\n",
    "        seq_bacteria_mapping = {}\n",
    "        for seq in self.aggregated_df['sequence'].unique():\n",
    "            seq_data = self.aggregated_df[self.aggregated_df['sequence'] == seq]\n",
    "            main_bacterium = seq_data['bacterium'].value_counts().index[0]\n",
    "            seq_bacteria_mapping[seq] = main_bacterium\n",
    "        \n",
    "        return seq_bacteria_mapping\n",
    "    \n",
    "    def create_train_val_test_splits(self, n_splits=5, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"步骤6: 创建数据划分 - 改进版使用真正的GroupKFold\"\"\"\n",
    "        print(\"\\n=== 步骤6: 数据划分（改进版GroupKFold） ===\")\n",
    "        \n",
    "        # 准备序列级数据用于划分\n",
    "        sequences = self.aggregated_df['sequence'].unique()\n",
    "        print(f\"唯一序列数: {len(sequences)}\")\n",
    "        \n",
    "        # 创建序列-主要菌株映射，用于分层\n",
    "        seq_bacteria_mapping = self.create_stratified_bacteria_splits()\n",
    "        \n",
    "        # 统计每个菌株的序列数\n",
    "        bacteria_seq_counts = pd.Series(seq_bacteria_mapping.values()).value_counts()\n",
    "        print(f\"各菌株的序列数分布 (Top 10):\")\n",
    "        print(bacteria_seq_counts.head(10))\n",
    "        \n",
    "        # 为了保证菌株分布平衡，我们使用分层策略\n",
    "        # 首先按菌株频次分组\n",
    "        high_freq_bacteria = bacteria_seq_counts[bacteria_seq_counts >= 50].index  # 高频菌株\n",
    "        medium_freq_bacteria = bacteria_seq_counts[(bacteria_seq_counts >= 10) & (bacteria_seq_counts < 50)].index\n",
    "        low_freq_bacteria = bacteria_seq_counts[bacteria_seq_counts < 10].index\n",
    "        \n",
    "        print(f\"高频菌株数 (>=50序列): {len(high_freq_bacteria)}\")\n",
    "        print(f\"中频菌株数 (10-49序列): {len(medium_freq_bacteria)}\")  \n",
    "        print(f\"低频菌株数 (<10序列): {len(low_freq_bacteria)}\")\n",
    "        \n",
    "        # 分别对每个频次组使用GroupKFold\n",
    "        def stratified_group_split(sequences, bacteria_mapping, test_size, val_size, random_state=42):\n",
    "            \"\"\"按菌株分层的GroupKFold划分\"\"\"\n",
    "            np.random.seed(random_state)\n",
    "            \n",
    "            # 按菌株分组序列\n",
    "            bacteria_sequences = {}\n",
    "            for seq, bacterium in bacteria_mapping.items():\n",
    "                if bacterium not in bacteria_sequences:\n",
    "                    bacteria_sequences[bacterium] = []\n",
    "                bacteria_sequences[bacterium].append(seq)\n",
    "            \n",
    "            train_seqs, val_seqs, test_seqs = [], [], []\n",
    "            \n",
    "            # 对每个菌株的序列进行划分\n",
    "            for bacterium, seqs in bacteria_sequences.items():\n",
    "                seqs = np.array(seqs)\n",
    "                n_seqs = len(seqs)\n",
    "                \n",
    "                if n_seqs == 1:\n",
    "                    # 只有1个序列，随机分配\n",
    "                    split_choice = np.random.choice(['train', 'val', 'test'], p=[1-test_size-val_size, val_size, test_size])\n",
    "                    if split_choice == 'train':\n",
    "                        train_seqs.extend(seqs)\n",
    "                    elif split_choice == 'val':\n",
    "                        val_seqs.extend(seqs)\n",
    "                    else:\n",
    "                        test_seqs.extend(seqs)\n",
    "                elif n_seqs == 2:\n",
    "                    # 2个序列，一个给train，一个随机分配给val或test\n",
    "                    train_seqs.append(seqs[0])\n",
    "                    split_choice = np.random.choice(['val', 'test'])\n",
    "                    if split_choice == 'val':\n",
    "                        val_seqs.append(seqs[1])\n",
    "                    else:\n",
    "                        test_seqs.append(seqs[1])\n",
    "                else:\n",
    "                    # 多个序列，按比例划分\n",
    "                    shuffled = np.random.permutation(seqs)\n",
    "                    n_test = max(1, int(n_seqs * test_size))\n",
    "                    n_val = max(1, int(n_seqs * val_size))\n",
    "                    n_train = n_seqs - n_test - n_val\n",
    "                    \n",
    "                    if n_train < 1:  # 确保至少有1个训练样本\n",
    "                        n_train = 1\n",
    "                        n_test = max(1, n_seqs - n_train - n_val)\n",
    "                        n_val = n_seqs - n_train - n_test\n",
    "                    \n",
    "                    train_seqs.extend(shuffled[:n_train])\n",
    "                    val_seqs.extend(shuffled[n_train:n_train+n_val])\n",
    "                    test_seqs.extend(shuffled[n_train+n_val:])\n",
    "            \n",
    "            return set(train_seqs), set(val_seqs), set(test_seqs)\n",
    "        \n",
    "        # 执行分层划分\n",
    "        train_sequences, val_sequences, test_sequences = stratified_group_split(\n",
    "            sequences, seq_bacteria_mapping, test_size, val_size\n",
    "        )\n",
    "        \n",
    "        print(f\"训练集序列数: {len(train_sequences)}\")\n",
    "        print(f\"验证集序列数: {len(val_sequences)}\")\n",
    "        print(f\"测试集序列数: {len(test_sequences)}\")\n",
    "        \n",
    "        # 验证没有重叠\n",
    "        assert len(train_sequences & val_sequences) == 0, \"训练集和验证集有重叠序列\"\n",
    "        assert len(train_sequences & test_sequences) == 0, \"训练集和测试集有重叠序列\"\n",
    "        assert len(val_sequences & test_sequences) == 0, \"验证集和测试集有重叠序列\"\n",
    "        \n",
    "        # 为聚合数据集添加split标记\n",
    "        def assign_split(seq):\n",
    "            if seq in train_sequences:\n",
    "                return 'train'\n",
    "            elif seq in val_sequences:\n",
    "                return 'val'\n",
    "            else:\n",
    "                return 'test'\n",
    "        \n",
    "        self.aggregated_df['split'] = self.aggregated_df['sequence'].apply(assign_split)\n",
    "        self.seq_aggregated_df['split'] = self.seq_aggregated_df['sequence'].apply(assign_split)\n",
    "        \n",
    "        # 统计每个split的样本数和菌株分布\n",
    "        split_stats = self.aggregated_df.groupby('split').agg({\n",
    "            'sequence': 'count',\n",
    "            'bacterium': 'nunique',\n",
    "            'value_winsorized': ['mean', 'std']\n",
    "        }).round(3)\n",
    "        \n",
    "        print(f\"各split统计:\")\n",
    "        print(split_stats)\n",
    "        \n",
    "        # 检查菌株分布平衡性\n",
    "        print(f\"\\n各split菌株分布平衡性检查:\")\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            split_bacteria = self.aggregated_df[self.aggregated_df['split'] == split]['bacterium'].value_counts()\n",
    "            print(f\"{split}集 Top 5菌株: {dict(split_bacteria.head(5))}\")\n",
    "        \n",
    "        return train_sequences, val_sequences, test_sequences\n",
    "    \n",
    "    def save_processed_datasets(self):\n",
    "        \"\"\"保存处理后的数据集\"\"\"\n",
    "        print(\"\\n=== 保存数据集 ===\")\n",
    "        \n",
    "        # 保存完整的聚合数据集\n",
    "        output_file = os.path.join(self.output_dir, 'grampa_aggregated_full.csv')\n",
    "        self.aggregated_df.to_csv(output_file, index=False)\n",
    "        print(f\"完整聚合数据集已保存: {output_file}\")\n",
    "        \n",
    "        # 保存序列聚合数据集\n",
    "        seq_output_file = os.path.join(self.output_dir, 'grampa_sequence_aggregated.csv')\n",
    "        self.seq_aggregated_df.to_csv(seq_output_file, index=False)\n",
    "        print(f\"序列聚合数据集已保存: {seq_output_file}\")\n",
    "        \n",
    "        # 按split保存\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            # 条件回归数据集（包含菌株信息）\n",
    "            split_df = self.aggregated_df[self.aggregated_df['split'] == split].copy()\n",
    "            split_file = os.path.join(self.output_dir, f'grampa_conditional_{split}.csv')\n",
    "            split_df.to_csv(split_file, index=False)\n",
    "            print(f\"{split}集（条件回归）已保存: {split_file} ({len(split_df)} 样本)\")\n",
    "            \n",
    "            # 序列回归数据集\n",
    "            seq_split_df = self.seq_aggregated_df[self.seq_aggregated_df['split'] == split].copy()\n",
    "            seq_split_file = os.path.join(self.output_dir, f'grampa_sequence_{split}.csv')\n",
    "            seq_split_df.to_csv(seq_split_file, index=False)\n",
    "            print(f\"{split}集（序列回归）已保存: {seq_split_file} ({len(seq_split_df)} 样本)\")\n",
    "        \n",
    "        # 保存其他长度的数据集\n",
    "        other_splits = self.df[self.df['dataset_split'].isin(['short', 'long'])].copy()\n",
    "        if len(other_splits) > 0:\n",
    "            other_file = os.path.join(self.output_dir, 'grampa_other_lengths.csv')\n",
    "            other_splits.to_csv(other_file, index=False)\n",
    "            print(f\"其他长度数据集已保存: {other_file} ({len(other_splits)} 样本)\")\n",
    "        \n",
    "        # 保存处理报告\n",
    "        self.save_processing_report()\n",
    "    \n",
    "    def save_processing_report(self):\n",
    "        \"\"\"保存处理报告\"\"\"\n",
    "        report_file = os.path.join(self.output_dir, 'processing_report.md')\n",
    "        \n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# GRAMPA数据集处理报告\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 原始数据统计\\n\")\n",
    "            f.write(f\"- 原始样本数: {len(self.df)}\\n\")\n",
    "            f.write(f\"- 唯一序列数: {self.df['normalized_sequence'].nunique()}\\n\")\n",
    "            f.write(f\"- 唯一菌株数: {self.df['normalized_bacterium'].nunique()}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 序列长度分布\\n\")\n",
    "            length_dist = self.df['dataset_split'].value_counts()\n",
    "            for category, count in length_dist.items():\n",
    "                f.write(f\"- {category}: {count}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## 聚合后数据统计\\n\")\n",
    "            f.write(f\"- 聚合样本数: {len(self.aggregated_df)}\\n\")\n",
    "            f.write(f\"- 序列聚合样本数: {len(self.seq_aggregated_df)}\\n\")\n",
    "            f.write(f\"- 平均重复测定次数: {self.aggregated_df['n_measurements'].mean():.2f}\\n\")\n",
    "            f.write(f\"- 删失样本数: {self.aggregated_df['has_censoring'].sum()}\\n\")\n",
    "            f.write(f\"- 完全删失样本数: {(self.aggregated_df['n_uncensored'] == 0).sum()}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 数据划分统计\\n\")\n",
    "            split_stats = self.aggregated_df['split'].value_counts()\n",
    "            for split, count in split_stats.items():\n",
    "                f.write(f\"- {split}: {count}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Value分布统计\\n\")\n",
    "            f.write(f\"- 原始范围: [{self.aggregated_df['value_original'].min():.3f}, {self.aggregated_df['value_original'].max():.3f}]\\n\")\n",
    "            f.write(f\"- Winsorized范围: [{self.aggregated_df['value_winsorized'].min():.3f}, {self.aggregated_df['value_winsorized'].max():.3f}]\\n\")\n",
    "            f.write(f\"- 均值: {self.aggregated_df['value_winsorized'].mean():.3f}\\n\")\n",
    "            f.write(f\"- 标准差: {self.aggregated_df['value_winsorized'].std():.3f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 菌株分布（Top 10）\\n\")\n",
    "            bacteria_counts = self.aggregated_df['bacterium'].value_counts().head(10)\n",
    "            for bacterium, count in bacteria_counts.items():\n",
    "                f.write(f\"- {bacterium}: {count}\\n\")\n",
    "        \n",
    "        print(f\"处理报告已保存: {report_file}\")\n",
    "    \n",
    "    def run_full_pipeline(self):\n",
    "        \"\"\"运行完整的预处理流程\"\"\"\n",
    "        print(\"开始GRAMPA数据集预处理流程...\")\n",
    "        \n",
    "        # 加载数据\n",
    "        self.load_data()\n",
    "        \n",
    "        # 步骤1: 序列合法化\n",
    "        self.sequence_cleaning()\n",
    "        \n",
    "        # 步骤2: 菌株归一化\n",
    "        self.bacteria_normalization()\n",
    "        \n",
    "        # 步骤3: 重复测定处理\n",
    "        self.handle_duplicates()\n",
    "        \n",
    "        # 步骤4: 删失样本处理\n",
    "        self.handle_censoring()\n",
    "        \n",
    "        # 步骤5: 异常值稳健化\n",
    "        self.winsorize_values()\n",
    "        \n",
    "        # 创建序列聚合数据集\n",
    "        self.create_sequence_aggregated_dataset()\n",
    "        \n",
    "        # 步骤6: 数据划分\n",
    "        self.create_train_val_test_splits()\n",
    "        \n",
    "        # 保存所有数据集\n",
    "        self.save_processed_datasets()\n",
    "        \n",
    "        print(\"\\n数据预处理完成！\")\n",
    "        print(f\"所有文件已保存到: {self.output_dir}\")\n",
    "\n",
    "def main():\n",
    "    # 配置参数\n",
    "    input_file = \"/Users/ricardozhao/PycharmProjects/AMP/data/AMP/grampa_merged_dataset.csv\"\n",
    "    output_dir = \"/Users/ricardozhao/PycharmProjects/AMP/processed_data\"\n",
    "    \n",
    "    # 创建预处理器\n",
    "    preprocessor = GRAMPAPreprocessor(input_file, output_dir)\n",
    "    \n",
    "    # 运行完整流程\n",
    "    preprocessor.run_full_pipeline()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "特征构建脚本\n",
    "实现PLM表征、理化特征和菌株表示的生成\n",
    "基于筛选器设计.md的第三部分要求\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import EsmTokenizer, EsmModel\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 理化特征计算\n",
    "from Bio.SeqUtils import molecular_weight\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import math\n",
    "\n",
    "class AMP_FeatureExtractor:\n",
    "    def __init__(self, \n",
    "                 processed_data_dir='processed_data',\n",
    "                 features_output_dir='features',\n",
    "                 esm_model_name='facebook/esm2_t33_650M_UR50D',\n",
    "                 device='auto'):\n",
    "        \"\"\"\n",
    "        初始化特征提取器\n",
    "        \n",
    "        Args:\n",
    "            processed_data_dir: 处理后数据目录\n",
    "            features_output_dir: 特征输出目录\n",
    "            esm_model_name: ESM模型名称\n",
    "            device: 计算设备\n",
    "        \"\"\"\n",
    "        self.processed_data_dir = processed_data_dir\n",
    "        self.features_output_dir = features_output_dir\n",
    "        self.esm_model_name = esm_model_name\n",
    "        \n",
    "        # 设备配置\n",
    "        if device == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        print(f\"使用设备: {self.device}\")\n",
    "        \n",
    "        # 创建输出目录\n",
    "        os.makedirs(features_output_dir, exist_ok=True)\n",
    "        \n",
    "        # 初始化ESM模型和tokenizer\n",
    "        print(\"正在加载ESM-2模型...\")\n",
    "        self.tokenizer = EsmTokenizer.from_pretrained(esm_model_name)\n",
    "        self.esm_model = EsmModel.from_pretrained(esm_model_name).to(self.device)\n",
    "        self.esm_model.eval()\n",
    "        print(\"ESM-2模型加载完成\")\n",
    "        \n",
    "        # 氨基酸属性字典\n",
    "        self.aa_properties = {\n",
    "            # 疏水性 (Kyte-Doolittle scale)\n",
    "            'hydrophobicity': {\n",
    "                'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
    "                'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
    "                'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
    "                'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
    "            },\n",
    "            # 电荷 (pH 7.4)\n",
    "            'charge': {\n",
    "                'A': 0, 'R': 1, 'N': 0, 'D': -1, 'C': 0,\n",
    "                'Q': 0, 'E': -1, 'G': 0, 'H': 0, 'I': 0,\n",
    "                'L': 0, 'K': 1, 'M': 0, 'F': 0, 'P': 0,\n",
    "                'S': 0, 'T': 0, 'W': 0, 'Y': 0, 'V': 0\n",
    "            },\n",
    "            # 极性\n",
    "            'polarity': {\n",
    "                'A': 0, 'R': 1, 'N': 1, 'D': 1, 'C': 0,\n",
    "                'Q': 1, 'E': 1, 'G': 0, 'H': 1, 'I': 0,\n",
    "                'L': 0, 'K': 1, 'M': 0, 'F': 0, 'P': 0,\n",
    "                'S': 1, 'T': 1, 'W': 0, 'Y': 1, 'V': 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def extract_plm_embeddings(self, sequences, batch_size=32, max_length=512):\n",
    "        \"\"\"\n",
    "        提取PLM (ESM-2) 表征\n",
    "        \n",
    "        Args:\n",
    "            sequences: 序列列表\n",
    "            batch_size: 批次大小\n",
    "            max_length: 最大序列长度\n",
    "            \n",
    "        Returns:\n",
    "            embeddings: shape (n_sequences, 2*hidden_dim) 的embedding矩阵\n",
    "        \"\"\"\n",
    "        print(f\"正在提取 {len(sequences)} 个序列的PLM embeddings...\")\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        # 批次处理\n",
    "        for i in tqdm(range(0, len(sequences), batch_size), desc=\"提取PLM embeddings\"):\n",
    "            batch_sequences = sequences[i:i+batch_size]\n",
    "            \n",
    "            # 过滤掉过长的序列\n",
    "            valid_sequences = []\n",
    "            valid_indices = []\n",
    "            for j, seq in enumerate(batch_sequences):\n",
    "                if len(seq) <= max_length:\n",
    "                    valid_sequences.append(seq)\n",
    "                    valid_indices.append(j)\n",
    "            \n",
    "            if not valid_sequences:\n",
    "                # 如果批次中没有有效序列，添加零向量\n",
    "                batch_embeddings = torch.zeros(len(batch_sequences), 2 * self.esm_model.config.hidden_size)\n",
    "                all_embeddings.append(batch_embeddings)\n",
    "                continue\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                valid_sequences, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.esm_model(**inputs)\n",
    "                # 获取序列表征 (去除CLS和SEP tokens)\n",
    "                sequence_embeddings = outputs.last_hidden_state[:, 1:-1, :]  # (batch, seq_len, hidden_dim)\n",
    "                \n",
    "                # 池化操作\n",
    "                mean_pooled = torch.mean(sequence_embeddings, dim=1)  # (batch, hidden_dim)\n",
    "                max_pooled = torch.max(sequence_embeddings, dim=1)[0]  # (batch, hidden_dim)\n",
    "                \n",
    "                # 拼接均值和最大池化\n",
    "                combined_embeddings = torch.cat([mean_pooled, max_pooled], dim=1)  # (batch, 2*hidden_dim)\n",
    "                \n",
    "                # 创建完整批次的embedding矩阵\n",
    "                batch_embeddings = torch.zeros(len(batch_sequences), 2 * self.esm_model.config.hidden_size)\n",
    "                for j, valid_idx in enumerate(valid_indices):\n",
    "                    batch_embeddings[valid_idx] = combined_embeddings[j].cpu()\n",
    "                \n",
    "                all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        # 合并所有批次\n",
    "        final_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        print(f\"PLM embeddings shape: {final_embeddings.shape}\")\n",
    "        \n",
    "        return final_embeddings.numpy()\n",
    "    \n",
    "    def calculate_physicochemical_features(self, sequences):\n",
    "        \"\"\"\n",
    "        计算理化特征\n",
    "        \n",
    "        Args:\n",
    "            sequences: 序列列表\n",
    "            \n",
    "        Returns:\n",
    "            features: 理化特征矩阵\n",
    "        \"\"\"\n",
    "        print(f\"正在计算 {len(sequences)} 个序列的理化特征...\")\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for seq in tqdm(sequences, desc=\"计算理化特征\"):\n",
    "            seq_features = {}\n",
    "            \n",
    "            # 处理无效序列\n",
    "            if not seq or pd.isna(seq) or len(seq) == 0:\n",
    "                # 返回零特征\n",
    "                features.append([0] * 30)  # 预计30个特征\n",
    "                continue\n",
    "            \n",
    "            # 清理序列（移除非标准氨基酸）\n",
    "            clean_seq = ''.join([aa for aa in seq.upper() if aa in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "            if len(clean_seq) == 0:\n",
    "                features.append([0] * 30)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # 使用BioPython计算基本特征\n",
    "                analysis = ProteinAnalysis(clean_seq)\n",
    "                \n",
    "                # 1. 长度\n",
    "                length = len(clean_seq)\n",
    "                \n",
    "                # 2. 分子量\n",
    "                mw = analysis.molecular_weight()\n",
    "                \n",
    "                # 3. 等电点\n",
    "                try:\n",
    "                    isoelectric_point = analysis.isoelectric_point()\n",
    "                except:\n",
    "                    isoelectric_point = 7.0\n",
    "                \n",
    "                # 4. GRAVY (疏水性)\n",
    "                try:\n",
    "                    gravy = analysis.gravy()\n",
    "                except:\n",
    "                    gravy = 0.0\n",
    "                \n",
    "                # 5. 净电荷 (pH 7.4)\n",
    "                net_charge = sum([self.aa_properties['charge'].get(aa, 0) for aa in clean_seq])\n",
    "                \n",
    "                # 6. RK含量 (碱性残基比例)\n",
    "                rk_count = clean_seq.count('R') + clean_seq.count('K')\n",
    "                rk_ratio = rk_count / length if length > 0 else 0\n",
    "                \n",
    "                # 7. 氨基酸组成 (20维)\n",
    "                aa_composition = []\n",
    "                for aa in 'ACDEFGHIKLMNPQRSTVWY':\n",
    "                    aa_composition.append(clean_seq.count(aa) / length if length > 0 else 0)\n",
    "                \n",
    "                # 8. 疏水矩 (假设α螺旋)\n",
    "                hydrophobic_moment = self.calculate_hydrophobic_moment(clean_seq)\n",
    "                \n",
    "                # 9. 其他特征\n",
    "                positive_charge = sum([1 for aa in clean_seq if self.aa_properties['charge'].get(aa, 0) > 0])\n",
    "                negative_charge = sum([1 for aa in clean_seq if self.aa_properties['charge'].get(aa, 0) < 0])\n",
    "                polar_residues = sum([1 for aa in clean_seq if self.aa_properties['polarity'].get(aa, 0) == 1])\n",
    "                \n",
    "                # 组装特征向量\n",
    "                feature_vector = [\n",
    "                    length,\n",
    "                    mw,\n",
    "                    isoelectric_point,\n",
    "                    gravy,\n",
    "                    net_charge,\n",
    "                    rk_ratio,\n",
    "                    hydrophobic_moment,\n",
    "                    positive_charge / length if length > 0 else 0,\n",
    "                    negative_charge / length if length > 0 else 0,\n",
    "                    polar_residues / length if length > 0 else 0\n",
    "                ] + aa_composition\n",
    "                \n",
    "                features.append(feature_vector)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"计算序列 {seq[:20]}... 的理化特征时出错: {e}\")\n",
    "                features.append([0] * 30)\n",
    "        \n",
    "        features_array = np.array(features, dtype=np.float32)\n",
    "        print(f\"理化特征 shape: {features_array.shape}\")\n",
    "        \n",
    "        return features_array\n",
    "    \n",
    "    def calculate_hydrophobic_moment(self, sequence, window=100):\n",
    "        \"\"\"\n",
    "        计算疏水矩 (假设α螺旋结构)\n",
    "        \n",
    "        Args:\n",
    "            sequence: 氨基酸序列\n",
    "            window: 螺旋角度窗口 (度)\n",
    "            \n",
    "        Returns:\n",
    "            hydrophobic_moment: 疏水矩值\n",
    "        \"\"\"\n",
    "        if len(sequence) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # α螺旋每个残基旋转100度\n",
    "        angle_per_residue = math.radians(window)\n",
    "        \n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        \n",
    "        for i, aa in enumerate(sequence):\n",
    "            hydrophobicity = self.aa_properties['hydrophobicity'].get(aa, 0)\n",
    "            angle = i * angle_per_residue\n",
    "            \n",
    "            sum_x += hydrophobicity * math.cos(angle)\n",
    "            sum_y += hydrophobicity * math.sin(angle)\n",
    "        \n",
    "        hydrophobic_moment = math.sqrt(sum_x**2 + sum_y**2) / len(sequence)\n",
    "        return hydrophobic_moment\n",
    "    \n",
    "    def create_bacteria_embeddings(self, bacteria_names, embedding_dim=32):\n",
    "        \"\"\"\n",
    "        创建菌株embedding映射\n",
    "        \n",
    "        Args:\n",
    "            bacteria_names: 菌株名称列表\n",
    "            embedding_dim: embedding维度\n",
    "            \n",
    "        Returns:\n",
    "            bacteria_to_id: 菌株名到ID的映射\n",
    "            embedding_matrix: embedding矩阵\n",
    "        \"\"\"\n",
    "        print(f\"正在创建菌株embeddings...\")\n",
    "        \n",
    "        # 统计菌株频次\n",
    "        bacteria_counts = pd.Series(bacteria_names).value_counts()\n",
    "        print(f\"唯一菌株数: {len(bacteria_counts)}\")\n",
    "        \n",
    "        # 创建菌株到ID的映射\n",
    "        bacteria_to_id = {}\n",
    "        id_to_bacteria = {}\n",
    "        \n",
    "        # 为常见菌株分配ID\n",
    "        for i, (bacteria, count) in enumerate(bacteria_counts.items()):\n",
    "            bacteria_to_id[bacteria] = i\n",
    "            id_to_bacteria[i] = bacteria\n",
    "        \n",
    "        # 创建可学习的embedding矩阵\n",
    "        n_bacteria = len(bacteria_to_id)\n",
    "        embedding_matrix = np.random.normal(0, 0.1, (n_bacteria, embedding_dim)).astype(np.float32)\n",
    "        \n",
    "        print(f\"菌株embedding shape: {embedding_matrix.shape}\")\n",
    "        \n",
    "        return bacteria_to_id, embedding_matrix, id_to_bacteria\n",
    "    \n",
    "    def process_all_datasets(self):\n",
    "        \"\"\"\n",
    "        处理所有数据集，生成特征\n",
    "        \"\"\"\n",
    "        print(\"开始特征工程流程...\")\n",
    "        \n",
    "        # 1. 加载处理后的数据\n",
    "        datasets = {}\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            # 条件回归数据集 (包含菌株信息)\n",
    "            conditional_file = os.path.join(self.processed_data_dir, f'grampa_conditional_{split}.csv')\n",
    "            if os.path.exists(conditional_file):\n",
    "                try:\n",
    "                    datasets[f'conditional_{split}'] = pd.read_csv(conditional_file, low_memory=False)\n",
    "                    print(f\"加载 {conditional_file}: {len(datasets[f'conditional_{split}'])} 样本\")\n",
    "                except Exception as e:\n",
    "                    print(f\"加载 {conditional_file} 失败: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # 序列回归数据集 (不含菌株信息)\n",
    "            sequence_file = os.path.join(self.processed_data_dir, f'grampa_sequence_{split}.csv')\n",
    "            if os.path.exists(sequence_file):\n",
    "                try:\n",
    "                    datasets[f'sequence_{split}'] = pd.read_csv(sequence_file, low_memory=False)\n",
    "                    print(f\"加载 {sequence_file}: {len(datasets[f'sequence_{split}'])} 样本\")\n",
    "                except Exception as e:\n",
    "                    print(f\"加载 {sequence_file} 失败: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # 2. 收集所有唯一序列\n",
    "        all_sequences = set()\n",
    "        for dataset_name, df in datasets.items():\n",
    "            if 'sequence' in df.columns:\n",
    "                all_sequences.update(df['sequence'].dropna().unique())\n",
    "        \n",
    "        all_sequences = sorted(list(all_sequences))\n",
    "        print(f\"总共 {len(all_sequences)} 个唯一序列\")\n",
    "        \n",
    "        # 3. 提取PLM embeddings\n",
    "        print(\"\\n=== 提取PLM embeddings ===\")\n",
    "        plm_embeddings = self.extract_plm_embeddings(all_sequences)\n",
    "        \n",
    "        # 创建序列到embedding的映射\n",
    "        seq_to_embedding = {seq: plm_embeddings[i] for i, seq in enumerate(all_sequences)}\n",
    "        \n",
    "        # 4. 计算理化特征\n",
    "        print(\"\\n=== 计算理化特征 ===\")\n",
    "        physicochemical_features = self.calculate_physicochemical_features(all_sequences)\n",
    "        \n",
    "        # 创建序列到理化特征的映射\n",
    "        seq_to_physchem = {seq: physicochemical_features[i] for i, seq in enumerate(all_sequences)}\n",
    "        \n",
    "        # 5. 处理菌株embeddings (仅针对条件回归数据集)\n",
    "        print(\"\\n=== 创建菌株embeddings ===\")\n",
    "        all_bacteria = set()\n",
    "        for dataset_name, df in datasets.items():\n",
    "            if 'conditional' in dataset_name and 'bacterium' in df.columns:\n",
    "                all_bacteria.update(df['bacterium'].dropna().unique())\n",
    "        \n",
    "        all_bacteria = sorted(list(all_bacteria))\n",
    "        bacteria_to_id, bacteria_embedding_matrix, id_to_bacteria = self.create_bacteria_embeddings(all_bacteria)\n",
    "        \n",
    "        # 6. 为每个数据集生成特征\n",
    "        print(\"\\n=== 为各数据集生成特征 ===\")\n",
    "        for dataset_name, df in datasets.items():\n",
    "            print(f\"\\n处理数据集: {dataset_name}\")\n",
    "            \n",
    "            # 序列特征\n",
    "            sequences = df['sequence'].values\n",
    "            dataset_plm_embeddings = np.array([seq_to_embedding.get(seq, np.zeros(plm_embeddings.shape[1])) \n",
    "                                             for seq in sequences])\n",
    "            dataset_physchem_features = np.array([seq_to_physchem.get(seq, np.zeros(physicochemical_features.shape[1])) \n",
    "                                                for seq in sequences])\n",
    "            \n",
    "            # 保存序列特征\n",
    "            features_dict = {\n",
    "                'plm_embeddings': dataset_plm_embeddings,\n",
    "                'physicochemical_features': dataset_physchem_features,\n",
    "                'sequences': sequences\n",
    "            }\n",
    "            \n",
    "            # 如果是条件回归数据集，添加菌株信息\n",
    "            if 'conditional' in dataset_name and 'bacterium' in df.columns:\n",
    "                bacteria = df['bacterium'].values\n",
    "                bacteria_ids = np.array([bacteria_to_id.get(bact, 0) for bact in bacteria])\n",
    "                features_dict['bacteria_ids'] = bacteria_ids\n",
    "                features_dict['bacteria_names'] = bacteria\n",
    "            \n",
    "            # 添加目标变量和其他信息\n",
    "            if 'value_winsorized' in df.columns:\n",
    "                features_dict['targets'] = df['value_winsorized'].values\n",
    "            if 'is_censored' in df.columns:\n",
    "                features_dict['is_censored'] = df['is_censored'].values\n",
    "            if 'censoring_threshold' in df.columns:\n",
    "                features_dict['censoring_threshold'] = df['censoring_threshold'].fillna(0).values\n",
    "            if 'n_measurements' in df.columns:\n",
    "                features_dict['sample_weights'] = df['n_measurements'].values\n",
    "            \n",
    "            # 保存特征文件\n",
    "            output_file = os.path.join(self.features_output_dir, f'{dataset_name}_features.pkl')\n",
    "            with open(output_file, 'wb') as f:\n",
    "                pickle.dump(features_dict, f)\n",
    "            print(f\"保存特征文件: {output_file}\")\n",
    "            print(f\"  - PLM embeddings: {dataset_plm_embeddings.shape}\")\n",
    "            print(f\"  - 理化特征: {dataset_physchem_features.shape}\")\n",
    "            if 'bacteria_ids' in features_dict:\n",
    "                print(f\"  - 菌株IDs: {features_dict['bacteria_ids'].shape}\")\n",
    "        \n",
    "        # 7. 保存全局映射和embedding矩阵\n",
    "        print(\"\\n=== 保存全局映射 ===\")\n",
    "        \n",
    "        # 保存序列特征映射\n",
    "        seq_features_mapping = {\n",
    "            'seq_to_plm': seq_to_embedding,\n",
    "            'seq_to_physchem': seq_to_physchem,\n",
    "            'plm_embedding_dim': plm_embeddings.shape[1],\n",
    "            'physchem_feature_dim': physicochemical_features.shape[1]\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.features_output_dir, 'sequence_features_mapping.pkl'), 'wb') as f:\n",
    "            pickle.dump(seq_features_mapping, f)\n",
    "        \n",
    "        # 保存菌株映射和embedding\n",
    "        bacteria_mapping = {\n",
    "            'bacteria_to_id': bacteria_to_id,\n",
    "            'id_to_bacteria': id_to_bacteria,\n",
    "            'bacteria_embedding_matrix': bacteria_embedding_matrix,\n",
    "            'embedding_dim': bacteria_embedding_matrix.shape[1]\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.features_output_dir, 'bacteria_mapping.pkl'), 'wb') as f:\n",
    "            pickle.dump(bacteria_mapping, f)\n",
    "        \n",
    "        print(f\"\\n特征工程完成！所有文件保存在: {self.features_output_dir}\")\n",
    "        \n",
    "        # 8. 生成特征工程报告\n",
    "        self.generate_feature_report(datasets, seq_features_mapping, bacteria_mapping)\n",
    "    \n",
    "    def generate_feature_report(self, datasets, seq_features_mapping, bacteria_mapping):\n",
    "        \"\"\"\n",
    "        生成特征工程报告\n",
    "        \"\"\"\n",
    "        report_file = os.path.join(self.features_output_dir, 'feature_engineering_report.md')\n",
    "        \n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# 特征工程报告\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 数据集统计\\n\")\n",
    "            for dataset_name, df in datasets.items():\n",
    "                f.write(f\"- {dataset_name}: {len(df)} 样本\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## 特征维度\\n\")\n",
    "            f.write(f\"- PLM embeddings: {seq_features_mapping['plm_embedding_dim']} 维\\n\")\n",
    "            f.write(f\"- 理化特征: {seq_features_mapping['physchem_feature_dim']} 维\\n\")\n",
    "            f.write(f\"- 菌株embedding: {bacteria_mapping['embedding_dim']} 维\\n\")\n",
    "            f.write(f\"- 总序列数: {len(seq_features_mapping['seq_to_plm'])}\\n\")\n",
    "            f.write(f\"- 总菌株数: {len(bacteria_mapping['bacteria_to_id'])}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 理化特征列表\\n\")\n",
    "            feature_names = [\n",
    "                \"序列长度\", \"分子量\", \"等电点\", \"GRAVY疏水性\", \"净电荷\", \"RK比例\", \"疏水矩\",\n",
    "                \"正电荷比例\", \"负电荷比例\", \"极性残基比例\"\n",
    "            ] + [f\"氨基酸_{aa}_比例\" for aa in 'ACDEFGHIKLMNPQRSTVWY']\n",
    "            \n",
    "            for i, name in enumerate(feature_names):\n",
    "                f.write(f\"{i+1}. {name}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Top 10 菌株\\n\")\n",
    "            bacteria_counts = pd.Series(list(bacteria_mapping['bacteria_to_id'].keys())).value_counts()\n",
    "            for i, (bacteria, _) in enumerate(bacteria_counts.head(10).items(), 1):\n",
    "                f.write(f\"{i}. {bacteria}\\n\")\n",
    "            \n",
    "        print(f\"特征工程报告已保存: {report_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 配置参数\n",
    "    processed_data_dir = \"/Users/ricardozhao/PycharmProjects/AMP/processed_data\"\n",
    "    features_output_dir = \"/Users/ricardozhao/PycharmProjects/AMP/features\"\n",
    "    \n",
    "    # 创建特征提取器\n",
    "    extractor = AMP_FeatureExtractor(\n",
    "        processed_data_dir=processed_data_dir,\n",
    "        features_output_dir=features_output_dir,\n",
    "        esm_model_name='facebook/esm2_t33_650M_UR50D',  # ESM-2 650M参数版本\n",
    "        device='auto'\n",
    "    )\n",
    "    \n",
    "    # 执行特征工程\n",
    "    extractor.process_all_datasets()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
