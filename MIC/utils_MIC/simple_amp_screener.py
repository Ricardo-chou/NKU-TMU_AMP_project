#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆAMPç­›é€‰å™¨
åŸºäºä¸‰å±‚çº§è”ï¼šAå¿«ç­› â†’ Bâ‚‚åŒå¤´ç²¾ç­› â†’ Bâ‚é¢æ¿å¹¿è°±

æ ¸å¿ƒæ€è·¯ï¼š
1. Aå¿«ç­›ï¼šlogMIC â‰¤ 1.0ä¿ç•™ï¼Œâ‰¤ 0.7å¼ºæ¨ï¼Œæ‰¹å†…minmaxå½’ä¸€åŒ–
2. Bâ‚‚åŒå¤´ï¼šE.coli/S.aureusåŒèŒæ ªåº•çº¿ï¼ŒçŸ­æ¿ä¼˜å…ˆè¯„åˆ†
3. Bâ‚é¢æ¿ï¼š6èŒæ ªå¹¿è°±è¯„ä¼°ï¼Œhit@10Î¼M + broad + worst
4. æœ€ç»ˆæ‰“åˆ†ï¼šS = 0.4*s_A + 0.4*s_T + 0.2*s_B
5. åˆ†å±‚é€‰æ‹©ï¼šS/A/B/Cåˆ†å±‚ + CD-HITå»å†—ä½™ + é…é¢åˆ†é…
"""

import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import pickle
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹æ¶æ„å’Œç‰¹å¾æå–
sys.path.append('/root/NKU-TMU_AMP_project')
from train_discriminators import SequenceRegressionModel, ConditionalRegressionModel
from train_dual_head_model import DualHeadModel
from feature_engineering import AMP_FeatureExtractor

class SimpleAMPScreener:
    """ç®€åŒ–ç‰ˆAMPç­›é€‰å™¨"""
    
    def __init__(self, model_dir='model_outputs', features_dir='features', device='auto'):
        """
        åˆå§‹åŒ–ç­›é€‰å™¨
        
        Args:
            model_dir: æ¨¡å‹æ–‡ä»¶ç›®å½•
            features_dir: ç‰¹å¾æ–‡ä»¶ç›®å½•  
            device: è®¡ç®—è®¾å¤‡
        """
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        self.model_dir = model_dir
        self.features_dir = features_dir
        
        # ç­›é€‰å‚æ•°ï¼ˆå›ºå®šï¼Œä¸éœ€è¦å¤æ‚è°ƒä¼˜ï¼‰
        self.params = {
            # é˜ˆå€¼è®¾ç½®
            'gate_threshold': 1.0,      # Aæ¨¡å‹é—¨æ§é˜ˆå€¼ï¼ˆ10Î¼Mï¼‰
            'strong_threshold': 0.7,    # å¼ºæ¨é˜ˆå€¼ï¼ˆ5Î¼Mï¼‰
            'dual_threshold': 1.0,      # åŒå¤´é˜ˆå€¼
            
            # æƒé‡è®¾ç½®
            'w_A': 0.4,                 # Aæ¨¡å‹æƒé‡
            'w_T': 0.4,                 # åŒå¤´æ¨¡å‹æƒé‡  
            'w_B': 0.2,                 # å¹¿è°±æ¨¡å‹æƒé‡
            'dual_min_weight': 0.7,     # åŒå¤´çŸ­æ¿æƒé‡
            'dual_avg_weight': 0.3,     # åŒå¤´å‡å€¼æƒé‡
            
            # é¢æ¿æƒé‡
            'hit_weight': 0.6,          # å‘½ä¸­ç‡æƒé‡
            'broad_weight': 0.2,        # å¹¿è°±æƒé‡
            'worst_weight': 0.2,        # æœ€å·®æƒ…å†µæƒé‡
            
            # åˆ†å±‚æ¯”ä¾‹
            'tier_S_ratio': 0.10,       # Sçº§æ¯”ä¾‹
            'tier_A_ratio': 0.30,       # Açº§æ¯”ä¾‹  
            'tier_B_ratio': 0.60,       # Bçº§æ¯”ä¾‹
            
            # å»å†—ä½™è®¾ç½®
            'similarity_threshold': 0.8, # åºåˆ—ç›¸ä¼¼åº¦é˜ˆå€¼
        }
        
        # é¢æ¿èŒæ ªï¼ˆ6ä¸ªä»£è¡¨æ€§èŒæ ªï¼‰
        self.panel_bacteria = [
            'escherichia_coli', 
            'staphylococcus_aureus',
            'pseudomonas_aeruginosa', 
            'klebsiella_pneumoniae',
            'acinetobacter_baumannii', 
            'enterococcus_faecalis'
        ]
        
        print(f"ç®€åŒ–ç‰ˆAMPç­›é€‰å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_models(self):
        """åŠ è½½ä¸‰ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
        
        # åŠ è½½ç‰¹å¾æå–å™¨
        self.feature_extractor = AMP_FeatureExtractor(
            processed_data_dir=f'/root/NKU-TMU_AMP_project/processed_data',
            features_output_dir=f'/root/NKU-TMU_AMP_project/{self.features_dir}',
            device=self.device
        )
        
        # åŠ è½½æ ‡å‡†åŒ–å™¨
        with open(f'{self.features_dir}/sequence_train_features.pkl', 'rb') as f:
            seq_features = pickle.load(f)
        self.seq_scaler = StandardScaler()
        self.seq_scaler.fit(seq_features['physicochemical_features'])
        
        with open(f'{self.features_dir}/conditional_train_features.pkl', 'rb') as f:
            cond_features = pickle.load(f)
        self.cond_scaler = StandardScaler()
        self.cond_scaler.fit(cond_features['physicochemical_features'])
        
        # åŠ è½½èŒæ ªæ˜ å°„
        with open(f'{self.features_dir}/bacteria_mapping.pkl', 'rb') as f:
            self.bacteria_mapping = pickle.load(f)
        
        # è®¡ç®—è¾“å…¥ç»´åº¦
        plm_dim = seq_features['plm_embeddings'].shape[1]
        physchem_dim = seq_features['physicochemical_features'].shape[1]
        input_dim = plm_dim + physchem_dim
        n_bacteria = len(self.bacteria_mapping['bacteria_to_id'])
        
        # 1. åºåˆ—èšåˆæ¨¡å‹ (A)
        self.model_A = SequenceRegressionModel(input_dim=input_dim).to(self.device)
        self.model_A.load_state_dict(torch.load(f'{self.model_dir}/sequence_regression_best.pt', map_location=self.device))
        self.model_A.eval()
        
        # 2. æ¡ä»¶å›å½’æ¨¡å‹ (Bâ‚)  
        self.model_B1 = ConditionalRegressionModel(
            input_dim=input_dim, 
            n_bacteria=n_bacteria,
            bacteria_embedding_dim=32
        ).to(self.device)
        self.model_B1.load_state_dict(torch.load(f'{self.model_dir}/conditional_regression_best.pt', map_location=self.device))
        self.model_B1.eval()
        
        # 3. åŒå¤´æ¨¡å‹ (Bâ‚‚)
        self.model_B2 = DualHeadModel(input_dim=input_dim, hidden_dims=[512, 256]).to(self.device)
        self.model_B2.load_state_dict(torch.load(f'{self.model_dir}/dual_head_best.pt', map_location=self.device))
        self.model_B2.eval()
        
        print("âœ“ æ¨¡å‹A (åºåˆ—èšåˆ) åŠ è½½å®Œæˆ")
        print("âœ“ æ¨¡å‹Bâ‚ (æ¡ä»¶å›å½’) åŠ è½½å®Œæˆ") 
        print("âœ“ æ¨¡å‹Bâ‚‚ (åŒå¤´æ¨¡å‹) åŠ è½½å®Œæˆ")
    
    def extract_features(self, sequences):
        """æå–åºåˆ—ç‰¹å¾"""
        print(f"æ­£åœ¨æå– {len(sequences)} ä¸ªåºåˆ—çš„ç‰¹å¾...")
        
        # PLM embeddings
        plm_embeddings = self.feature_extractor.extract_plm_embeddings(sequences)
        
        # ç†åŒ–ç‰¹å¾
        physchem_features = self.feature_extractor.calculate_physicochemical_features(sequences)
        
        # æ ‡å‡†åŒ–ç†åŒ–ç‰¹å¾
        seq_features = np.concatenate([
            plm_embeddings,
            self.seq_scaler.transform(physchem_features)
        ], axis=1)
        
        cond_features = np.concatenate([
            plm_embeddings,
            self.cond_scaler.transform(physchem_features)
        ], axis=1)
        
        return {
            'seq_features': seq_features,
            'cond_features': cond_features,
            'sequences': sequences
        }
    
    def minmax_normalize(self, values):
        """æ‰¹å†…æœ€å°-æœ€å¤§å½’ä¸€åŒ–"""
        min_val = np.min(values)
        max_val = np.max(values)
        if max_val == min_val:
            return np.ones_like(values) * 0.5  # å¦‚æœéƒ½ç›¸åŒï¼Œè¿”å›ä¸­æ€§åˆ†æ•°
        return (values - min_val) / (max_val - min_val)
    
    def step1_gate_screening(self, features):
        """Step 1: Aæ¨¡å‹å¿«ç­›ï¼ˆä¸€çº§é—¨æ§ï¼‰"""
        print("\n=== Step 1: Aæ¨¡å‹å¿«ç­› ===")
        
        seq_features = features['seq_features']
        
        # Aæ¨¡å‹é¢„æµ‹
        with torch.no_grad():
            seq_tensor = torch.FloatTensor(seq_features).to(self.device)
            logMIC_A = self.model_A(seq_tensor).cpu().numpy()
        
        # é—¨æ§è§„åˆ™
        gate_pass = logMIC_A <= self.params['gate_threshold']
        strong_pass = logMIC_A <= self.params['strong_threshold']
        
        # æ‰¹å†…å½’ä¸€åŒ–åˆ†æ•°ï¼ˆ1 - minmaxï¼Œè¶Šå¤§è¶Šå¥½ï¼‰
        s_A = 1 - self.minmax_normalize(logMIC_A)
        
        print(f"é—¨æ§é€šè¿‡: {gate_pass.sum()}/{len(logMIC_A)} ({100*gate_pass.sum()/len(logMIC_A):.1f}%)")
        print(f"å¼ºæ¨å€™é€‰: {strong_pass.sum()}/{len(logMIC_A)} ({100*strong_pass.sum()/len(logMIC_A):.1f}%)")
        print(f"Aæ¨¡å‹é¢„æµ‹èŒƒå›´: [{np.min(logMIC_A):.3f}, {np.max(logMIC_A):.3f}]")
        
        return {
            'logMIC_A': logMIC_A,
            'gate_pass': gate_pass,
            'strong_pass': strong_pass,
            's_A': s_A
        }
    
    def step2_dual_head_screening(self, features, gate_results):
        """Step 2: Bâ‚‚åŒå¤´ç²¾ç­›ï¼ˆåŒèŒæ ªåº•çº¿ï¼‰"""
        print("\n=== Step 2: Bâ‚‚åŒå¤´ç²¾ç­› ===")
        
        cond_features = features['cond_features']
        gate_pass = gate_results['gate_pass']
        
        # åªå¯¹é€šè¿‡é—¨æ§çš„åºåˆ—è¿›è¡ŒåŒå¤´é¢„æµ‹
        if gate_pass.sum() == 0:
            print("âš ï¸ æ²¡æœ‰åºåˆ—é€šè¿‡é—¨æ§ç­›é€‰")
            return self._empty_dual_results(len(features['sequences']))
        
        # Bâ‚‚æ¨¡å‹é¢„æµ‹
        with torch.no_grad():
            cond_tensor = torch.FloatTensor(cond_features).to(self.device)
            logMIC_E, logMIC_S = self.model_B2(cond_tensor)
            logMIC_E = logMIC_E.cpu().numpy()
            logMIC_S = logMIC_S.cpu().numpy()
        
        # åŒå¤´è§„åˆ™
        dual_threshold = self.params['dual_threshold']
        strong_threshold = self.params['strong_threshold']
        
        dual_pass = (logMIC_E <= dual_threshold) & (logMIC_S <= dual_threshold)
        strong_dual_pass = ((logMIC_E <= strong_threshold) | (logMIC_S <= strong_threshold)) & dual_pass
        single_pass = ((logMIC_E <= dual_threshold) & (logMIC_S > dual_threshold)) | \
                     ((logMIC_E > dual_threshold) & (logMIC_S <= dual_threshold))
        
        # æ‰¹å†…å½’ä¸€åŒ–åˆ†æ•°
        s_E = 1 - self.minmax_normalize(logMIC_E)
        s_S = 1 - self.minmax_normalize(logMIC_S)
        
        # çŸ­æ¿ä¼˜å…ˆåˆæˆåˆ†æ•°
        min_score = np.minimum(s_E, s_S)
        avg_score = (s_E + s_S) / 2
        s_T = self.params['dual_min_weight'] * min_score + self.params['dual_avg_weight'] * avg_score
        
        print(f"åŒèŒæ ªé€šè¿‡: {dual_pass.sum()}/{len(logMIC_E)} ({100*dual_pass.sum()/len(logMIC_E):.1f}%)")
        print(f"å¼ºåŒèŒæ ª: {strong_dual_pass.sum()}/{len(logMIC_E)} ({100*strong_dual_pass.sum()/len(logMIC_E):.1f}%)")
        print(f"å•èŒæ ªé€šè¿‡: {single_pass.sum()}/{len(logMIC_E)} ({100*single_pass.sum()/len(logMIC_E):.1f}%)")
        print(f"E.colié¢„æµ‹èŒƒå›´: [{np.min(logMIC_E):.3f}, {np.max(logMIC_E):.3f}]")
        print(f"S.aureusé¢„æµ‹èŒƒå›´: [{np.min(logMIC_S):.3f}, {np.max(logMIC_S):.3f}]")
        
        return {
            'logMIC_E': logMIC_E,
            'logMIC_S': logMIC_S,
            'dual_pass': dual_pass,
            'strong_dual_pass': strong_dual_pass,
            'single_pass': single_pass,
            's_E': s_E,
            's_S': s_S,
            's_T': s_T
        }
    
    def _empty_dual_results(self, n_sequences):
        """åˆ›å»ºç©ºçš„åŒå¤´ç»“æœ"""
        return {
            'logMIC_E': np.full(n_sequences, 10.0),  # é«˜å€¼è¡¨ç¤ºæ— æ•ˆ
            'logMIC_S': np.full(n_sequences, 10.0),
            'dual_pass': np.zeros(n_sequences, dtype=bool),
            'strong_dual_pass': np.zeros(n_sequences, dtype=bool),
            'single_pass': np.zeros(n_sequences, dtype=bool),
            's_E': np.zeros(n_sequences),
            's_S': np.zeros(n_sequences),
            's_T': np.zeros(n_sequences)
        }
    
    def step3_panel_evaluation(self, features, gate_results, dual_results):
        """Step 3: Bâ‚é¢æ¿å¹¿è°±è¯„ä¼°"""
        print("\n=== Step 3: Bâ‚é¢æ¿å¹¿è°±è¯„ä¼° ===")
        
        cond_features = features['cond_features']
        gate_pass = gate_results['gate_pass']
        
        # åªå¯¹é€šè¿‡å‰ä¸¤æ­¥çš„åºåˆ—è¿›è¡Œé¢æ¿è¯„ä¼°
        candidates = gate_pass & (dual_results['dual_pass'] | dual_results['single_pass'])
        
        if candidates.sum() == 0:
            print("âš ï¸ æ²¡æœ‰åºåˆ—é€šè¿‡å‰ä¸¤æ­¥ç­›é€‰")
            return self._empty_panel_results(len(features['sequences']))
        
        print(f"é¢æ¿è¯„ä¼°å€™é€‰: {candidates.sum()}/{len(features['sequences'])} ä¸ªåºåˆ—")
        
        # Bâ‚æ¨¡å‹é¢æ¿é¢„æµ‹
        panel_predictions = {}
        
        for bacteria_name in self.panel_bacteria:
            if bacteria_name in self.bacteria_mapping['bacteria_to_id']:
                bacteria_id = self.bacteria_mapping['bacteria_to_id'][bacteria_name]
                bacteria_ids = np.full(len(features['sequences']), bacteria_id)
                
                with torch.no_grad():
                    cond_tensor = torch.FloatTensor(cond_features).to(self.device)
                    bacteria_tensor = torch.LongTensor(bacteria_ids).to(self.device)
                    pred = self.model_B1(cond_tensor, bacteria_tensor).cpu().numpy()
                
                panel_predictions[bacteria_name] = pred
        
        if len(panel_predictions) == 0:
            print("âš ï¸ é¢æ¿èŒæ ªæ˜ å°„å¤±è´¥")
            return self._empty_panel_results(len(features['sequences']))
        
        # è½¬æ¢ä¸ºæ•°ç»„
        panel_matrix = np.array(list(panel_predictions.values()))  # (n_bacteria, n_sequences)
        
        # è®¡ç®—é¢æ¿æŒ‡æ ‡
        hit_at_10 = np.mean(panel_matrix <= 1.0, axis=0)  # å‘½ä¸­ç‡@10Î¼M
        broad_mean = np.mean(panel_matrix, axis=0)         # é¢æ¿å‡å€¼
        worst_max = np.max(panel_matrix, axis=0)           # æœ€å·®å€¼
        
        # æ‰¹å†…å½’ä¸€åŒ–åˆ†æ•°
        s_hit = hit_at_10  # å‘½ä¸­ç‡æœ¬èº«å°±æ˜¯0-1
        s_broad = 1 - self.minmax_normalize(broad_mean)    # å‡å€¼è¶Šå°è¶Šå¥½
        s_worst = 1 - self.minmax_normalize(worst_max)     # æœ€å·®è¶Šå°è¶Šå¥½
        
        # åˆæˆé¢æ¿åˆ†æ•°
        s_B = (self.params['hit_weight'] * s_hit + 
               self.params['broad_weight'] * s_broad + 
               self.params['worst_weight'] * s_worst)
        
        print(f"å¹³å‡å‘½ä¸­ç‡@10Î¼M: {np.mean(hit_at_10):.3f}")
        print(f"é¢æ¿å‡å€¼èŒƒå›´: [{np.min(broad_mean):.3f}, {np.max(broad_mean):.3f}]")
        print(f"æœ€å·®å€¼èŒƒå›´: [{np.min(worst_max):.3f}, {np.max(worst_max):.3f}]")
        
        return {
            'panel_predictions': panel_predictions,
            'hit_at_10': hit_at_10,
            'broad_mean': broad_mean,
            'worst_max': worst_max,
            's_hit': s_hit,
            's_broad': s_broad,
            's_worst': s_worst,
            's_B': s_B
        }
    
    def _empty_panel_results(self, n_sequences):
        """åˆ›å»ºç©ºçš„é¢æ¿ç»“æœ"""
        return {
            'panel_predictions': {},
            'hit_at_10': np.zeros(n_sequences),
            'broad_mean': np.full(n_sequences, 5.0),
            'worst_max': np.full(n_sequences, 10.0),
            's_hit': np.zeros(n_sequences),
            's_broad': np.zeros(n_sequences),
            's_worst': np.zeros(n_sequences),
            's_B': np.zeros(n_sequences)
        }
    
    def calculate_final_scores(self, gate_results, dual_results, panel_results):
        """è®¡ç®—æœ€ç»ˆåˆ†æ•°"""
        print("\n=== è®¡ç®—æœ€ç»ˆåˆ†æ•° ===")
        
        s_A = gate_results['s_A']
        s_T = dual_results['s_T']
        s_B = panel_results['s_B']
        
        # æœ€ç»ˆåˆ†æ•°
        S_final = (self.params['w_A'] * s_A + 
                   self.params['w_T'] * s_T + 
                   self.params['w_B'] * s_B)
        
        print(f"æœ€ç»ˆåˆ†æ•°èŒƒå›´: [{np.min(S_final):.3f}, {np.max(S_final):.3f}]")
        print(f"æœ€ç»ˆåˆ†æ•°å‡å€¼: {np.mean(S_final):.3f}")
        
        return S_final
    
    def apply_tier_classification(self, final_scores):
        """åº”ç”¨åˆ†å±‚åˆ†ç±»"""
        print("\n=== åˆ†å±‚åˆ†ç±» ===")
        
        n_sequences = len(final_scores)
        
        # è®¡ç®—åˆ†ä½æ•°é˜ˆå€¼
        tier_S_threshold = np.percentile(final_scores, 100 - self.params['tier_S_ratio'] * 100)
        tier_A_threshold = np.percentile(final_scores, 100 - self.params['tier_A_ratio'] * 100)
        tier_B_threshold = np.percentile(final_scores, 100 - self.params['tier_B_ratio'] * 100)
        
        # åˆ†å±‚
        tiers = []
        for score in final_scores:
            if score >= tier_S_threshold:
                tiers.append('S')
            elif score >= tier_A_threshold:
                tiers.append('A')
            elif score >= tier_B_threshold:
                tiers.append('B')
            else:
                tiers.append('C')
        
        tiers = np.array(tiers)
        
        # ç»Ÿè®¡
        tier_counts = {tier: (tiers == tier).sum() for tier in ['S', 'A', 'B', 'C']}
        
        print("åˆ†å±‚ç»Ÿè®¡:")
        for tier, count in tier_counts.items():
            print(f"  {tier}çº§: {count} ({100*count/n_sequences:.1f}%)")
        
        return tiers, tier_counts
    
    def apply_diversity_sampling(self, sequences, final_scores, tiers, top_k=2000):
        """åº”ç”¨å¤šæ ·æ€§å»å†—ä½™å’Œé…é¢åˆ†é…"""
        print(f"\n=== å¤šæ ·æ€§é‡‡æ · (ç›®æ ‡: {top_k}) ===")
        
        # ç®€åŒ–ç‰ˆï¼šæŒ‰åˆ†å±‚ä¼˜å…ˆçº§ + åˆ†æ•°æ’åº
        # å®é™…é¡¹ç›®ä¸­å¯ä»¥åŠ å…¥CD-HITæˆ–åºåˆ—ç›¸ä¼¼åº¦å»å†—ä½™
        
        tier_priority = {'S': 0, 'A': 1, 'B': 2, 'C': 3}
        
        # åˆ›å»ºæ’åºç´¢å¼•
        indices = np.arange(len(sequences))
        sort_keys = [(tier_priority[tier], -score, idx) for idx, (tier, score) in enumerate(zip(tiers, final_scores))]
        sorted_indices = sorted(range(len(sort_keys)), key=lambda i: sort_keys[i])
        
        # å–å‰top_kä¸ª
        selected_indices = sorted_indices[:min(top_k, len(sorted_indices))]
        
        # ç»Ÿè®¡é€‰ä¸­çš„åˆ†å±‚åˆ†å¸ƒ
        selected_tiers = tiers[selected_indices]
        selected_tier_counts = {tier: (selected_tiers == tier).sum() for tier in ['S', 'A', 'B', 'C']}
        
        print("é€‰ä¸­åºåˆ—åˆ†å±‚åˆ†å¸ƒ:")
        for tier, count in selected_tier_counts.items():
            if count > 0:
                print(f"  {tier}çº§: {count} ({100*count/len(selected_indices):.1f}%)")
        
        return {
            'selected_indices': selected_indices,
            'selected_sequences': [sequences[i] for i in selected_indices],
            'selected_scores': final_scores[selected_indices],
            'selected_tiers': selected_tiers,
            'tier_counts': selected_tier_counts
        }
    
    def screen_sequences(self, sequences, top_k=2000):
        """ä¸»è¦ç­›é€‰å‡½æ•°"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹ç®€åŒ–ç‰ˆAMPç­›é€‰ - è¾“å…¥åºåˆ—: {len(sequences)}")
        print(f"{'='*60}")
        
        # ç‰¹å¾æå–
        features = self.extract_features(sequences)
        
        # Step 1: Aæ¨¡å‹å¿«ç­›
        gate_results = self.step1_gate_screening(features)
        
        # Step 2: Bâ‚‚åŒå¤´ç²¾ç­›
        dual_results = self.step2_dual_head_screening(features, gate_results)
        
        # Step 3: Bâ‚é¢æ¿è¯„ä¼°
        panel_results = self.step3_panel_evaluation(features, gate_results, dual_results)
        
        # æœ€ç»ˆåˆ†æ•°è®¡ç®—
        final_scores = self.calculate_final_scores(gate_results, dual_results, panel_results)
        
        # åˆ†å±‚åˆ†ç±»
        tiers, tier_counts = self.apply_tier_classification(final_scores)
        
        # å¤šæ ·æ€§é‡‡æ ·
        selection_results = self.apply_diversity_sampling(sequences, final_scores, tiers, top_k)
        
        # æ•´åˆç»“æœ
        screening_results = {
            'input_sequences': sequences,
            'features': features,
            'gate_results': gate_results,
            'dual_results': dual_results,
            'panel_results': panel_results,
            'final_scores': final_scores,
            'tiers': tiers,
            'tier_counts': tier_counts,
            'selection_results': selection_results,
            'params': self.params.copy()
        }
        
        print(f"\n{'='*60}")
        print(f"ç­›é€‰å®Œæˆï¼ä» {len(sequences)} ä¸ªåºåˆ—ä¸­é€‰å‡º {len(selection_results['selected_sequences'])} ä¸ª")
        print(f"{'='*60}")
        
        return screening_results
    
    def save_results(self, screening_results, output_prefix):
        """ä¿å­˜ç­›é€‰ç»“æœ"""
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(f'{output_prefix}_detailed_results.pkl', 'wb') as f:
            pickle.dump(screening_results, f)
        
        # ä¿å­˜é€‰ä¸­åºåˆ—CSV
        selection = screening_results['selection_results']
        df_selected = pd.DataFrame({
            'sequence': selection['selected_sequences'],
            'final_score': selection['selected_scores'],
            'tier': selection['selected_tiers'],
            'rank': range(1, len(selection['selected_sequences']) + 1)
        })
        
        df_selected.to_csv(f'{output_prefix}_selected_sequences.csv', index=False)
        
        # ä¿å­˜ç­›é€‰æŠ¥å‘Š
        self.generate_report(screening_results, f'{output_prefix}_screening_report.md')
        
        print(f"âœ“ è¯¦ç»†ç»“æœ: {output_prefix}_detailed_results.pkl")
        print(f"âœ“ é€‰ä¸­åºåˆ—: {output_prefix}_selected_sequences.csv")
        print(f"âœ“ ç­›é€‰æŠ¥å‘Š: {output_prefix}_screening_report.md")
    
    def generate_report(self, screening_results, report_file):
        """ç”Ÿæˆç­›é€‰æŠ¥å‘Š"""
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ç®€åŒ–ç‰ˆAMPç­›é€‰æŠ¥å‘Š\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            f.write("## ç­›é€‰æ¦‚å†µ\n")
            f.write(f"- è¾“å…¥åºåˆ—æ•°: {len(screening_results['input_sequences'])}\n")
            f.write(f"- æœ€ç»ˆé€‰ä¸­æ•°: {len(screening_results['selection_results']['selected_sequences'])}\n")
            f.write(f"- ç­›é€‰æˆåŠŸç‡: {len(screening_results['selection_results']['selected_sequences']) / len(screening_results['input_sequences']) * 100:.1f}%\n\n")
            
            # å„æ­¥éª¤ç»Ÿè®¡
            f.write("## å„æ­¥éª¤ç­›é€‰ç»Ÿè®¡\n")
            gate_pass = screening_results['gate_results']['gate_pass'].sum()
            dual_pass = screening_results['dual_results']['dual_pass'].sum()
            
            f.write(f"- Step 1 (Aå¿«ç­›): {gate_pass}/{len(screening_results['input_sequences'])} ({100*gate_pass/len(screening_results['input_sequences']):.1f}%) é€šè¿‡\n")
            f.write(f"- Step 2 (åŒå¤´): {dual_pass}/{len(screening_results['input_sequences'])} ({100*dual_pass/len(screening_results['input_sequences']):.1f}%) åŒèŒæ ªé€šè¿‡\n")
            f.write(f"- Step 3 (é¢æ¿): å¯¹é€šè¿‡å‰ä¸¤æ­¥çš„åºåˆ—è¿›è¡Œå¹¿è°±è¯„ä¼°\n\n")
            
            # åˆ†å±‚ç»Ÿè®¡
            f.write("## åˆ†å±‚ç»Ÿè®¡\n")
            for tier, count in screening_results['tier_counts'].items():
                f.write(f"- {tier}çº§: {count} ä¸ªåºåˆ—\n")
            f.write("\n")
            
            # é€‰ä¸­åºåˆ—åˆ†å±‚åˆ†å¸ƒ
            f.write("## é€‰ä¸­åºåˆ—åˆ†å¸ƒ\n")
            for tier, count in screening_results['selection_results']['tier_counts'].items():
                if count > 0:
                    f.write(f"- {tier}çº§: {count} ä¸ªåºåˆ—\n")
            f.write("\n")
            
            # å‚æ•°è®¾ç½®
            f.write("## å‚æ•°è®¾ç½®\n")
            for key, value in screening_results['params'].items():
                f.write(f"- {key}: {value}\n")
            f.write("\n")
            
            # Top 20åºåˆ—
            f.write("## Top 20 é€‰ä¸­åºåˆ—\n")
            selection = screening_results['selection_results']
            for i in range(min(20, len(selection['selected_sequences']))):
                seq = selection['selected_sequences'][i]
                score = selection['selected_scores'][i]
                tier = selection['selected_tiers'][i]
                f.write(f"{i+1:2d}. [{tier}] {seq} (åˆ†æ•°: {score:.4f})\n")

def main():
    """ä¸»å‡½æ•°"""
    # è¯»å–å€™é€‰åºåˆ—
    input_file = '/root/NKU-TMU_AMP_project/decode/filtered_candidate_sequences.csv'
    
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        print("è¯·å…ˆè¿è¡Œåºåˆ—é¢„å¤„ç†è„šæœ¬ç”Ÿæˆå€™é€‰åºåˆ—é›†")
        return
    
    print(f"è¯»å–å€™é€‰åºåˆ—: {input_file}")
    df = pd.read_csv(input_file)
    
    # æ£€æŸ¥åºåˆ—åˆ—å
    seq_column = None
    for col in ['aa_seq', 'sequence', 'seq']:
        if col in df.columns:
            seq_column = col
            break
    
    if seq_column is None:
        print(f"âŒ è¾“å…¥æ–‡ä»¶ç¼ºå°‘åºåˆ—åˆ—ï¼Œå¯ç”¨åˆ—: {list(df.columns)}")
        return
    
    sequences = df[seq_column].tolist()
    print(f"åŠ è½½äº† {len(sequences)} ä¸ªå€™é€‰åºåˆ—ï¼ˆåˆ—å: {seq_column}ï¼‰")
    
    # åˆ›å»ºç­›é€‰å™¨
    screener = SimpleAMPScreener(
        model_dir='/root/NKU-TMU_AMP_project/model_outputs',
        features_dir='/root/NKU-TMU_AMP_project/features'
    )
    
    # åŠ è½½æ¨¡å‹
    screener.load_models()
    
    # æ‰§è¡Œç­›é€‰
    screening_results = screener.screen_sequences(sequences, top_k=2000)
    
    # ä¿å­˜ç»“æœ
    output_prefix = '/root/NKU-TMU_AMP_project/decode/screening_results'
    screener.save_results(screening_results, output_prefix)
    
    print(f"\nğŸ‰ ç­›é€‰å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_prefix}_*")

if __name__ == "__main__":
    main()
