#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ProT-Diff Embeddingsè§£ç ä¸åˆ†æä¸€ä½“åŒ–è„šæœ¬

åŠŸèƒ½ï¼š
1. è§£ç ProT-Diffç”Ÿæˆçš„embeddingsä¸ºæ°¨åŸºé…¸åºåˆ—
2. åˆ†ææ°¨åŸºé…¸ç»„æˆå’Œè´¨é‡è¯„ä¼°
3. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šå’Œå¯è§†åŒ–ç»“æœ
4. æä¾›ä¼˜åŒ–å»ºè®®

ä½œè€…ï¼šåŸºäºProT-Diffå’ŒProtT5çš„æŠ—èŒè‚½åºåˆ—è§£ç 
"""

import os, sys, json, math, time, argparse, traceback
from typing import List, Optional, Dict, Tuple
from collections import Counter
import torch
import torch.nn as nn
import pandas as pd
import numpy as np

# HF Transformers
from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer
from transformers.modeling_outputs import BaseModelOutput

def parse_args():
    p = argparse.ArgumentParser(description="è§£ç å¹¶åˆ†æProT-Diff embeddings")
    
    # è¾“å…¥è¾“å‡ºå‚æ•°
    p.add_argument("--pt_path", type=str, required=True,
                   help="ProT-Diffç”Ÿæˆçš„embeddingsæ–‡ä»¶è·¯å¾„")
    p.add_argument("--model_dir", type=str, default="/root/autodl-tmp/prot_t5_xl_uniref50",
                   help="ProtT5æ¨¡å‹ç›®å½•")
    p.add_argument("--out_prefix", type=str, default="decoded",
                   help="è¾“å‡ºæ–‡ä»¶å‰ç¼€")
    
    # è§£ç å‚æ•°
    p.add_argument("--batch_size", type=int, default=32, help="æ‰¹å¤„ç†å¤§å°")
    p.add_argument("--device", type=str, default="auto", choices=["auto","cpu","cuda"],
                   help="è¿è¡Œè®¾å¤‡")
    p.add_argument("--fp16", action="store_true", help="å¯ç”¨æ··åˆç²¾åº¦")
    p.add_argument("--max_new_tokens", type=int, default=48, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    
    # ç”Ÿæˆç­–ç•¥å‚æ•°ï¼ˆæ¨èè®¾ç½®ï¼‰
    p.add_argument("--num_beams", type=int, default=1, help="æŸæœç´¢å¤§å°")
    p.add_argument("--temperature", type=float, default=1.2, help="é‡‡æ ·æ¸©åº¦ï¼ˆæ¨è1.2ï¼‰")
    p.add_argument("--top_p", type=float, default=0.9, help="æ ¸é‡‡æ ·å‚æ•°ï¼ˆæ¨è0.9ï¼‰")
    p.add_argument("--top_k", type=int, default=0, help="Top-ké‡‡æ ·")
    p.add_argument("--repetition_penalty", type=float, default=1.5, help="é‡å¤æƒ©ç½šï¼ˆæ¨è1.5ï¼‰")
    p.add_argument("--no_repeat_ngram_size", type=int, default=3, help="é˜²é‡å¤n-gramå¤§å°")
    
    # åå¤„ç†å‚æ•°
    p.add_argument("--truncate_by_mask", action="store_true", default=True,
                   help="æ ¹æ®maskæˆªæ–­åºåˆ—")
    p.add_argument("--filter_near_zero", action="store_true", default=True,
                   help="è¿‡æ»¤è¿‘é›¶embeddings")
    p.add_argument("--near_zero_threshold", type=float, default=1e-6,
                   help="è¿‘é›¶è¿‡æ»¤é˜ˆå€¼")
    
    # è´¨é‡æ§åˆ¶å‚æ•°
    p.add_argument("--max_e_ratio", type=float, default=20.0,
                   help="æœ€å¤§Eå«é‡ç™¾åˆ†æ¯”ï¼ˆè¶…è¿‡åˆ™æ ‡è®°ä¸ºå¼‚å¸¸ï¼‰")
    p.add_argument("--min_length", type=int, default=5,
                   help="æœ€å°åºåˆ—é•¿åº¦")
    p.add_argument("--max_length", type=int, default=50,
                   help="æœ€å¤§åºåˆ—é•¿åº¦")
    
    # åˆ†æå‚æ•°
    p.add_argument("--n_samples", type=int, default=None,
                   help="é™åˆ¶å¤„ç†æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    p.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    p.add_argument("--generate_report", action="store_true", default=True,
                   help="ç”Ÿæˆåˆ†ææŠ¥å‘Š")
    
    return p.parse_args()

def smart_device(arg: str) -> torch.device:
    """æ™ºèƒ½è®¾å¤‡é€‰æ‹©"""
    if arg == "cpu": return torch.device("cpu")
    if arg == "cuda":
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # auto
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_pt(pt_path: str):
    """åŠ è½½embeddingsæ–‡ä»¶"""
    obj = torch.load(pt_path, map_location="cpu")
    for k in ["embeddings", "masks"]:
        if k not in obj:
            raise KeyError(f"Missing key '{k}' in {pt_path}")
    return obj

def validate_inputs(embeds: torch.Tensor, masks: torch.Tensor, lengths=None):
    """éªŒè¯è¾“å…¥æ•°æ®"""
    if embeds.ndim != 3:
        raise ValueError(f"Expected embeddings to be 3D, got {embeds.ndim}D")
    if embeds.shape[1] != 48:
        raise ValueError(f"Expected sequence length 48, got {embeds.shape[1]}")
    if embeds.shape[2] != 1024:
        raise ValueError(f"Expected embedding dim 1024, got {embeds.shape[2]}")
    if masks.shape[:2] != embeds.shape[:2]:
        raise ValueError(f"Mask shape {masks.shape} doesn't match embeddings {embeds.shape}")
    
    # æ•°æ®ç±»å‹è½¬æ¢
    if embeds.dtype != torch.float32:
        print(f"[WARN] Converting embeddings from {embeds.dtype} to float32")
        embeds = embeds.float()
    if masks.dtype != torch.bool:
        print(f"[WARN] Converting masks from {masks.dtype} to bool")
        masks = masks.bool()
    
    # æ•°æ®è´¨é‡æ£€æŸ¥
    if torch.isnan(embeds).any():
        print("[WARN] Found NaN values in embeddings")
    if torch.isinf(embeds).any():
        print("[WARN] Found Inf values in embeddings")
    
    valid_mask_counts = masks.sum(dim=1)
    if (valid_mask_counts == 0).any():
        print("[WARN] Found sequences with no valid positions in mask")
    
    return embeds, masks

def filter_near_zero_embeddings(embeds: torch.Tensor, masks: torch.Tensor, 
                                threshold: float = 1e-6, verbose: bool = False) -> torch.Tensor:
    """è¿‡æ»¤è¿‘é›¶embeddingsæå‡ç¨³å®šæ€§"""
    embed_norms = torch.norm(embeds, dim=-1)  # [B, L]
    near_zero = embed_norms < threshold
    updated_masks = masks & (~near_zero)
    
    # ç»Ÿè®¡ä¿¡æ¯
    original_valid = masks.sum().item()
    filtered_valid = updated_masks.sum().item()
    filtered_count = original_valid - filtered_valid
    
    if verbose and filtered_count > 0:
        print(f"[DEBUG] Filtered {filtered_count}/{original_valid} near-zero positions")
    
    # ç¡®ä¿æ¯ä¸ªåºåˆ—è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆä½ç½®
    valid_counts = updated_masks.sum(dim=1)
    restored_count = 0
    for i, count in enumerate(valid_counts):
        if count == 0:
            first_valid = masks[i].nonzero(as_tuple=True)[0]
            if len(first_valid) > 0:
                updated_masks[i, first_valid[0]] = True
                restored_count += 1
    
    if verbose and restored_count > 0:
        print(f"[DEBUG] Restored {restored_count} sequences that were completely filtered")
    
    return updated_masks

def decode_ids_to_sequence(token_ids: List[int], tokenizer) -> str:
    """å°†token IDsè§£ç ä¸ºæ°¨åŸºé…¸åºåˆ—"""
    sequence = []
    for token_id in token_ids:
        if token_id == tokenizer.pad_token_id:
            continue
        if token_id == tokenizer.eos_token_id:
            break
        
        token_str = tokenizer.convert_ids_to_tokens(token_id)
        if token_str.startswith('â–') and len(token_str) == 2:
            aa_char = token_str[1:]
            if aa_char in "ACDEFGHIKLMNPQRSTVWY":
                sequence.append(aa_char)
    
    return "".join(sequence)

def build_encoder_outputs(batch_embeds: torch.Tensor, attention_mask: torch.Tensor):
    """æ„å»ºencoder outputs"""
    return BaseModelOutput(last_hidden_state=batch_embeds)

def decode_batch(model, tokenizer, device, embeds: torch.Tensor, masks: torch.Tensor,
                args, enable_near_zero_filter: bool = True) -> List[str]:
    """æ‰¹é‡è§£ç """
    # è¿‘é›¶è¿‡æ»¤
    if enable_near_zero_filter:
        threshold = getattr(args, 'near_zero_threshold', 1e-6)
        verbose = getattr(args, 'verbose', False)
        masks = filter_near_zero_embeddings(embeds, masks, threshold, verbose)
    
    # é‡‡æ ·ç­–ç•¥
    do_sample = args.temperature != 1.0 or args.top_p < 1.0 or args.top_k > 0
    
    gen_kwargs = dict(
        max_new_tokens=args.max_new_tokens,
        num_beams=args.num_beams,
        do_sample=do_sample,
        temperature=args.temperature if do_sample else 1.0,
        top_p=args.top_p if do_sample else 1.0,
        top_k=args.top_k if args.top_k > 0 else None,
        repetition_penalty=args.repetition_penalty if args.repetition_penalty != 1.0 else None,
        no_repeat_ngram_size=args.no_repeat_ngram_size if args.no_repeat_ngram_size > 0 else None,
        attention_mask=masks,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        length_penalty=1.0,
    )
    
    # ç¡®ä¿decoderé…ç½®
    if model.config.decoder_start_token_id is None:
        model.config.decoder_start_token_id = tokenizer.pad_token_id
    
    # æ„å»ºencoder outputs
    enc_out = build_encoder_outputs(embeds, masks)
    
    try:
        # ç”Ÿæˆ
        if device.type == "cuda" and args.fp16:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                out_ids = model.generate(encoder_outputs=enc_out, **gen_kwargs)
        else:
            out_ids = model.generate(encoder_outputs=enc_out, **gen_kwargs)
            
        # è§£ç 
        sequences = []
        for i in range(out_ids.shape[0]):
            seq = decode_ids_to_sequence(out_ids[i].tolist(), tokenizer)
            sequences.append(seq)
        
        return sequences
        
    except Exception as e:
        print(f"[ERROR] Generation failed: {e}")
        raise

def trim_by_mask(seq: str, mask_row: torch.Tensor, length_meta: Optional[int]) -> str:
    """æ ¹æ®maskå’Œé•¿åº¦ä¿¡æ¯æˆªæ–­åºåˆ—"""
    if length_meta is not None and length_meta > 0 and length_meta <= len(seq):
        return seq[:length_meta]
    valid_len = int(mask_row.sum().item())
    if 0 < valid_len <= len(seq):
        return seq[:valid_len]
    return seq

def analyze_aa_composition(sequences: List[str]) -> Dict:
    """åˆ†ææ°¨åŸºé…¸ç»„æˆ"""
    if not sequences:
        return {}
    
    # åˆå¹¶æ‰€æœ‰åºåˆ—
    all_sequences = ''.join(seq for seq in sequences if seq)
    total_aa = len(all_sequences)
    
    if total_aa == 0:
        return {}
    
    # ç»Ÿè®¡æ°¨åŸºé…¸
    aa_counts = Counter(all_sequences)
    
    # å¤©ç„¶è›‹ç™½è´¨æ°¨åŸºé…¸åˆ†å¸ƒï¼ˆå‚è€ƒå€¼ï¼‰
    natural_composition = {
        'A': 8.25, 'R': 5.53, 'N': 4.06, 'D': 5.45, 'C': 1.37,
        'Q': 3.93, 'E': 6.75, 'G': 7.07, 'H': 2.27, 'I': 5.96,
        'L': 9.66, 'K': 5.84, 'M': 2.42, 'F': 3.86, 'P': 4.70,
        'S': 6.56, 'T': 5.34, 'W': 1.08, 'Y': 2.92, 'V': 6.87
    }
    
    # è®¡ç®—åˆ†æç»“æœ
    analysis = {
        'total_sequences': len(sequences),
        'valid_sequences': sum(1 for seq in sequences if len(seq) > 0),
        'total_aa': total_aa,
        'avg_length': total_aa / len(sequences) if sequences else 0,
        'aa_counts': aa_counts,
        'aa_frequencies': {aa: count/total_aa*100 for aa, count in aa_counts.items()},
        'natural_composition': natural_composition,
        'deviations': {},
        'fold_changes': {},
        'quality_flags': []
    }
    
    # è®¡ç®—åå·®å’Œå€æ•°å˜åŒ–
    for aa in "ACDEFGHIKLMNPQRSTVWY":
        observed = analysis['aa_frequencies'].get(aa, 0)
        natural = natural_composition.get(aa, 0)
        
        deviation = observed - natural
        fold_change = observed / natural if natural > 0 else float('inf')
        
        analysis['deviations'][aa] = deviation
        analysis['fold_changes'][aa] = fold_change
    
    # è´¨é‡è¯„ä¼°
    e_ratio = analysis['aa_frequencies'].get('E', 0)
    if e_ratio > 20:
        analysis['quality_flags'].append(f"Eå«é‡å¼‚å¸¸é«˜: {e_ratio:.1f}%")
    elif e_ratio > 15:
        analysis['quality_flags'].append(f"Eå«é‡åé«˜: {e_ratio:.1f}%")
    
    # æ£€æŸ¥å…¶ä»–å¼‚å¸¸
    for aa, fold in analysis['fold_changes'].items():
        if fold > 3:
            analysis['quality_flags'].append(f"{aa}å«é‡å¼‚å¸¸é«˜: {fold:.1f}å€")
        elif fold < 0.2:
            analysis['quality_flags'].append(f"{aa}å«é‡å¼‚å¸¸ä½: {fold:.1f}å€")
    
    return analysis

def generate_quality_report(analysis: Dict, args) -> str:
    """ç”Ÿæˆè´¨é‡è¯„ä¼°æŠ¥å‘Š"""
    if not analysis:
        return "æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼šåˆ†ææ•°æ®ä¸ºç©º"
    
    report = []
    report.append("=" * 60)
    report.append("ProT-Diffåºåˆ—è´¨é‡åˆ†ææŠ¥å‘Š")
    report.append("=" * 60)
    
    # åŸºæœ¬ç»Ÿè®¡
    report.append(f"åŸºæœ¬ç»Ÿè®¡:")
    report.append(f"  æ€»åºåˆ—æ•°: {analysis['total_sequences']}")
    report.append(f"  æœ‰æ•ˆåºåˆ—æ•°: {analysis['valid_sequences']}")
    report.append(f"  æœ‰æ•ˆç‡: {analysis['valid_sequences']/analysis['total_sequences']*100:.1f}%")
    report.append(f"  å¹³å‡é•¿åº¦: {analysis['avg_length']:.1f}")
    report.append(f"  æ€»æ°¨åŸºé…¸æ•°: {analysis['total_aa']}")
    
    # ç”Ÿæˆå‚æ•°
    report.append(f"ç”Ÿæˆå‚æ•°:")
    report.append(f"  æ¸©åº¦: {args.temperature}")
    report.append(f"  é‡å¤æƒ©ç½š: {args.repetition_penalty}")
    report.append(f"  Top-p: {args.top_p}")
    report.append(f"  N-gramé˜²é‡å¤: {args.no_repeat_ngram_size}")
    
    # æ°¨åŸºé…¸ç»„æˆåˆ†æ
    report.append(f"æ°¨åŸºé…¸ç»„æˆåˆ†æ:")
    report.append(f"{'AA':<3} {'è§‚å¯Ÿ%':<8} {'å¤©ç„¶%':<8} {'åå·®':<8} {'å€æ•°':<8} {'çŠ¶æ€'}")
    report.append("-" * 50)
    
    for aa in "ACDEFGHIKLMNPQRSTVWY":
        observed = analysis['aa_frequencies'].get(aa, 0)
        natural = analysis['natural_composition'].get(aa, 0)
        deviation = analysis['deviations'].get(aa, 0)
        fold = analysis['fold_changes'].get(aa, 0)
        
        status = ""
        if fold > 2.5:
            status = "å¼‚å¸¸é«˜"
        elif fold > 1.5:
            status = "åé«˜"
        elif fold < 0.3:
            status = "åä½"
        elif abs(deviation) < 1:
            status = "æ­£å¸¸"
        
        report.append(f"{aa:<3} {observed:<8.1f} {natural:<8.1f} {deviation:<+8.1f} {fold:<8.1f} {status}")
    
    # è´¨é‡è¯„ä¼°
    report.append(f"è´¨é‡è¯„ä¼°:")
    if analysis['quality_flags']:
        for flag in analysis['quality_flags']:
            report.append(f"{flag}")
    else:
        report.append(f"æœªå‘ç°æ˜æ˜¾å¼‚å¸¸")
    
    # Eå«é‡ç‰¹åˆ«åˆ†æ
    e_ratio = analysis['aa_frequencies'].get('E', 0)
    e_fold = analysis['fold_changes'].get('E', 0)
    report.append(f"Eå«é‡æ·±åº¦åˆ†æ:")
    report.append(f"  è§‚å¯Ÿå€¼: {e_ratio:.1f}%")
    report.append(f"  å¤©ç„¶å€¼: {analysis['natural_composition']['E']:.1f}%")
    report.append(f"  å€æ•°: {e_fold:.1f}x")
    
    if e_ratio > 20:
        report.append(f"  è¯„ä¼°: ä¸¥é‡åé«˜ï¼Œå¯èƒ½å½±å“ç”Ÿç‰©æ´»æ€§")
    elif e_ratio > 15:
        report.append(f"  è¯„ä¼°: æ˜æ˜¾åé«˜ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
    elif e_ratio > 10:
        report.append(f"  è¯„ä¼°: è½»åº¦åé«˜ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…")
    else:
        report.append(f"  è¯„ä¼°: æ­£å¸¸èŒƒå›´")
    
    # æ”¹è¿›å»ºè®®
    report.append(f"æ”¹è¿›å»ºè®®:")
    if e_ratio > 15:
        report.append(f"  1. æé«˜repetition_penaltyè‡³{args.repetition_penalty + 0.3:.1f}")
        report.append(f"  2. æé«˜temperatureè‡³{args.temperature + 0.3:.1f}")
        report.append(f"  3. å¢åŠ no_repeat_ngram_sizeè‡³{args.no_repeat_ngram_size + 1}")
        report.append(f"  4. è€ƒè™‘åå¤„ç†è¿‡æ»¤Eå«é‡>15%çš„åºåˆ—")
    else:
        report.append(f"  å½“å‰å‚æ•°è®¾ç½®åˆç†ï¼Œåºåˆ—è´¨é‡è‰¯å¥½")
    
    report.append(f"ç»“è®º:")
    if analysis['quality_flags']:
        report.append(f"  ç”Ÿæˆçš„åºåˆ—å­˜åœ¨ä¸€å®šç¨‹åº¦çš„æ°¨åŸºé…¸åˆ†å¸ƒåå‘ï¼Œ")
        report.append(f"  ä¸»è¦ä½“ç°åœ¨Eå«é‡åé«˜ã€‚è¿™å¯èƒ½æºäºProT-Diffè®­ç»ƒæ•°æ®")
        report.append(f"  çš„åˆ†å¸ƒç‰¹å¾ã€‚å»ºè®®ç»“åˆç”Ÿç‰©å­¦éªŒè¯ä½¿ç”¨è¿™äº›åºåˆ—ã€‚")
    else:
        report.append(f"  ç”Ÿæˆçš„åºåˆ—æ°¨åŸºé…¸åˆ†å¸ƒç›¸å¯¹å‡è¡¡ï¼Œè´¨é‡è‰¯å¥½ã€‚")
    
    report.append("=" * 60)
    
    return "\n".join(report)

def save_results(out_rows: List[Dict], fasta_lines: List[str], analysis: Dict, 
                report: str, args):
    """ä¿å­˜ç»“æœæ–‡ä»¶"""
    # CSVæ–‡ä»¶
    csv_path = f"{args.out_prefix}.csv"
    import csv
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["id","length_pred","length_meta","aa_seq","e_ratio","quality"])
        for r in out_rows:
            w.writerow([r["id"], r["length_pred"], r["length_meta"], 
                       r["aa_seq"], r.get("e_ratio", 0), r.get("quality", "unknown")])
    
    # FASTAæ–‡ä»¶
    fasta_path = f"{args.out_prefix}.fasta"
    with open(fasta_path, "w", encoding="utf-8") as f:
        f.writelines(fasta_lines)
    
    # JSONLæ–‡ä»¶
    jsonl_path = f"{args.out_prefix}.jsonl"
    with open(jsonl_path, "w", encoding="utf-8") as f:
        for r in out_rows:
            f.write(json.dumps(r, ensure_ascii=False)+"\n")
    
    # åˆ†ææŠ¥å‘Š
    if args.generate_report:
        report_path = f"{args.out_prefix}_analysis_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)
        
        # åˆ†ææ•°æ®JSON
        analysis_path = f"{args.out_prefix}_analysis.json"
        # è½¬æ¢Counterå¯¹è±¡ä¸ºdictä»¥ä¾¿JSONåºåˆ—åŒ–
        analysis_copy = analysis.copy()
        if 'aa_counts' in analysis_copy:
            analysis_copy['aa_counts'] = dict(analysis_copy['aa_counts'])
        
        with open(analysis_path, "w", encoding="utf-8") as f:
            json.dump(analysis_copy, f, ensure_ascii=False, indent=2)
    
    return csv_path, fasta_path, jsonl_path

def main():
    args = parse_args()
    # åœ¨ main() é‡Œ parse_args() ä¹‹ååŠ 
    if args.n_samples is not None:
        os.makedirs("test_data", exist_ok=True)
        args.out_prefix = os.path.join("test_data", args.out_prefix)
    else:
        os.makedirs("full_data", exist_ok=True)
        args.out_prefix = os.path.join("full_data", args.out_prefix)

    t0 = time.time()
    device = smart_device(args.device)
    
    print(f"ProT-Diff Embeddingsè§£ç ä¸åˆ†æ")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ç”Ÿæˆå‚æ•°: temp={args.temperature}, rep_penalty={args.repetition_penalty}, top_p={args.top_p}")
    
    try:
        # 1. åŠ è½½æ•°æ®
        print(f"åŠ è½½æ•°æ®...")
        pack = load_pt(args.pt_path)
        embeds = pack["embeddings"]
        masks = pack["masks"]
        lengths = pack.get("lengths", None)
        
        # é™åˆ¶æ ·æœ¬æ•°
        if args.n_samples is not None:
            embeds = embeds[:args.n_samples]
            masks = masks[:args.n_samples]
            if lengths is not None:
                lengths = lengths[:args.n_samples]
            print(f"é™åˆ¶å¤„ç† {args.n_samples} ä¸ªæ ·æœ¬")
        
        # éªŒè¯æ•°æ®
        embeds, masks = validate_inputs(embeds, masks, lengths)
        N = embeds.shape[0]
        print(f"åŠ è½½äº† {N} ä¸ªæ ·æœ¬ï¼Œembeddingå½¢çŠ¶: {embeds.shape},æ–‡ä»¶åä¸º{args.pt_path},å¯¼å‡ºåœ°å€ä¸º{args.out_prefix}")
        
        # 2. åŠ è½½æ¨¡å‹
        print(f"åŠ è½½ProtT5æ¨¡å‹...")
        if not os.path.exists(args.model_dir):
            raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {args.model_dir}")
        
        # åŠ è½½tokenizer
        try:
            tokenizer = T5Tokenizer.from_pretrained(args.model_dir, legacy=False)
        except Exception as e1:
            try:
                tokenizer = T5Tokenizer.from_pretrained(args.model_dir, legacy=True)
            except Exception as e2:
                tokenizer = AutoTokenizer.from_pretrained(args.model_dir, legacy=False)
        
        model = T5ForConditionalGeneration.from_pretrained(args.model_dir)
        model = model.to(device)
        model.eval()
        
        print(f"æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        
        # å†…å­˜ä¼˜åŒ–
        if device.type == "cuda":
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print(f"GPUå†…å­˜: {torch.cuda.memory_allocated()/1e9:.2f}GB")
        
    except Exception as e:
        print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)
    
    # 3. æ‰¹é‡è§£ç 
    print(f"å¼€å§‹è§£ç ...")
    bs = args.batch_size
    out_rows = []
    fasta_lines = []
    
    # å¤„ç†lengths
    lengths_list = None
    if lengths is not None and torch.is_tensor(lengths):
        lengths_list = lengths.tolist()
    
    with torch.no_grad():
        for s in range(0, N, bs):
            e = min(s+bs, N)
            batch_emb = embeds[s:e].to(device, non_blocking=True)
            batch_msk = masks[s:e].to(device, non_blocking=True)
            
            try:
                seqs = decode_batch(model, tokenizer, device, batch_emb, batch_msk, 
                                  args, enable_near_zero_filter=args.filter_near_zero)
            except Exception as ex:
                print(f"æ‰¹æ¬¡ {s}:{e} è§£ç å¤±è´¥: {ex}")
                print(f"å°è¯•ä¿å®ˆè®¾ç½®...")
                
                # ä¿å®ˆfallbackè®¾ç½®
                class FallbackArgs:
                    def __init__(self, original_args):
                        self.max_new_tokens = original_args.max_new_tokens
                        self.num_beams = 1
                        self.temperature = 1.0
                        self.top_p = 0.95
                        self.top_k = 0
                        self.repetition_penalty = 1.2
                        self.no_repeat_ngram_size = 2
                        self.fp16 = False
                        self.filter_near_zero = False
                        self.verbose = getattr(original_args, 'verbose', False)
                        self.near_zero_threshold = original_args.near_zero_threshold
                
                try:
                    fallback_args = FallbackArgs(args)
                    seqs = decode_batch(model, tokenizer, device, batch_emb, batch_msk, 
                                      fallback_args, enable_near_zero_filter=False)
                    print(f"ä¿å®ˆè®¾ç½®è§£ç æˆåŠŸ")
                except Exception as ex2:
                    print(f"ä¿å®ˆè®¾ç½®ä¹Ÿå¤±è´¥: {ex2}")
                    batch_size = batch_emb.shape[0]
                    seqs = [""] * batch_size
                    print(f"ä½¿ç”¨ç©ºåºåˆ—å ä½")
            
            # å¤„ç†æ¯ä¸ªåºåˆ—
            for i, seq in enumerate(seqs):
                idx = s + i
                seq_raw = seq
                length_meta = lengths_list[idx] if lengths_list is not None else None
                
                # æ ¹æ®maskæˆªæ–­
                if args.truncate_by_mask:
                    seq_final = trim_by_mask(seq_raw, batch_msk[i], length_meta)
                else:
                    seq_final = seq_raw
                
                # æ¸…ç†åºåˆ—ï¼ˆåªä¿ç•™æ ‡å‡†20ä¸ªæ°¨åŸºé…¸ï¼‰
                allowed = set("ACDEFGHIKLMNPQRSTVWY")
                cleaned = "".join([c for c in seq_final if c in allowed])
                
                # è´¨é‡è¯„ä¼°
                e_count = cleaned.count('E')
                e_ratio = e_count / len(cleaned) * 100 if len(cleaned) > 0 else 0
                
                quality = "good"
                if len(cleaned) == 0:
                    quality = "empty"
                elif len(cleaned) < args.min_length:
                    quality = "too_short"
                elif len(cleaned) > args.max_length:
                    quality = "too_long"
                elif e_ratio > args.max_e_ratio:
                    quality = "high_e"
                
                row = {
                    "id": idx,
                    "length_pred": len(cleaned),
                    "length_meta": int(length_meta) if length_meta is not None else None,
                    "aa_seq": cleaned,
                    "e_ratio": e_ratio,
                    "quality": quality
                }
                out_rows.append(row)
                
                # FASTAæ ¼å¼
                quality_tag = f" quality={quality}" if quality != "good" else ""
                e_tag = f" E={e_ratio:.1f}%" if e_ratio > 0 else ""
                fasta_lines.append(f">{idx} len={len(cleaned)}{quality_tag}{e_tag}\n{cleaned}\n")
            
            # è¿›åº¦æŠ¥å‘Š
            if device.type == "cuda":
                torch.cuda.empty_cache()
            
            if (e % (bs*5) == 0) or (e==N):
                elapsed = time.time() - t0
                rate = e / elapsed if elapsed > 0 else 0
                eta = (N - e) / rate if rate > 0 else 0
                mem_info = f", GPU: {torch.cuda.memory_allocated()/1e9:.2f}GB" if device.type == "cuda" else ""
                print(f"ğŸ“Š è¿›åº¦: {e}/{N} ({100*e/N:.1f}%), é€Ÿåº¦: {rate:.1f} seq/s, å‰©ä½™: {eta:.0f}s{mem_info}")
    
    # 4. åˆ†æç»“æœ
    print(f"\nğŸ“ˆ åˆ†æç»“æœ...")
    sequences = [r["aa_seq"] for r in out_rows]
    analysis = analyze_aa_composition(sequences)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_quality_report(analysis, args)
    
    # 5. ä¿å­˜ç»“æœ
    print(f" ä¿å­˜ç»“æœ...")
    csv_path, fasta_path, jsonl_path = save_results(out_rows, fasta_lines, analysis, report, args)
    
    # 6. æ€»ç»“
    dt = time.time() - t0
    total_seqs = len(out_rows)
    valid_seqs = sum(1 for r in out_rows if len(r["aa_seq"]) > 0)
    good_quality = sum(1 for r in out_rows if r["quality"] == "good")
    
    print(f"è§£ç å®Œæˆ!")
    print(f"æ€»è€—æ—¶: {dt:.1f}s ({total_seqs/dt:.1f} seq/s)")
    print(f"ç»Ÿè®¡: {valid_seqs}/{total_seqs} æœ‰æ•ˆåºåˆ—, {good_quality}/{total_seqs} é«˜è´¨é‡åºåˆ—")
    print(f"è¾“å‡ºæ–‡ä»¶:")
    print(f"   - {csv_path}")
    print(f"   - {fasta_path}")
    print(f"   - {jsonl_path}")
    if args.generate_report:
        print(f"   - {args.out_prefix}_analysis_report.txt")
        print(f"   - {args.out_prefix}_analysis.json")
    
    # 7. æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
    if analysis:
        e_ratio = analysis['aa_frequencies'].get('E', 0)
        avg_length = analysis.get('avg_length', 0)
        print(f"è´¨é‡æ‘˜è¦:")
        print(f"   å¹³å‡é•¿åº¦: {avg_length:.1f}")
        print(f"   Eå«é‡: {e_ratio:.1f}% (å¤©ç„¶: 6.8%)")
        
        if e_ratio > 20:
            print(f"   Eå«é‡ä¸¥é‡åé«˜ï¼Œå»ºè®®ä¼˜åŒ–å‚æ•°")
        elif e_ratio > 15:
            print(f"   Eå«é‡æ˜æ˜¾åé«˜ï¼Œå¯è€ƒè™‘åå¤„ç†")
        else:
            print(f"   Eå«é‡åœ¨å¯æ¥å—èŒƒå›´å†…")
    
    print(f"æŸ¥çœ‹å®Œæ•´åˆ†ææŠ¥å‘Š: cat {args.out_prefix}_analysis_report.txt")

if __name__ == "__main__":
    main()
