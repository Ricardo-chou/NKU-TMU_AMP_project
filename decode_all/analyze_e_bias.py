#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ·±åº¦åˆ†æEå«é‡å¼‚å¸¸é«˜çš„åŸå› 
"""

import torch
import numpy as np
from transformers import T5ForConditionalGeneration, T5Tokenizer
from transformers.modeling_outputs import BaseModelOutput
import matplotlib.pyplot as plt
from collections import Counter

def analyze_model_bias(model, tokenizer, device):
    """åˆ†ææ¨¡å‹æœ¬èº«çš„åå‘æ€§"""
    print("=== æ¨¡å‹åå‘æ€§åˆ†æ ===")
    
    # 1. æ£€æŸ¥LM headæƒé‡
    lm_head = model.lm_head
    lm_weights = lm_head.weight.data  # [vocab_size, hidden_size]
    
    print(f"LM headæƒé‡å½¢çŠ¶: {lm_weights.shape}")
    
    # åˆ†ææ°¨åŸºé…¸tokençš„æƒé‡åˆ†å¸ƒ
    aa_tokens = {}
    for aa in "ACDEFGHIKLMNPQRSTVWY":
        token_id = tokenizer.convert_tokens_to_ids(f"â–{aa}")
        if token_id != tokenizer.unk_token_id:
            aa_tokens[aa] = token_id
    
    print(f"æ‰¾åˆ° {len(aa_tokens)} ä¸ªæ°¨åŸºé…¸token")
    
    # è®¡ç®—æ¯ä¸ªæ°¨åŸºé…¸tokenæƒé‡çš„ç»Ÿè®¡ä¿¡æ¯
    aa_weight_stats = {}
    for aa, token_id in aa_tokens.items():
        if token_id < lm_weights.shape[0]:
            weight_vec = lm_weights[token_id]
            stats = {
                'mean': weight_vec.mean().item(),
                'std': weight_vec.std().item(),
                'norm': torch.norm(weight_vec).item(),
                'max': weight_vec.max().item(),
                'min': weight_vec.min().item()
            }
            aa_weight_stats[aa] = stats
    
    # æŒ‰æƒé‡èŒƒæ•°æ’åº
    sorted_by_norm = sorted(aa_weight_stats.items(), key=lambda x: x[1]['norm'], reverse=True)
    
    print("\næ°¨åŸºé…¸tokenæƒé‡èŒƒæ•°æ’åº:")
    print("AA\tToken_ID\tNorm\tMean\tStd")
    print("-" * 50)
    for aa, stats in sorted_by_norm:
        token_id = aa_tokens[aa]
        print(f"{aa}\t{token_id}\t{stats['norm']:.3f}\t{stats['mean']:.3f}\t{stats['std']:.3f}")
    
    # æ£€æŸ¥E tokençš„ç‰¹æ®Šæ€§
    e_token_id = aa_tokens.get('E')
    if e_token_id:
        e_stats = aa_weight_stats['E']
        print(f"\n*** E token (id={e_token_id}) åˆ†æ ***")
        print(f"æƒé‡èŒƒæ•°: {e_stats['norm']:.6f}")
        print(f"æƒé‡å‡å€¼: {e_stats['mean']:.6f}")
        print(f"æƒé‡æ ‡å‡†å·®: {e_stats['std']:.6f}")
        
        # ä¸å…¶ä»–æ°¨åŸºé…¸æ¯”è¾ƒ
        avg_norm = np.mean([stats['norm'] for stats in aa_weight_stats.values()])
        print(f"å¹³å‡æƒé‡èŒƒæ•°: {avg_norm:.6f}")
        print(f"Eç›¸å¯¹åå·®: {(e_stats['norm'] - avg_norm) / avg_norm * 100:.1f}%")
    
    return aa_tokens, aa_weight_stats

def analyze_embeddings_bias(embeddings_path, n_samples=100):
    """åˆ†æè¾“å…¥embeddingsçš„åå‘æ€§"""
    print("\n=== è¾“å…¥Embeddingsåå‘æ€§åˆ†æ ===")
    
    data = torch.load(embeddings_path, map_location="cpu")
    embeds = data["embeddings"][:n_samples]  # [N, 48, 1024]
    masks = data["masks"][:n_samples]       # [N, 48]
    
    print(f"åˆ†æ {n_samples} ä¸ªæ ·æœ¬çš„embeddings")
    
    # 1. æ•´ä½“ç»Ÿè®¡
    valid_embeds = embeds[masks]  # åªè€ƒè™‘æœ‰æ•ˆä½ç½®
    print(f"æœ‰æ•ˆembeddingæ•°é‡: {valid_embeds.shape[0]}")
    print(f"Embeddingç»Ÿè®¡:")
    print(f"  å‡å€¼: {valid_embeds.mean().item():.6f}")
    print(f"  æ ‡å‡†å·®: {valid_embeds.std().item():.6f}")
    print(f"  æœ€å°å€¼: {valid_embeds.min().item():.6f}")
    print(f"  æœ€å¤§å€¼: {valid_embeds.max().item():.6f}")
    
    # 2. æ£€æŸ¥embeddingçš„åˆ†å¸ƒç‰¹å¾
    # è®¡ç®—æ¯ä¸ªä½ç½®çš„embeddingå‘é‡çš„ç‰¹å¾
    position_stats = []
    for i in range(embeds.shape[1]):  # å¯¹æ¯ä¸ªä½ç½®
        pos_embeds = embeds[:, i, :][masks[:, i]]  # è¯¥ä½ç½®çš„æ‰€æœ‰æœ‰æ•ˆembedding
        if len(pos_embeds) > 0:
            stats = {
                'position': i,
                'count': len(pos_embeds),
                'mean_norm': torch.norm(pos_embeds, dim=1).mean().item(),
                'mean_value': pos_embeds.mean().item(),
                'std_value': pos_embeds.std().item()
            }
            position_stats.append(stats)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä½ç½®åå‘
    if position_stats:
        norms = [s['mean_norm'] for s in position_stats]
        print(f"\nä½ç½®embeddingèŒƒæ•°ç»Ÿè®¡:")
        print(f"  æœ€å°èŒƒæ•°: {min(norms):.6f}")
        print(f"  æœ€å¤§èŒƒæ•°: {max(norms):.6f}")
        print(f"  èŒƒæ•°æ ‡å‡†å·®: {np.std(norms):.6f}")
    
    return position_stats

def test_generation_without_embeddings(model, tokenizer, device):
    """æµ‹è¯•ä¸ä½¿ç”¨ProT-Diff embeddingsçš„ç”Ÿæˆ"""
    print("\n=== æµ‹è¯•çº¯éšæœºç”Ÿæˆ ===")
    
    # åˆ›å»ºéšæœºencoder outputs
    batch_size = 8
    seq_len = 32
    hidden_size = 1024
    
    # éšæœºåˆå§‹åŒ–encoder outputs
    random_embeds = torch.randn(batch_size, seq_len, hidden_size, device=device) * 0.1
    random_mask = torch.ones(batch_size, seq_len, dtype=torch.bool, device=device)
    
    encoder_outputs = BaseModelOutput(last_hidden_state=random_embeds)
    
    # ç¡®ä¿decoderé…ç½®
    if model.config.decoder_start_token_id is None:
        model.config.decoder_start_token_id = tokenizer.pad_token_id
    
    gen_kwargs = {
        "encoder_outputs": encoder_outputs,
        "attention_mask": random_mask,
        "max_new_tokens": 20,
        "num_beams": 1,
        "do_sample": True,
        "temperature": 1.0,
        "top_p": 0.95,
        "repetition_penalty": 1.2,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
    }
    
    with torch.no_grad():
        output_ids = model.generate(**gen_kwargs)
    
    # è§£ç å¹¶åˆ†æ
    sequences = []
    for i in range(output_ids.shape[0]):
        seq = ""
        for token_id in output_ids[i].tolist():
            if token_id == tokenizer.pad_token_id:
                continue
            if token_id == tokenizer.eos_token_id:
                break
            
            token_str = tokenizer.convert_ids_to_tokens(token_id)
            if token_str.startswith('â–') and len(token_str) == 2:
                aa_char = token_str[1:]
                if aa_char in "ACDEFGHIKLMNPQRSTVWY":
                    seq += aa_char
        sequences.append(seq)
    
    # åˆ†ææ°¨åŸºé…¸ç»„æˆ
    all_aas = ''.join(sequences)
    if all_aas:
        aa_counts = Counter(all_aas)
        total_aa = len(all_aas)
        e_ratio = aa_counts.get('E', 0) / total_aa * 100
        
        print(f"éšæœºembeddingç”Ÿæˆçš„åºåˆ—:")
        for i, seq in enumerate(sequences[:3]):
            print(f"  {i+1}: {seq}")
        
        print(f"\néšæœºembeddingçš„Eå«é‡: {e_ratio:.1f}%")
        print("æ°¨åŸºé…¸åˆ†å¸ƒ:")
        for aa in sorted(aa_counts.keys()):
            ratio = aa_counts[aa] / total_aa * 100
            print(f"  {aa}: {ratio:.1f}%")
    else:
        print("éšæœºembeddingæœªç”Ÿæˆæœ‰æ•ˆåºåˆ—")

def analyze_training_data_hypothesis():
    """åˆ†æè®­ç»ƒæ•°æ®åå‘çš„å‡è®¾"""
    print("\n=== è®­ç»ƒæ•°æ®åå‘å‡è®¾åˆ†æ ===")
    
    print("å¯èƒ½çš„åŸå› :")
    print("1. ProtT5è®­ç»ƒæ—¶ä½¿ç”¨çš„è›‹ç™½è´¨æ•°æ®åº“å¯èƒ½æœ‰Eå«é‡åå‘")
    print("2. ProT-Diffè®­ç»ƒæ•°æ®ä¸­æŠ—èŒè‚½å¯èƒ½å¯Œå«å¸¦ç”µæ°¨åŸºé…¸(E,D,K,R)")
    print("3. æ¨¡å‹å­¦ä¹ åˆ°äº†ç‰¹å®šçš„åºåˆ—æ¨¡å¼åå¥½")
    
    print("\næŠ—èŒè‚½çš„å…¸å‹ç‰¹å¾:")
    print("- å¯Œå«å¸¦æ­£ç”µè·æ°¨åŸºé…¸ (K, R)")
    print("- å¯Œå«ç–æ°´æ€§æ°¨åŸºé…¸ (L, V, I, F)")
    print("- å¯èƒ½å«æœ‰è¾ƒå¤šææ€§æ°¨åŸºé…¸ç”¨äºè†œç›¸äº’ä½œç”¨")
    print("- Eä½œä¸ºå¸¦è´Ÿç”µè·æ°¨åŸºé…¸ï¼Œå¯èƒ½åœ¨æŸäº›AMPä¸­èµ·é‡è¦ä½œç”¨")

def main():
    model_dir = "/root/autodl-tmp/prot_t5_xl_uniref50"
    embeddings_path = "/root/NKU-TMU_AMP_project/generated_embeddings.pt"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    tokenizer = T5Tokenizer.from_pretrained(model_dir, legacy=False)
    model = T5ForConditionalGeneration.from_pretrained(model_dir)
    model = model.to(device)
    model.eval()
    
    # 1. åˆ†ææ¨¡å‹åå‘æ€§
    aa_tokens, aa_weight_stats = analyze_model_bias(model, tokenizer, device)
    
    # 2. åˆ†æè¾“å…¥embeddings
    position_stats = analyze_embeddings_bias(embeddings_path, n_samples=100)
    
    # 3. æµ‹è¯•éšæœºç”Ÿæˆ
    test_generation_without_embeddings(model, tokenizer, device)
    
    # 4. åˆ†æè®­ç»ƒæ•°æ®å‡è®¾
    analyze_training_data_hypothesis()
    
    print("\n=== ç»“è®ºä¸å»ºè®® ===")
    print("Eå«é‡å¼‚å¸¸é«˜çš„å¯èƒ½åŸå› æ’åº:")
    print("1. ğŸ”¥ æ¨¡å‹æƒé‡åå‘: LM headä¸­E tokenæƒé‡å¯èƒ½å¼‚å¸¸")
    print("2. ğŸ”¥ ProT-Diff embeddingsç‰¹å¾: è¾“å…¥embeddingså¯èƒ½ç¼–ç äº†Eåå‘")
    print("3. ğŸ”¥ è®­ç»ƒæ•°æ®åå‘: æŠ—èŒè‚½æ•°æ®é›†å¯èƒ½å¤©ç„¶å¯Œå«E")
    print("4. è§£ç ç­–ç•¥: è™½ç„¶å·²ä¼˜åŒ–ï¼Œä½†ä»å¯èƒ½ä¸å¤Ÿ")
    
    print("\nè¿›ä¸€æ­¥æ”¹è¿›å»ºè®®:")
    print("1. å°è¯•æ›´å¼ºçš„repetition_penalty (1.5-2.0)")
    print("2. ä½¿ç”¨æ›´é«˜çš„temperature (1.5-2.0)")
    print("3. è€ƒè™‘åå¤„ç†è¿‡æ»¤å¼‚å¸¸é«˜Eå«é‡çš„åºåˆ—")
    print("4. åˆ†æåŸå§‹ProT-Diffè®­ç»ƒæ•°æ®çš„æ°¨åŸºé…¸åˆ†å¸ƒ")

if __name__ == "__main__":
    main()
