#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Test improved generation strategies to reduce E bias
"""

import os, sys
import torch
import torch.nn as nn
from transformers import T5ForConditionalGeneration, T5Tokenizer
from transformers.modeling_outputs import BaseModelOutput
from collections import Counter

def load_small_sample(pt_path: str, n_samples: int = 16):
    """åŠ è½½å°‘é‡æ ·æœ¬ç”¨äºè°ƒè¯•"""
    data = torch.load(pt_path, map_location="cpu")
    embeds = data["embeddings"][:n_samples]
    masks = data["masks"][:n_samples]
    lengths = data.get("lengths", None)
    if lengths is not None:
        lengths = lengths[:n_samples]
    return embeds, masks, lengths

def decode_ids_to_sequence(token_ids, tokenizer):
    """è§£ç token IDsåˆ°æ°¨åŸºé…¸åºåˆ—"""
    sequence = []
    for token_id in token_ids:
        if token_id == tokenizer.pad_token_id:
            continue
        if token_id == tokenizer.eos_token_id:
            break
        
        token_str = tokenizer.convert_ids_to_tokens(token_id)
        if token_str.startswith('â–') and len(token_str) == 2:
            aa_char = token_str[1:]
            if aa_char in "ACDEFGHIKLMNPQRSTVWY":
                sequence.append(aa_char)
    
    return "".join(sequence)

def test_generation_strategies(model, tokenizer, device, embeds, masks):
    """æµ‹è¯•ä¸åŒçš„ç”Ÿæˆç­–ç•¥"""
    
    strategies = [
        {
            "name": "baseline_temp0.8",
            "max_new_tokens": 32,
            "num_beams": 1,
            "do_sample": True,
            "temperature": 0.8,
            "top_p": 0.95,
            "repetition_penalty": 1.0
        },
        {
            "name": "higher_temp1.2", 
            "max_new_tokens": 32,
            "num_beams": 1,
            "do_sample": True,
            "temperature": 1.2,
            "top_p": 0.9,
            "repetition_penalty": 1.0
        },
        {
            "name": "with_repetition_penalty",
            "max_new_tokens": 32,
            "num_beams": 1,
            "do_sample": True,
            "temperature": 1.0,
            "top_p": 0.95,
            "repetition_penalty": 1.2  # æƒ©ç½šé‡å¤
        },
        {
            "name": "top_k_sampling",
            "max_new_tokens": 32,
            "num_beams": 1,
            "do_sample": True,
            "temperature": 1.0,
            "top_k": 10,  # é™åˆ¶å€™é€‰tokenæ•°é‡
            "top_p": 1.0,
            "repetition_penalty": 1.1
        },
        {
            "name": "diverse_beam",
            "max_new_tokens": 32,
            "num_beams": 3,
            "do_sample": False,
            "num_beam_groups": 3,
            "diversity_penalty": 0.5,
            "repetition_penalty": 1.1
        }
    ]
    
    results = {}
    
    for strategy in strategies:
        print(f"\n=== æµ‹è¯•ç­–ç•¥: {strategy['name']} ===")
        
        # è®¾ç½®decoder start token
        if model.config.decoder_start_token_id is None:
            model.config.decoder_start_token_id = tokenizer.pad_token_id
        
        # æ„å»ºencoder outputs
        encoder_outputs = BaseModelOutput(last_hidden_state=embeds.to(device))
        
        gen_kwargs = {
            "encoder_outputs": encoder_outputs,
            "attention_mask": masks.to(device),
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id,
            **{k: v for k, v in strategy.items() if k != "name"}
        }
        
        try:
            with torch.no_grad():
                output_ids = model.generate(**gen_kwargs)
            
            # è§£ç æ‰€æœ‰åºåˆ—
            sequences = []
            for i in range(output_ids.shape[0]):
                seq = decode_ids_to_sequence(output_ids[i].tolist(), tokenizer)
                sequences.append(seq)
            
            # åˆ†ææ°¨åŸºé…¸ç»„æˆ
            all_aas = ''.join(sequences)
            aa_counts = Counter(all_aas)
            total_aa = len(all_aas)
            
            if total_aa > 0:
                e_ratio = aa_counts.get('E', 0) / total_aa * 100
                l_ratio = aa_counts.get('L', 0) / total_aa * 100
                
                print(f"ç”Ÿæˆåºåˆ—æ•°: {len(sequences)}")
                print(f"å¹³å‡é•¿åº¦: {total_aa/len(sequences):.1f}")
                print(f"Eå«é‡: {e_ratio:.1f}%")
                print(f"Lå«é‡: {l_ratio:.1f}%")
                
                # æ˜¾ç¤ºå‰3ä¸ªåºåˆ—
                print("ç¤ºä¾‹åºåˆ—:")
                for i, seq in enumerate(sequences[:3]):
                    print(f"  {i+1}: {seq}")
                
                results[strategy['name']] = {
                    'sequences': sequences,
                    'e_ratio': e_ratio,
                    'l_ratio': l_ratio,
                    'avg_length': total_aa/len(sequences),
                    'aa_counts': aa_counts
                }
            else:
                print("æœªç”Ÿæˆæœ‰æ•ˆåºåˆ—")
                results[strategy['name']] = None
                
        except Exception as e:
            print(f"ç­–ç•¥å¤±è´¥: {e}")
            results[strategy['name']] = None
    
    return results

def analyze_results(results):
    """åˆ†æä¸åŒç­–ç•¥çš„ç»“æœ"""
    print(f"\n=== ç­–ç•¥å¯¹æ¯”åˆ†æ ===")
    print("ç­–ç•¥\t\tEå«é‡\tLå«é‡\tå¹³å‡é•¿åº¦")
    print("-" * 50)
    
    best_strategy = None
    best_e_ratio = float('inf')
    
    for name, result in results.items():
        if result is not None:
            e_ratio = result['e_ratio']
            l_ratio = result['l_ratio']
            avg_len = result['avg_length']
            
            print(f"{name:<20}\t{e_ratio:.1f}%\t{l_ratio:.1f}%\t{avg_len:.1f}")
            
            # æ‰¾åˆ°Eå«é‡æœ€ä½çš„ç­–ç•¥
            if e_ratio < best_e_ratio:
                best_e_ratio = e_ratio
                best_strategy = name
        else:
            print(f"{name:<20}\tå¤±è´¥\tå¤±è´¥\tå¤±è´¥")
    
    if best_strategy:
        print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_strategy} (Eå«é‡: {best_e_ratio:.1f}%)")
        
        # æ˜¾ç¤ºæœ€ä½³ç­–ç•¥çš„è¯¦ç»†æ°¨åŸºé…¸åˆ†å¸ƒ
        best_result = results[best_strategy]
        aa_counts = best_result['aa_counts']
        total_aa = sum(aa_counts.values())
        
        print(f"\n{best_strategy} çš„æ°¨åŸºé…¸åˆ†å¸ƒ:")
        for aa in "ACDEFGHIKLMNPQRSTVWY":
            count = aa_counts.get(aa, 0)
            if count > 0:
                ratio = count / total_aa * 100
                print(f"  {aa}: {ratio:.1f}%")
    
    return best_strategy

def main():
    pt_path = "/root/NKU-TMU_AMP_project/generated_embeddings.pt"
    model_dir = "/root/autodl-tmp/prot_t5_xl_uniref50"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    embeds, masks, lengths = load_small_sample(pt_path, n_samples=16)
    print(f"åŠ è½½äº† {embeds.shape[0]} ä¸ªæ ·æœ¬")
    
    # åŠ è½½æ¨¡å‹
    tokenizer = T5Tokenizer.from_pretrained(model_dir, legacy=False)
    model = T5ForConditionalGeneration.from_pretrained(model_dir)
    model = model.to(device)
    model.eval()
    
    print("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æµ‹è¯•ä¸åŒç­–ç•¥
    results = test_generation_strategies(model, tokenizer, device, embeds, masks)
    
    # åˆ†æç»“æœ
    best_strategy = analyze_results(results)
    
    return best_strategy, results

if __name__ == "__main__":
    best_strategy, results = main()
